@inproceedings{petersen2008,
author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
title = {Systematic mapping studies in software engineering},
year = {2008},
publisher = {BCS Learning \& Development Ltd.},
address = {Swindon, GBR},
abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions.OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps.METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews.RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined.CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
booktitle = {Proceedings of the 12th International Conference on Evaluation and Assessment in Software Engineering},
pages = {68–77},
numpages = {10},
keywords = {evidence based software engineering, systematic mapping studies, systematic reviews},
location = {Italy},
series = {EASE'08}
}

@book{strauss1998opencoding,
  title     = {Basics of Qualitative Research: Techniques and Procedures for Developing Grounded Theory},
  author    = {Strauss, Anselm and Corbin, Juliet},
  year      = {1998},
  edition   = {2},
  publisher = {Sage Publications},
  address   = {Thousand Oaks, CA}
}

@inproceedings{Li2020,
   abstract = {For large industrial applications, system test cases are still often described in natural language (NL), and their number can reach thousands. Test automation is to automatically execute the test cases. Achieving test automation typically requires substantial manual effort for creating executable test scripts from these NL test cases. In particular, given that each NL test case consists of a sequence of NL test steps, testers first implement a test API method for each test step and then write a test script for invoking these test API methods sequentially for test automation. Across different test cases, multiple test steps can share semantic similarities, supposedly mapped to the same API method. However, due to numerous test steps in various NL forms under manual inspection, testers may not realize those semantically similar test steps and thus waste effort to implement duplicate test API methods for them. To address this issue, in this paper, we propose a new approach based on natural language processing to cluster similar NL test steps together such that the test steps in each cluster can be mapped to the same test API method. Our approach includes domain-specific word embedding training along with measurement based on Relaxed Word Mover'sDistance to analyze the similarity of test steps. Our approach also includes a technique to combine hierarchical agglomerative clustering and K-means clustering post-refinement to derive high-quality and manually-adjustable clustering results. The evaluation results of our approach on a large industrial mobile app, WeChat, show that our approach can cluster the test steps with high accuracy, substantially reducing the number of clusters and thus reducing the downstream manual effort. In particular, compared with the baseline approach, our approach achieves 79.8\% improvement on cluster quality, reducing 65.9\% number of clusters, i.e., the number of test API methods to be implemented.},
   author = {Linyi Li and Zhenwen Li and Weijie Zhang and Jun Zhou and Pengcheng Wang and Jing Wu and Guanghua He and Xia Zeng and Yuetang Deng and Tao Xie},
   doi = {10.1145/3368089.3417067},
   booktitle = {ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   title = {Clustering test steps in natural language toward automating test automation},
   year = {2020}
}
@inproceedings{Jorgensen2024,
   abstract = {Genetic programming (GP) is a popular problem-solving and optimization technique. However, generating effective test cases for training and evaluating GP programs requires strong domain knowledge. Furthermore, GP programs often prematurely converge on local optima when given excessively difficult problems early in their training. Curriculum learning (CL) has been effective in addressing similar issues across different reinforcement learning (RL) domains, but it requires the manual generation of progressively difficult test cases as well as their careful scheduling. In this work, we leverage the domain knowledge and the strong generative abilities of large language models (LLMs) to generate effective test cases of increasing difficulties and schedule them according to various curricula. We show that by integrating a curriculum scheduler with LLM-generated test cases we can effectively train a GP agent player with environments-based curricula for a single-player game and opponent-based curricula for a multi-player game. Finally, we discuss the benefits and challenges of implementing this method for other problem domains.},
   author = {Steven Jorgensen and Giorgia Nadizar and Gloria Pietropolli and Luca Manzoni and Eric Medvet and Una-May O'Reilly and Erik Hemberg},
   city = {New York, NY, USA},
   doi = {10.1145/3638529.3654056},
   isbn = {9798400704949},
   booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
   keywords = {curriculum learning,large language models,linear GP},
   pages = {914-923},
   publisher = {Association for Computing Machinery},
   title = {Large Language Model-based Test Case Generation for GP Agents},
   url = {https://doi.org/10.1145/3638529.3654056},
   year = {2024}
}
@article{Yuan2024,
   abstract = {Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code. In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3\% more compilable tests and 18.7\% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLLama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.},
   author = {Zhiqiang Yuan and Mingwei Liu and Shiji Ding and Kaixin Wang and Yixuan Chen and Xin Peng and Yiling Lou},
   city = {New York, NY, USA},
   doi = {10.1145/3660783},
   issue = {FSE},
   journal = {Proc. ACM Softw. Eng.},
   keywords = {Large language model,Test generation,Unit testing},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {Evaluating and Improving ChatGPT for Unit Test Generation},
   volume = {1},
   url = {https://doi.org/10.1145/3660783},
   year = {2024}
}
@inproceedings{Yu2024,
   abstract = {Automated test generation can help developers craft high-quality software tests while mitigating the manual effort needed for writing test code. Despite significant research efforts in automated test generation for nearly 50 years, there is a lack of clarity about what practitioners expect from automated test generation tools and whether the existing research meets their needs. To address this issue, we follow a mixed-methods approach to gain insights into practitioners' expectations of automated test generation. We first conduct the qualitative analysis from semi-structured interviews with 13 professionals, followed by a quantitative survey of 339 practitioners from 46 countries across five continents. We then conduct a literature review of premier venue papers from 2022 to 2024 (in the last three years) and compare current research findings with practitioners' expectations. From this comparison, we outline future research directions for researchers to bridge the gap between automated test generation research and practitioners' expectations.},
   author = {Xiao Yu and Lei Liu and Xing Hu and Jacky Keung and Xin Xia and David Lo},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680386},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Empirical Study,Practitioners' Expectations,Test Generation},
   pages = {1618-1630},
   publisher = {Association for Computing Machinery},
   title = {Practitioners’ Expectations on Automated Test Generation},
   url = {https://doi.org/10.1145/3650212.3680386},
   year = {2024}
}
@inproceedings{Xue2024-1,
   abstract = {FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency.    In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.	Our prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin’s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18\% and an average of 20\%−110\% improvement on business scenario coverage, and up to 93.72\% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework’s practical applicability and efficiency, marking a significant advancement in FinTech software testing.},
   author = {Zhiyi Xue and Liangguo Li and Senyue Tian and Xiaohong Chen and Pingping Li and Liangyu Chen and Tingting Jiang and Min Zhang},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680388},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Software acceptance testing,fintech software,large language model,test case generation},
   pages = {1643-1655},
   publisher = {Association for Computing Machinery},
   title = {LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing},
   url = {https://doi.org/10.1145/3650212.3680388},
   year = {2024}
}
@inproceedings{Wang2024-1,
   abstract = {Large language models (LLMs) have behaved well in generating unit tests for Java projects. However, the performance for covering the complex focal methods within the projects is poor. Complex methods comprise many conditions and loops, requiring the test cases to be various enough to cover all lines and branches. However, existing test generation methods with LLMs provide the whole method-to-test to the LLM without assistance on input analysis. The LLM has difficulty inferring the test inputs to cover all conditions, resulting in missing lines and branches. To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice. Our method simplifies the analysis scope, making it easier for the LLM to cover more lines and branches in each slice. We build a dataset comprising complex focal methods collected from the projects used by existing state-of-the-art approaches. Our experiment results show that our method significantly outperforms current test case generation methods with LLMs and the typical SBST method Evosuite regarding both line and branch coverage scores.},
   author = {Zejun Wang and Kaibo Liu and Ge Li and Zhi Jin},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695501},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {AI for SE,large language model,program decomposition,program slicing,testing and analysis,unit test generation},
   pages = {1258-1268},
   publisher = {Association for Computing Machinery},
   title = {HITS: High-coverage LLM-based Unit Test Generation via Method Slicing},
   url = {https://doi.org/10.1145/3691620.3695501},
   year = {2024}
}
@inproceedings{Chen2024-3,
   abstract = {Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain. ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.},
   author = {Yinghao Chen and Zehao Hu and Chen Zhi and Junxiao Han and Shuiguang Deng and Jianwei Yin},
   city = {New York, NY, USA},
   doi = {10.1145/3663529.3663801},
   isbn = {9798400706585},
   booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
   keywords = {Automatic Unit Testing Generation,Large Language Models},
   pages = {572-576},
   publisher = {Association for Computing Machinery},
   title = {ChatUniTest: A Framework for LLM-Based Test Generation},
   url = {https://doi.org/10.1145/3663529.3663801},
   year = {2024}
}
@inproceedings{Wang2024-2,
   abstract = {Malicious Android applications often employ covert behaviors to exfiltrate sensitive data, thereby compromising user privacy. Traditional detection techniques predominantly utilize static analysis of the source code to detect such sensitive behaviors, yet they are frequently plagued by elevated false positive rates. While dynamic analysis methods offer greater precision, they contend with the challenge of limited coverage. This paper introduces LIReDroid, a hybrid testing approach that aims to replicate sensitive behaviors identified in static analysis call chains. LIReDroid firstly analyze the application’s static invocation chain. Then LIReDroid devises a prompt word model for the generation of test instructions and injection script code. Ultimately, sensitive API call chains are dynamically invoked through code injection, with their activation being meticulously recorded. We presented preliminary experimental results to substantiate the efficacy of LIReDroid. Given these results, we outline future research directions for LIReDroid.},
   author = {Yin Wang and Ming Fan and Xicheng Zhang and Jifei Shi and Zhaoyu Qiu and Haijun Wang and Ting Liu},
   city = {New York, NY, USA},
   doi = {10.1145/3671016.3671404},
   isbn = {9798400707056},
   booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
   keywords = {Android Application Security,Large Language Model,Sensitive Behavior Reproduction,Test Case Generation},
   pages = {81-84},
   publisher = {Association for Computing Machinery},
   title = {LIReDroid: LLM-Enhanced Test Case Generation for Static Sensitive Behavior Replication},
   url = {https://doi.org/10.1145/3671016.3671404},
   year = {2024}
}
@inproceedings{Boukhlif2024,
   abstract = {The need for effective and timely testing processes has become critical in the constantly changing field of software development. Large Language Models (LLMs) have demonstrated promise in automating test case creation, defect detection, and other software testing tasks through the use of the capabilities of machine/deep learning and natural language processing. This work explores the field of intelligent software testing, with a focus on the use of LLMs in this context. The purpose of this comparative study is to assess the corpus of research in the field in terms of used LLMs, how to interact with them, the use of fine-tuning, and prompt engineering, and explore the different technologies and testing types automated using LLMs. The findings of this study not only contribute to the growing body of knowledge on intelligent software testing but also guide fellow researchers and industry engineers in selecting the most suitable LLM for their specific testing needs.},
   author = {Mohamed Boukhlif and Nassim Kharmoum and Mohamed Hanine},
   city = {New York, NY, USA},
   doi = {10.1145/3659677.3659749},
   isbn = {9798400709296},
   booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
   keywords = {Comparative Study,Large Language Models,Natural Language Processing,Software Testing,Test Case Generation},
   publisher = {Association for Computing Machinery},
   title = {LLMs for Intelligent Software Testing: A Comparative Study},
   url = {https://doi.org/10.1145/3659677.3659749},
   year = {2024}
}
@inproceedings{Guilherme2023,
   abstract = {Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis.},
   author = {Vitor Guilherme and Auri Vincenzi},
   city = {New York, NY, USA},
   doi = {10.1145/3624032.3624035},
   isbn = {9798400716294},
   booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
   keywords = {automated test generation,coverage testing,experimental software engineering,mutation testing,software testing,testing tools},
   pages = {15-24},
   publisher = {Association for Computing Machinery},
   title = {An initial investigation of ChatGPT unit test generation capability},
   url = {https://doi.org/10.1145/3624032.3624035},
   year = {2023}
}
@inproceedings{Deljouyi2024,
   abstract = {Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques.Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles.In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.},
   author = {Amirhossein Deljouyi},
   city = {New York, NY, USA},
   doi = {10.1145/3639478.3639789},
   isbn = {9798400705021},
   booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
   keywords = {automatic test generation,carving and replaying,large language models,readability,understandability,unit testing},
   pages = {261-263},
   publisher = {Association for Computing Machinery},
   title = {Understandable Test Generation Through Capture/Replay and LLMs},
   url = {https://doi.org/10.1145/3639478.3639789},
   year = {2024}
}
@inproceedings{Alian2024,
   abstract = {Automated test generation for web forms has been a longstanding challenge, exacerbated by the intrinsic human-centric design of forms and their complex, device-agnostic structures. We introduce an innovative approach, called FormNexus, for automated web form test generation, which emphasizes deriving semantic insights from individual form elements and relations among them, utilizing textual content, DOM tree structures, and visual proximity. The insights gathered are transformed into a new conceptual graph, the Form Entity Relation Graph (FERG), which offers machine-friendly semantic information extraction. Leveraging LLMs, FormNexus adopts a feedback-driven mechanism for generating and refining input constraints based on real-time form submission responses. The culmination of this approach is a robust set of test cases, each produced by methodically invalidating constraints, ensuring comprehensive testing scenarios for web forms. This work bridges the existing gap in automated web form testing by intertwining the capabilities of LLMs with advanced semantic inference methods. Our evaluation demonstrates that FormNexus combined with GPT-4 achieves 89\% coverage in form submission states. This outcome significantly outstrips the performance of the best baseline model by a margin of 25\%.},
   author = {Parsa Alian and Noor Nashid and Mobina Shahbandeh and Ali Mesbah},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680332},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Large Language Models,Test Input Generation,Web Forms},
   pages = {932-944},
   publisher = {Association for Computing Machinery},
   title = {Semantic Constraint Inference for Web Form Test Generation},
   url = {https://doi.org/10.1145/3650212.3680332},
   year = {2024}
}
@article{Bnder2019,
   abstract = {The majority of users interacts with an application through its graphical user interface (GUI). To ensure high quality and expected behavior, those graphical user interfaces have to be tested thoroughly. Yet, creating graphical user interface test cases is considered expensive in comparison to unit or integration tests. In addition, test cases are perceived to be expensive to run and brittle, therefore causing a lot of false negative test results. Behavior-driven test case design addresses this challenges by bringing requirement specifications and test cases closer together. Although industry-proven tools map test specifications automatically, test methods making test scripts executable need to be implemented manually. The specification language Slang introduced by this paper generates automatically executable test cases from BDD-like feature descriptions that integrate low-fidelity prototypes in form of wireframesketcher models. To quantify the economic advantage of our approach an AB/BA crossover designed experiment was conducted. The experiment showed that creating automatically executable test cases utilizing Slang takes 63\% less time compared to the industry-proven tool JBehave. In addition to presenting the experiment's results, the paper elaborates on first experience from applying the approach in a large Swiss bank. The findings of our experiments are supported by results from applying our approach in real-world scenarios. In addition, experiment as well as case study participants appreciated the sophisticated editor support of Slang.},
   author = {Hendrik Bünder and Herbert Kuchen},
   city = {New York, NY, USA},
   doi = {10.1145/3357385.3357386},
   issn = {1559-6915},
   issue = {2},
   journal = {SIGAPP Appl. Comput. Rev.},
   keywords = {GUI test case generation,behavior-driven development,case study,domain-specific language,model-driven engineering},
   month = {8},
   pages = {5-17},
   publisher = {Association for Computing Machinery},
   title = {Towards behavior-driven graphical user interface testing},
   volume = {19},
   url = {https://doi.org/10.1145/3357385.3357386},
   year = {2019}
}
@inproceedings{Okanovi2020,
   abstract = {Even though load testing is an established technique to assess load-related quality properties of software systems, it is applied only seldom and with questionable results. Indeed, configuring, executing, and interpreting results of a load test require high effort and expertise. Since chatbots have shown promising results for interactively supporting complex tasks in various domains (including software engineering), we hypothesize that chatbots can provide developers suitable support for load testing. In this paper, we present PerformoBot, our chatbot for configuring and running load tests. In a natural language conversation, PerformoBot guides developers through the process of properly specifying the parameters of a load test, which is then automatically executed by PerformoBot using a state-of-the-art load testing tool. After the execution, PerformoBot provides developers a report that answers the respective concern. We report on results of a user study that involved 47 participants, in which we assessed our tool's acceptance and effectiveness. We found that participants in the study, particularly those with a lower level of expertise in performance engineering, had a mostly positive view of PerformoBot.},
   author = {Dušan Okanović and Samuel Beck and Lasse Merz and Christoph Zorn and Leonel Merino and André van Hoorn and Fabian Beck},
   city = {New York, NY, USA},
   doi = {10.1145/3358960.3375792},
   isbn = {9781450369916},
   booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
   keywords = {chatbots,reporting,software performance},
   pages = {120-129},
   publisher = {Association for Computing Machinery},
   title = {Can a Chatbot Support Software Engineers with Load Testing? Approach and Experiences},
   url = {https://doi.org/10.1145/3358960.3375792},
   year = {2020}
}
@inproceedings{Nguyen2021,
   abstract = {Fuzzing or fuzz testing is an established technique that aims to discover unexpected program behavior (e.g., bugs, security vulnerabilities, or crashes) by feeding automatically generated data into a program under test. However, the application of fuzzing to test Model-Driven Software Engineering (MDSE) tools is still limited because of the difficulty of existing fuzzers to provide structured, well-typed inputs, namely models that conform to typing and consistency constraints induced by a given meta-model and underlying modeling framework. By drawing from recent advances on both fuzz testing and automated model generation, we present three different approaches for fuzzing MDSE tools: A graph grammar-based fuzzer and two variants of a coverage-guided mutation-based fuzzer working with different sets of model mutation operators. Our evaluation on a set of real-world MDSE tools shows that our approaches can outperform both standard fuzzers and model generators w.r.t. their fuzzing capabilities. Moreover, we found that each of our approaches comes with its own strengths and weaknesses in terms of fault finding capabilities and the ability to cover different aspects of the system under test. Thus the approaches complement each other, forming a fuzzer suite for testing MDSE tools.},
   author = {Hoang Lam Nguyen and Nebras Nassar and Timo Kehrer and Lars Grunske},
   city = {New York, NY, USA},
   doi = {10.1145/3324884.3416668},
   isbn = {9781450367684},
   booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {automated model generation,eclipse modeling framework,fuzzing,model-driven software engineering,modeling tools},
   pages = {1103-1115},
   publisher = {Association for Computing Machinery},
   title = {MoFuzz: a fuzzer suite for testing model-driven software engineering tools},
   url = {https://doi.org/10.1145/3324884.3416668},
   year = {2021}
}
@inproceedings{Roy2021,
   abstract = {Automated test case generation tools have been successfully proposed to reduce the amount of human and infrastructure resources required to write and run test cases. However, recent studies demonstrate that the readability of generated tests is very limited due to (i) uninformative identifiers and (ii) lack of proper documentation. Prior studies proposed techniques to improve test readability by either generating natural language summaries or meaningful methods names. While these approaches are shown to improve test readability, they are also affected by two limitations: (1) generated summaries are often perceived as too verbose and redundant by developers, and (2) readable tests require both proper method names but also meaningful identifiers (within-method readability).In this work, we combine template based methods and Deep Learning (DL) approaches to automatically generate test case scenarios (elicited from natural language patterns of test case statements) as well as to train DL models on path-based representations of source code to generate meaningful identifier names. Our approach, called DeepTC-Enhancer, recommends documentation and identifier names with the ultimate goal of enhancing readability of automatically generated test cases.An empirical evaluation with 36 external and internal developers shows that (1) DeepTC-Enhancer outperforms significantly the baseline approach for generating summaries and performs equally with the baseline approach for test case renaming, (2) the transformation proposed by DeepTC-Enhancer results in a significant increase in readability of automatically generated test cases, and (3) there is a significant difference in the feature preferences between external and internal developers.},
   author = {Devjeet Roy and Ziyi Zhang and Maggie Ma and Venera Arnaoudova and Annibale Panichella and Sebastiano Panichella and Danielle Gonzalez and Mehdi Mirakhorli},
   city = {New York, NY, USA},
   doi = {10.1145/3324884.3416622},
   isbn = {9781450367684},
   booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
   pages = {287-298},
   publisher = {Association for Computing Machinery},
   title = {DeepTC-enhancer: improving the readability of automatically generated tests},
   url = {https://doi.org/10.1145/3324884.3416622},
   year = {2021}
}
@inproceedings{Gupta2020,
   abstract = {Machine translation software has become heavily integrated into our daily lives due to the recent improvement in the performance of deep neural networks. However, machine translation software has been shown to regularly return erroneous translations, which can lead to harmful consequences such as economic loss and political conflicts. Additionally, due to the complexity of the underlying neural models, testing machine translation systems presents new challenges. To address this problem, we introduce a novel methodology called PatInv. The main intuition behind PatInv is that sentences with different meanings should not have the same translation. Under this general idea, we provide two realizations of PatInv that given an arbitrary sentence, generate syntactically similar but semantically different sentences by: (1) replacing one word in the sentence using a masked language model or (2) removing one word or phrase from the sentence based on its constituency structure. We then test whether the returned translations are the same for the original and modified sentences. We have applied PatInv to test Google Translate and Bing Microsoft Translator using 200 English sentences. Two language settings are considered: English-Hindi (En-Hi) and English-Chinese (En-Zh). The results show that PatInv can accurately find 308 erroneous translations in Google Translate and 223 erroneous translations in Bing Microsoft Translator, most of which cannot be found by the state-of-the-art approaches.},
   author = {Shashij Gupta and Pinjia He and Clara Meister and Zhendong Su},
   city = {New York, NY, USA},
   doi = {10.1145/3368089.3409756},
   isbn = {9781450370431},
   booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Machine translation,Pathological Invariance,Testing},
   pages = {863-875},
   publisher = {Association for Computing Machinery},
   title = {Machine translation testing via pathological invariance},
   url = {https://doi.org/10.1145/3368089.3409756},
   year = {2020}
}
@article{Smolka2019,
   abstract = {Guarded Kleene Algebra with Tests (GKAT) is a variation on Kleene Algebra with Tests (KAT) that arises by restricting the union (+) and iteration (*) operations from KAT to predicate-guarded versions. We develop the (co)algebraic theory of GKAT and show how it can be efficiently used to reason about imperative programs. In contrast to KAT, whose equational theory is PSPACE-complete, we show that the equational theory of GKAT is (almost) linear time. We also provide a full Kleene theorem and prove completeness for an analogue of Salomaa’s axiomatization of Kleene Algebra.},
   author = {Steffen Smolka and Nate Foster and Justin Hsu and Tobias Kappé and Dexter Kozen and Alexandra Silva},
   city = {New York, NY, USA},
   doi = {10.1145/3371129},
   issue = {POPL},
   journal = {Proc. ACM Program. Lang.},
   keywords = {Kleene algebra with Tests,coalgebra,guarded automata,program equivalence,program schemes,uninterpreted programs},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {Guarded Kleene algebra with tests: verification of uninterpreted programs in nearly linear time},
   volume = {4},
   url = {https://doi.org/10.1145/3371129},
   year = {2019}
}
@inproceedings{He2020,
   abstract = {In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored.To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of "similar" source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5\% and 70\% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.},
   author = {Pinjia He and Clara Meister and Zhendong Su},
   city = {New York, NY, USA},
   doi = {10.1145/3377811.3380339},
   isbn = {9781450371216},
   booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
   keywords = {machine translation,metamorphic testing,structural invariance},
   pages = {961-973},
   publisher = {Association for Computing Machinery},
   title = {Structure-invariant testing for machine translation},
   url = {https://doi.org/10.1145/3377811.3380339},
   year = {2020}
}

@inproceedings{Tang2020,
   abstract = {Development of Internet of Things (IoT) brings a variety of IoT applications that involve housing, navigation, payment, and healthcare. Since IoT applications play an important role in our lives, security is critical to these applications and must be guaranteed. In order to realize this, the paper proposes a pre-execution verification method for downloaded IoT applications, which meets security and safety requirements of users using model checking. The model checking requires special models for verification, which is difficult to describe for developers. So we introduce a domain-specific modeling language (DSML) to describe IoT application and a generator from the DSML into the model to pre-execution verification and execution. Also, as a case study, we provide a study of our method used in a smart house application, which is one of the most representative examples in IoT applications.},
   author = {Wentao Tang and Hao Feng and Kenji Hisazumi and Akira Fukuda},
   city = {New York, NY, USA},
   doi = {10.1145/3388176.3388211},
   isbn = {9781450377256},
   booktitle = {Proceedings of the 3rd International Conference on Information Science and Systems},
   keywords = {Code Generating,IoT; DSML,Lustre,Model-Checking,Smart House},
   pages = {166-170},
   publisher = {Association for Computing Machinery},
   title = {A Verification Method for Security and Safety of IoT Applications Through DSM Language and Lustre},
   url = {https://doi.org/10.1145/3388176.3388211},
   year = {2020}
}
@inproceedings{Liu2020,
   abstract = {Security is unarguably the most serious concern for Web applications, to which SQL injection (SQLi) attack is one of the most devastating attacks. Automatically testing SQLi vulnerabilities is of ultimate importance, yet is unfortunately far from trivial to implement. This is because the existence of a huge, or potentially infinite, number of variants and semantic possibilities of SQL leading to SQLi attacks on various Web applications. In this paper, we propose a deep natural language processing based tool, dubbed DeepSQLi, to generate test cases for detecting SQLi vulnerabilities. Through adopting deep learning based neural language model and sequence of words prediction, DeepSQLi is equipped with the ability to learn the semantic knowledge embedded in SQLi attacks, allowing it to translate user inputs (or a test case) into a new test case, which is se- mantically related and potentially more sophisticated. Experiments are conducted to compare DeepSQLi with SQLmap, a state-of-the-art SQLi testing automation tool, on six real-world Web applications that are of different scales, characteristics and domains. Empirical results demonstrate the effectiveness and the remarkable superiority of DeepSQLi over SQLmap, such that more SQLi vulnerabilities can be identified by using a less number of test cases, whilst running much faster.},
   author = {Muyang Liu and Ke Li and Tao Chen},
   city = {New York, NY, USA},
   doi = {10.1145/3395363.3397375},
   isbn = {9781450380089},
   booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {SQL injection,Web security,deep learning,natural language processing,test case generation},
   pages = {286-297},
   publisher = {Association for Computing Machinery},
   title = {DeepSQLi: deep semantic learning for testing SQL injection},
   url = {https://doi.org/10.1145/3395363.3397375},
   year = {2020}
}
@inproceedings{Rabin2021,
   abstract = {The correctness of compilers is instrumental in the safety and reliability of other software systems, as bugs in compilers can produce executables that do not reflect the intent of programmers. Such errors are difficult to identify and debug. Random test program generators are commonly used in testing compilers, and they have been effective in uncovering bugs. However, the problem of guiding these test generators to produce test programs that are more likely to find bugs remains challenging.In this paper, we use the code snippets in the bug reports to guide the test generation. The main idea of this work is to extract insights from the bug reports about the language features that are more prone to inadequate implementation and using the insights to guide the test generators. We use the GCC C compiler to evaluate the effectiveness of this approach. In particular, we first cluster the test programs in the GCC bugs reports based on their features. We then use the centroids of the clusters to compute configurations for Csmith, a popular test generator for C compilers. We evaluated this approach on eight versions of GCC and found that our approach provides higher coverage and triggers more miscompilation failures than the state-of-the-art test generation techniques for GCC.},
   author = {Md Rafiqul Islam Rabin and Mohammad Amin Alipour},
   city = {New York, NY, USA},
   doi = {10.1145/3412841.3442047},
   isbn = {9781450381048},
   booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
   keywords = {clustering,compiler,evaluation,testing},
   pages = {1750-1758},
   publisher = {Association for Computing Machinery},
   title = {Configuring test generators using bug reports: a case study of GCC compiler and Csmith},
   url = {https://doi.org/10.1145/3412841.3442047},
   year = {2021}
}
@inproceedings{Ye2021,
   abstract = {JavaScript (JS) is a popular, platform-independent programming language. To ensure the interoperability of JS programs across different platforms, the implementation of a JS engine should conform to the ECMAScript standard. However, doing so is challenging as there are many subtle definitions of API behaviors, and the definitions keep evolving.  We present COMFORT, a new compiler fuzzing framework for detecting JS engine bugs and behaviors that deviate from the ECMAScript standard. COMFORT leverages the recent advance in deep learning-based language models to automatically generate JS test code. As a departure from prior fuzzers, COMFORT utilizes the well-structured ECMAScript specifications to automatically generate test data along with the test programs to expose bugs that could be overlooked by the developers or manually written test cases. COMFORT then applies differential testing methodologies on the generated test cases to expose standard conformance bugs. We apply COMFORT to ten mainstream JS engines. In 200 hours of automated concurrent testing runs, we discover bugs in all tested JS engines. We had identified 158 unique JS engine bugs, of which 129 have been verified, and 115 have already been fixed by the developers. Furthermore, 21 of the COMFORT-generated test cases have been added to Test262, the official ECMAScript conformance test suite.},
   author = {Guixin Ye and Zhanyong Tang and Shin Hwei Tan and Songfang Huang and Dingyi Fang and Xiaoyang Sun and Lizhong Bian and Haibo Wang and Zheng Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3453483.3454054},
   isbn = {9781450383912},
   booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
   keywords = {Compiler fuzzing,Conformance bugs,Deep learning,Differential testing,JavaScript},
   pages = {435-450},
   publisher = {Association for Computing Machinery},
   title = {Automated conformance testing for JavaScript engines via deep compiler fuzzing},
   url = {https://doi.org/10.1145/3453483.3454054},
   year = {2021}
}
@inproceedings{Mariani2021,
   abstract = {GUI testing is an important but expensive activity. Recently, research on test reuse approaches for Android applications produced interesting results. Test reuse approaches automatically migrate human-designed GUI tests from a source app to a target app that shares similar functionalities. They achieve this by exploiting semantic similarity among textual information of GUI widgets. Semantic matching of GUI events plays a crucial role in these approaches. In this paper, we present the first empirical study on semantic matching of GUI events. Our study involves 253 configurations of the semantic matching, 337 unique queries, and 8,099 distinct GUI events. We report several key findings that indicate how to improve semantic matching of test reuse approaches, propose SemFinder a novel semantic matching algorithm that outperforms existing solutions, and identify several interesting research directions.},
   author = {Leonardo Mariani and Ali Mohebbi and Mauro Pezzè and Valerio Terragni},
   city = {New York, NY, USA},
   doi = {10.1145/3460319.3464827},
   isbn = {9781450384599},
   booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Android applications,GUI testing,NLP,mobile testing,test reuse,word embedding},
   pages = {177-190},
   publisher = {Association for Computing Machinery},
   title = {Semantic matching of GUI events for test reuse: are we there yet?},
   url = {https://doi.org/10.1145/3460319.3464827},
   year = {2021}
}
@inproceedings{Liu2021,
   abstract = {With the tremendous advancement of recurrent neural networks(RNN), dialogue systems have achieved significant development. Many RNN-driven dialogue systems, such as Siri, Google Home, and Alexa, have been deployed to assist various tasks. However, accompanying this outstanding performance, RNN-driven dialogue systems, which are essentially a kind of software, could also produce erroneous behaviors and result in massive losses. Meanwhile, the complexity and intractability of RNN models that power the dialogue systems make their testing challenging. In this paper, we design and implement DialTest, the first RNN-driven dialogue system testing tool. DialTest employs a series of transformation operators to make realistic changes on seed data while preserving their oracle information properly. To improve the efficiency of detecting faults, DialTest further adopts Gini impurity to guide the test generation process. We conduct extensive experiments to validate DialTest. We first experiment it on two fundamental tasks, i.e., intent detection and slot filling, of natural language understanding. The experiment results show that DialTest can effectively detect hundreds of erroneous behaviors for different RNN-driven natural language understanding (NLU) modules of dialogue systems and improve their accuracy via retraining with the generated data. Further, we conduct a case study on an industrial dialogue system to investigate the performance of DialTest under the real usage scenario. The study shows DialTest can detect errors and improve the robustness of RNN-driven dialogue systems effectively.},
   author = {Zixi Liu and Yang Feng and Zhenyu Chen},
   city = {New York, NY, USA},
   doi = {10.1145/3460319.3464829},
   isbn = {9781450384599},
   booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {automated testing,deep learning testing,dialog system testing},
   pages = {115-126},
   publisher = {Association for Computing Machinery},
   title = {DialTest: automated testing for recurrent-neural-network-driven dialogue systems},
   url = {https://doi.org/10.1145/3460319.3464829},
   year = {2021}
}
@inproceedings{Einarsdttir2021,
   abstract = {We present RoughSpec, a template-based extension of the theory exploration tool QuickSpec. QuickSpec uses testing to automatically discover equational properties about functions in a Haskell program. These properties can help the user understand the program or be used as a source of possible lemmas in proofs of the program’s correctness. In RoughSpec, the user supplies templates, which describe families of laws such as associativity and distributivity, and we only consider properties that match the templates. This restriction limits the search space and ensures that only relevant properties are discovered. In this way, we sacrifice broad search for more direction towards desirable property patterns, which makes theory exploration tractable and scalable. We also combine RoughSpec with QuickSpec, using QuickSpec to perform a complete search for smaller term sizes, while using templates for larger, more complex properties, in order to leverage the strengths of both systems.},
   author = {Sólrún Halla Einarsdóttir and Nicholas Smallbone and Moa Johansson},
   city = {New York, NY, USA},
   doi = {10.1145/3462172.3462192},
   isbn = {9781450389631},
   booktitle = {Proceedings of the 32nd Symposium on Implementation and Application of Functional Languages},
   keywords = {Algebraic properties,Functional programming,Program understanding,Property-based testing,QuickSpec,Theory exploration},
   pages = {67-78},
   publisher = {Association for Computing Machinery},
   title = {Template-based Theory Exploration: Discovering Properties of Functional Programs by Testing},
   url = {https://doi.org/10.1145/3462172.3462192},
   year = {2021}
}
@inproceedings{Chen2021,
   abstract = {Machine Reading Comprehension (MRC) in Natural Language Processing has seen great progress recently. But almost all the current MRC software is validated with a reference-based method, which requires well-annotated labels for test cases and tests the software by checking the consistency between the labels and the outputs. However, labeling test cases of MRC could be very costly due to their complexity, which makes reference-based validation hard to be extensible and sufficient. Furthermore, solely checking the consistency and measuring the overall score may not be sensible and flexible for assessing the language understanding capability. In this paper, we propose a property-based validation method for MRC software with Metamorphic Testing to supplement the reference-based validation. It does not refer to the labels and hence can make much data available for testing. Besides, it validates MRC software against various linguistic properties to give a specific and in-depth picture on linguistic capabilities of MRC software. Comprehensive experimental results show that our method can successfully reveal violations to the target linguistic properties without the labels. Moreover, it can reveal problems that have been concealed by the traditional validation. Comparison according to the properties provides deeper and more concrete ideas about different language understanding capabilities of the MRC software.},
   author = {Songqiang Chen and Shuo Jin and Xiaoyuan Xie},
   city = {New York, NY, USA},
   doi = {10.1145/3468264.3468569},
   isbn = {9781450385626},
   booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {language understanding capability,machine reading comprehension,metamorphic relation,property-based validation},
   pages = {590-602},
   publisher = {Association for Computing Machinery},
   title = {Validation on machine reading comprehension software without annotated labels: a property-based method},
   url = {https://doi.org/10.1145/3468264.3468569},
   year = {2021}
}
@inproceedings{Zafar2021,
   abstract = {Model-based testing (MBT) has been previously used to validate embedded systems. However, (i) creation of a model conforming to the behavioural aspects of an embedded system, (ii) generation of executable test scripts and (iii) assessment of test verdict, re-quires a systematic process. In this paper, we have presented a three-phase tool-supported MBT workflow for the testing of an embedded system, that spans from requirements specification to test verdict assessment. The workflow starts with a simplistic, yet practical, application of a Domain-Specific Language (DSL) based on Gherkin-like style, which allows the requirements engineer to specify requirements and to extract information about model elements(i.e. states and transitions). This is done to assist the graphical modelling of the complete system under test (SUT). Later stages of the workflow generates an executable test script that runs on a domain-specific simulation platform. We have evaluated this tool-supported workflow by specifying the requirements, extracting information from the DSL and developing a model of a subsystem of the train control management system developed at Alstom Transport AB in Sweden. The C# test script generated from the SUT model is successfully executed at the Software-in-the-Loop (SIL) execution platform and test verdicts are visualized as a sequence of passed and failed test steps.},
   author = {Muhammad Nouman Zafar and Wasif Afzal and Eduard Enoiu},
   city = {New York, NY, USA},
   doi = {10.1145/3472672.3473956},
   isbn = {9781450386234},
   booktitle = {Proceedings of the 12th International Workshop on Automating TEST Case Design, Selection, and Evaluation},
   keywords = {Domain-Specific Language,Model-Based Testing,Software-in-the-Loop},
   pages = {33-40},
   publisher = {Association for Computing Machinery},
   title = {Towards a workflow for model-based testing of embedded systems},
   url = {https://doi.org/10.1145/3472672.3473956},
   year = {2021}
}
@inproceedings{Lin2022,
   abstract = {Ad hoc dataset retrieval is a trending topic in IR research. Methods and systems are evolving from metadata-based to content-based ones which exploit the data itself for improving retrieval accuracy but thus far lack a specialized test collection. In this paper, we build and release the first test collection for ad hoc content-based dataset retrieval, where content-oriented dataset queries and content-based relevance judgments are annotated by human experts who are assisted with a dashboard designed specifically for comprehensively and conveniently browsing both the metadata and data of a dataset. We conduct extensive experiments on the test collection to analyze its difficulty and provide insights into the underlying task.},
   author = {Tengteng Lin and Qiaosheng Chen and Gong Cheng and Ahmet Soylu and Basil Ell and Ruoqi Zhao and Qing Shi and Xiaxia Wang and Yu Gu and Evgeny Kharlamov},
   city = {New York, NY, USA},
   doi = {10.1145/3477495.3531729},
   isbn = {9781450387323},
   booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {ad hoc dataset retrieval,dataset browsing,dataset search,rdf,test collection},
   pages = {2981-2991},
   publisher = {Association for Computing Machinery},
   title = {ACORDAR: A Test Collection for Ad Hoc Content-Based (RDF) Dataset Retrieval},
   url = {https://doi.org/10.1145/3477495.3531729},
   year = {2022}
}
@article{Park2021,
   abstract = {We propose Generative Type-Aware Mutation, an effective approach for testing SMT solvers. The key idea is to realize generation through the mutation of expressions rooted with parametric operators from the SMT-LIB specification. Generative Type-Aware Mutation is a hybrid of mutation-based and grammar-based fuzzing and features an infinite mutation space—overcoming a major limitation of OpFuzz, the state-of-the-art fuzzer for SMT solvers. We have realized Generative Type-Aware Mutation in a practical SMT solver bug hunting tool, TypeFuzz. During our testing period with TypeFuzz, we reported over 237 bugs in the state-of-the-art SMT solvers Z3 and CVC4. Among these, 189 bugs were confirmed and 176 bugs were fixed. Most notably, we found 18 soundness bugs in CVC4’s default mode alone. Several of them were two years latent (7/18). CVC4 has been proved to be a very stable SMT solver and has resisted several fuzzing campaigns.},
   author = {Jiwon Park and Dominik Winterer and Chengyu Zhang and Zhendong Su},
   city = {New York, NY, USA},
   doi = {10.1145/3485529},
   issue = {OOPSLA},
   journal = {Proc. ACM Program. Lang.},
   keywords = {Fuzz testing,Generative type-aware mutation,SMT solvers},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {Generative type-aware mutation for testing SMT solvers},
   volume = {5},
   url = {https://doi.org/10.1145/3485529},
   year = {2021}
}
@inproceedings{Khan2021,
   abstract = {Mobile crowdsourcing services (MCS), enable fast and economical data acquisition at scale and find applications in a variety of domains. Prior work has shown that Foursquare and Waze (a location-based and a navigation MCS) are vulnerable to different kinds of data poisoning attacks. Such attacks can be upsetting and even dangerous especially when they are used to inject improper inputs to mislead users. However, to date, there is no comprehensive study on the extent of improper input validation (IIV) vulnerabilities and the feasibility of their exploits in MCSs across domains. In this work, we leverage the fact that MCS interface with their participants through mobile apps to design tools and new methodologies embodied in an end-to-end feedback-driven analysis framework which we use to study 10 popular and previously unexplored services in five different domains. Using our framework we send tens of thousands of API requests with automatically generated input values to characterize their IIV attack surface. Alarmingly, we found that most of them (8/10) suffer from grave IIV vulnerabilities which allow an adversary to launch data poisoning attacks at scale: 7400 spoofed API requests were successful in faking online posts for robberies, gunshots, and other dangerous incidents, faking fitness activities with supernatural speeds and distances among many others. Lastly, we discuss easy to implement and deploy mitigation strategies which can greatly reduce the IIV attack surface and argue for their use as a necessary complementary measure working toward trustworthy mobile crowdsourcing services.},
   author = {Sojhal Ismail Khan and Dominika C Woszczyk and Chengzeng You and Soteris Demetriou and Muhammad Naveed},
   city = {New York, NY, USA},
   doi = {10.1145/3485832.3485888},
   isbn = {9781450385794},
   booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
   keywords = {api fuzzing,crowdsourcing,data-poisoning,real-time},
   pages = {944-956},
   publisher = {Association for Computing Machinery},
   title = {Characterizing Improper Input Validation Vulnerabilities of Mobile Crowdsourcing Services},
   url = {https://doi.org/10.1145/3485832.3485888},
   year = {2021}
}
@inproceedings{Wan2022,
   abstract = {An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API.This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically generate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evaluation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many previously unknown bugs.},
   author = {Chengcheng Wan and Shicheng Liu and Sophie Xie and Yifan Liu and Henry Hoffmann and Michael Maire and Shan Lu},
   city = {New York, NY, USA},
   doi = {10.1145/3510003.3510068},
   isbn = {9781450392211},
   booktitle = {Proceedings of the 44th International Conference on Software Engineering},
   keywords = {machine learning,machine learning API,software testing},
   pages = {212-224},
   publisher = {Association for Computing Machinery},
   title = {Automated testing of software that uses machine learning APIs},
   url = {https://doi.org/10.1145/3510003.3510068},
   year = {2022}
}
@inproceedings{First2022,
   abstract = {Formally verified correctness is one of the most desirable properties of software systems. But despite great progress made via interactive theorem provers, such as Coq, writing proof scripts for verification remains one of the most effort-intensive (and often prohibitively difficult) software development activities. Recent work has created tools that automatically synthesize proofs or proof scripts. For example, CoqHammer can prove 26.6\% of theorems completely automatically by reasoning using precomputed facts, while TacTok and ASTactic, which use machine learning to model proof scripts and then perform biased search through the proof-script space, can prove 12.9\% and 12.3\% of the theorems, respectively. Further, these three tools are highly complementary; together, they can prove 30.4\% of the theorems fully automatically. Our key insight is that control over the learning process can produce a diverse set of models, and that, due to the unique nature of proof synthesis (the existence of the theorem prover, an oracle that infallibly judges a proof's correctness), this diversity can significantly improve these tools' proving power. Accordingly, we develop Diva, which uses a diverse set of models with TacTok's and ASTactic's search mechanism to prove 21.7\% of the theorems. That is, Diva proves 68\% more theorems than TacTok and 77\% more than ASTactic. Complementary to CoqHammer, Diva proves 781 theorems (27\% added value) that CoqHammer does not, and 364 theorems no existing tool has proved automatically. Together with CoqHammer, Diva proves 33.8\% of the theorems, the largest fraction to date. We explore nine dimensions for learning diverse models, and identify which dimensions lead to the most useful diversity. Further, we develop an optimization to speed up Diva's execution by 40X. Our study introduces a completely new idea for using diversity in machine learning to improve the power of state-of-the-art proof-script synthesis techniques, and empirically demonstrates that the improvement is significant on a dataset of 68K theorems from 122 open-source software projects.},
   author = {Emily First and Yuriy Brun},
   city = {New York, NY, USA},
   doi = {10.1145/3510003.3510138},
   isbn = {9781450392211},
   booktitle = {Proceedings of the 44th International Conference on Software Engineering},
   keywords = {automated formal verification,coq,interactive proof assistants,language models,proof synthesis},
   pages = {749-761},
   publisher = {Association for Computing Machinery},
   title = {Diversity-driven automated formal verification},
   url = {https://doi.org/10.1145/3510003.3510138},
   year = {2022}
}
@inproceedings{Viggiato2022,
   abstract = {Despite the recent advancements in test automation, testing often remains a manual, and costly, activity in many industries. Manual test cases, often described only in natural language, consist of one or more test steps, which are instructions that must be performed to achieve the testing objective. Having different employees specifying test cases might result in redundant, unclear, or incomplete test cases. Manually reviewing and validating newly-specified test cases is time-consuming and becomes impractical in a scenario with a large test suite. Therefore, in this paper, we propose an automated framework to automatically analyze test cases that are specified in natural language and provide actionable recommendations on how to improve the test cases. Our framework consists of configurable components and modules for analysis, which are capable of recommending improvements to the following: (1) the terminology of a new test case through language modeling, (2) potentially missing test steps for a new test case through frequent itemset and association rule mining, and (3) recommendation of similar test cases that already exist in the test suite through text embedding and clustering. We thoroughly evaluated the three modules on data from our industry partner. Our framework can provide actionable recommendations, which is an important challenge given the widespread occurrence of test cases that are described only in natural language in the software industry (in particular, the game industry).},
   author = {Markos Viggiato and Dale Paas and Chris Buzon and Cor-Paul Bezemer},
   city = {New York, NY, USA},
   doi = {10.1145/3510457.3513045},
   isbn = {9781450392266},
   booktitle = {Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice},
   keywords = {association rules,clustering,game testing,language modeling,natural language processing},
   pages = {311-320},
   publisher = {Association for Computing Machinery},
   title = {Using natural language processing techniques to improve manual test case descriptions},
   url = {https://doi.org/10.1145/3510457.3513045},
   year = {2022}
}
@article{Tian2022,
   abstract = {How do we know a generated patch is correct? This is a key challenging question that automated program repair (APR) systems struggle to address given the incompleteness of available test suites. Our intuition is that we can triage correct patches by checking whether each generated patch implements code changes (i.e., behavior) that are relevant to the bug it addresses. Such a bug is commonly specified by a failing test case. Towards predicting patch correctness in APR, we propose a novel yet simple hypothesis on how the link between the patch behavior and failing test specifications can be drawn: similar failing test cases should require similar patches. We then propose BATS, an unsupervised learning-based approach to predict patch correctness by checking patch Behavior Against failing Test Specification. BATS exploits deep representation learning models for code and patches: For a given failing test case, the yielded embedding is used to compute similarity metrics in the search for historical similar test cases to identify the associated applied patches, which are then used as a proxy for assessing the correctness of the APR-generated patches. Experimentally, we first validate our hypothesis by assessing whether ground-truth developer patches cluster together in the same way that their associated failing test cases are clustered. Then, after collecting a large dataset of 1,278 plausible patches (written by developers or generated by 32 APR tools), we use BATS to predict correct patches: BATS achieves AUC between 0.557 to 0.718 and recall between 0.562 and 0.854 in identifying correct patches. Our approach outperforms state-of-the-art techniques for identifying correct patches without the need for large labeled patch datasets—as is the case with machine learning-based approaches. While BATS is constrained by the availability of similar test cases, we show that it can still be complementary to existing approaches: When combined with a recent approach that relies on supervised learning, BATS improves the overall recall in detecting correct patches. We finally show that BATS is complementary to the state-of-the-art PATCH-SIM dynamic approach for identifying correct patches generated by APR tools.},
   author = {Haoye Tian and Yinghua Li and Weiguo Pian and Abdoul Kader Kaboré and Kui Liu and Andrew Habib and Jacques Klein and Tegawendé F Bissyandé},
   city = {New York, NY, USA},
   doi = {10.1145/3511096},
   issn = {1049-331X},
   issue = {4},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Program repair,patch correctness,patch semantics,test behavior},
   month = {8},
   publisher = {Association for Computing Machinery},
   title = {Predicting Patch Correctness Based on the Similarity of Failing Test Cases},
   volume = {31},
   url = {https://doi.org/10.1145/3511096},
   year = {2022}
}
@inproceedings{Tufano2022,
   abstract = {Unit testing represents the foundational basis of the software testing pyramid, beneath integration and end-to-end testing. Automated software testing researchers have proposed a variety of techniques to assist developers in this time-consuming task.In this paper we present an approach to support developers in writing unit test cases by generating accurate and useful assert statements. Our approach is based on a state-of-the-art transformer model initially pretrained on an English textual corpus. This semantically rich model is then trained in a semi-supervised fashion on a large corpus of source code. Finally, we finetune this model on the task of generating assert statements for unit tests.The resulting model is able to generate accurate assert statements for a given method under test. In our empirical evaluation, the model was able to predict the exact assert statements written by developers in 62\% of the cases in the first attempt. The results show 80\% relative improvement for top-1 accuracy over the previous RNN-based approach in the literature, as well as 33\% improvement over the recent Transformer-based T5 approach. We also show the substantial impact of the pretraining process on the performances of our model, as well as comparing it with assert auto-completion task. Finally, we demonstrate how our approach can be used to augment EvoSuite test cases, with additional asserts leading to improved test coverage.},
   author = {Michele Tufano and Dawn Drain and Alexey Svyatkovskiy and Neel Sundaresan},
   city = {New York, NY, USA},
   doi = {10.1145/3524481.3527220},
   isbn = {9781450392860},
   booktitle = {Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test},
   keywords = {neural networks,software testing,unit test},
   pages = {54-64},
   publisher = {Association for Computing Machinery},
   title = {Generating accurate assert statements for unit test cases using pretrained transformers},
   url = {https://doi.org/10.1145/3524481.3527220},
   year = {2022}
}
@inproceedings{Liu2022,
   abstract = {End-to-end test cases that exercise the application under test via its user interface (UI) are known to be hard for developers to read and understand; consequently, diagnosing failures in these tests and maintaining them can be tedious. Techniques for computing natural-language descriptions of test cases can help increase test readability. However, so far, such techniques have been developed for unit test cases; they are not applicable to end-to-end test cases.In this paper, we focus on the problem of computing natural-language labels for the steps of end-to-end UI test cases for web applications. We present two techniques that apply natural-language processing to information available in the browser document object model (DOM). The first technique is an instance of a supervised approach in which labeling-relevant DOM attributes are ranked via manual analysis and fed into label computation. However, supervised approach requires a training dataset. So we propose the second technique, which is unsupervised: it leverages probabilistic context-free grammar learning to compute dominant DOM attributes automatically. We implemented these techniques, along with two simpler baseline techniques, in a tool called CrawLabel (available as a plugin to Crawljax, a state-of-the-art UI test-generation tool for web applications) and evaluated their effectiveness on open-source web applications. Our results indicate that the supervised approach can achieve precision, recall, and Fl-score of 83.38, 60.64, and 66.40, respectively. The unsupervised approach, although less effective, is competitive, achieving scores of 72.37, 58.12, and 59.77. We highlight key results and discuss the implications of our findings.},
   author = {Yu Liu and Rahulkrishna Yandrapally and Anup K Kalia and Saurabh Sinha and Rachel Tzoref-Brill and Ali Mesbah},
   city = {New York, NY, USA},
   doi = {10.1145/3524481.3527229},
   isbn = {9781450392860},
   booktitle = {Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test},
   pages = {103-114},
   publisher = {Association for Computing Machinery},
   title = {CrawLabel: computing natural-language labels for UI test cases},
   url = {https://doi.org/10.1145/3524481.3527229},
   year = {2022}
}
@inproceedings{Temtsin2022,
   abstract = {The constantly rising demand for human-like conversational agents and the accelerated development of natural language processing technology raise expectations for a breakthrough in intelligent machine research and development. However, measuring intelligence is impossible without a proper test. Alan Turing proposed a test for machine intelligence based on imitation and unconstrained conversations between a machine and a human. To the best of our knowledge, no one has ever conducted Turing’s test as Turing prescribed, even though the Turing Test has been a bone of contention for more than seventy years. Conducting a bona fide Turing Test will contribute to machine intelligence evaluation research and has the potential to advance AI researchers in their ultimate quest, developing an intelligent machine.},
   author = {Sharon Temtsin and Diane Proudfoot and Christoph Bartneck},
   city = {New York, NY, USA},
   doi = {10.1145/3527188.3563918},
   isbn = {9781450393232},
   booktitle = {Proceedings of the 10th International Conference on Human-Agent Interaction},
   keywords = {Turing test,artificial intelligence,conversational agent,imitation game},
   pages = {250-252},
   publisher = {Association for Computing Machinery},
   title = {A Bona Fide Turing Test},
   url = {https://doi.org/10.1145/3527188.3563918},
   year = {2022}
}
@article{Wu2023-2,
   abstract = {In the context of testing, descriptive test names are desirable because they document the purpose of tests and facilitate comprehension tasks during maintenance. Unfortunately, prior work has shown that tests often do not have descriptive names. To address this limitation, techniques have been developed to automatically generate descriptive names. However, they often generated names that are invalid or do not meet developer approval. To help address these limitations, we present a novel approach to extract the attributes of a given test that make it unique among its siblings. Because such attributes often serve as the basis for descriptive names, identifying them is an important first step towards improving test name generation approaches. To evaluate the approach, we created a prototype implementation for JUnit tests and compared its output with human judgment. The results of the evaluation demonstrate that the attributes identified by the approach are consistent with human judgment and are likely to be useful for future name generation techniques.},
   author = {Jianwei Wu and James Clause},
   city = {New York, NY, USA},
   doi = {10.1145/3533313},
   issn = {1049-331X},
   issue = {1},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Unit testing,formal concept analysis},
   month = {2},
   publisher = {Association for Computing Machinery},
   title = {Automated Identification of Uniqueness in JUnit Tests},
   volume = {32},
   url = {https://doi.org/10.1145/3533313},
   year = {2023}
}
@inproceedings{Ji2022,
   abstract = {With the rapid development of deep neural networks and end-to-end learning techniques, automatic speech recognition (ASR) systems have been deployed into our daily and assist in various tasks. However, despite their tremendous progress, ASR systems could also suffer from software defects and exhibit incorrect behaviors. While the nature of DNN makes conventional software testing techniques inapplicable for ASR systems, lacking diverse tests and oracle information further hinders their testing. In this paper, we propose and implement a testing approach, namely ASR, specifically for the DNN-driven ASR systems. ASRTest is built upon the theory of metamorphic testing. We first design the metamorphic relation for ASR systems and then implement three families of transformation operators that can simulate practical application scenarios to generate speeches. Furthermore, we adopt Gini impurity to guide the generation process and improve the testing efficiency. To validate the effectiveness of ASRTest, we apply ASRTest to four ASR models with four widely-used datasets. The results show that ASRTest can detect erroneous behaviors under different realistic application conditions efficiently and improve 19.1\% recognition performance on average via retraining with the generated data. Also, we conduct a case study on an industrial ASR system to investigate the performance of ASRTest under the real usage scenario. The study shows that ASRTest can detect errors and improve the performance of DNN-driven ASR systems effectively.},
   author = {Pin Ji and Yang Feng and Jia Liu and Zhihong Zhao and Zhenyu Chen},
   city = {New York, NY, USA},
   doi = {10.1145/3533767.3534391},
   isbn = {9781450393799},
   booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Automated Testing,Automatic Speech Recognition,Deep Neural Networks,Metamorphic Testing},
   pages = {189-201},
   publisher = {Association for Computing Machinery},
   title = {ASRTest: automated testing for deep-neural-network-driven speech recognition systems},
   url = {https://doi.org/10.1145/3533767.3534391},
   year = {2022}
}
@inproceedings{Chen2022,
   abstract = {Neural Machine Translation (NMT) systems have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of NMT systems, which is of paramount importance due to often vast translation demands and real-time requirements, has surprisingly received little attention. In this paper, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art NMT systems. By analyzing the working mechanism and implementation of 1455 public-accessible NMT systems, we observe a fundamental property in NMT systems that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of NMT systems instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that NMT systems would have to go through enough iterations to satisfy the pre-configured threshold. We present NMTSloth, which develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level, which sufficiently delays the appearance of EOS and forces these inputs to reach the naturally-unreachable threshold. To demonstrate the effectiveness of NMTSloth, we conduct a systematic evaluation on three public-available NMT systems: Google T5, AllenAI WMT14, and Helsinki-NLP translators. Experimental results show that NMTSloth can increase NMT systems' response latency and energy consumption by 85\% to 3153\% and 86\% to 3052\%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by NMTSloth significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).},
   author = {Simin Chen and Cong Liu and Mirazul Haque and Zihe Song and Wei Yang},
   city = {New York, NY, USA},
   doi = {10.1145/3540250.3549102},
   isbn = {9781450394130},
   booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Machine learning,neural machine translation,software testing},
   pages = {1148-1160},
   publisher = {Association for Computing Machinery},
   title = {NMTSloth: understanding and testing efficiency degradation of neural machine translation systems},
   url = {https://doi.org/10.1145/3540250.3549102},
   year = {2022}
}
@inproceedings{Zhao2022,
   abstract = {Writing and maintaining UI tests for mobile apps is a time-consuming and tedious task. While decades of research have produced auto- mated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69\% of the tests Avgust generates successfully execute the desired usage, and that Avgust’s classifiers outperform the state of the art.},
   author = {Yixue Zhao and Saghar Talebipour and Kesina Baral and Hyojae Park and Leon Yee and Safwat Ali Khan and Yuriy Brun and Nenad Medvidović and Kevin Moran},
   city = {New York, NY, USA},
   doi = {10.1145/3540250.3549134},
   isbn = {9781450394130},
   booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {AI/ML,Mobile Application,Test Generation,UI Understanding},
   pages = {421-433},
   publisher = {Association for Computing Machinery},
   title = {Avgust: automating usage-based test generation from videos of app executions},
   url = {https://doi.org/10.1145/3540250.3549134},
   year = {2022}
}
@inproceedings{Chen2022,
   abstract = {In modern code reviews, many artifacts play roles in knowledge- sharing and documentation: summaries, test plans, and comments, etc. Improving developer tools and facilitating better code reviews require an understanding of the quality of pull requests and their artifacts. This is difficult to measure, however, because they are often free-form natural language and unstructured text data. In this paper, we focus on measuring the quality of test plans at Meta. Test plans are used as a communication mechanism between the author of a pull request and its reviewers, serving as walkthroughs to help confirm that the changed code is behaving as expected. We collected developer opinions on over 650 test plans from more than 500 Meta developers, then introduced a transformer-based model to leverage the success of natural language processing (NLP) tech- niques in the code review domain. In our study, we show that the learned model is able to capture the sentiment of developers and reflect a correlation of test plan quality with review engagement and reversions: compared to a decision tree model, our proposed transformer-based model achieves a 7\% higher F1-score. Finally, we present a case study of how such a metric may be useful in experiments to inform improvements in developer tools and experiences.},
   author = {Lawrence Chen and Rui Abreu and Tobi Akomolede and Peter C Rigby and Satish Chandra and Nachiappan Nagappan},
   city = {New York, NY, USA},
   doi = {10.1145/3540250.3558952},
   isbn = {9781450394130},
   booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Code Reviews,Natural Language Processing,Pull Requests,Test Plans},
   pages = {1320-1330},
   publisher = {Association for Computing Machinery},
   title = {Leveraging test plan quality to improve code review efficacy},
   url = {https://doi.org/10.1145/3540250.3558952},
   year = {2022}
}
@inproceedings{Guglielmi2023,
   abstract = {Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers on which they can build their own apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows to use natural language commands to perform actions. Testing such apps is far from trivial: The same command can be expressed in different ways. To support developers in testing VUIs, Deep Learning (DL)-based tools have been integrated in the development environments (e.g., the Alexa Developer Console, or ADC) to generate paraphrases for the commands (seed utterances) specified by the developers. Such tools, however, generate few paraphrases that do not always cover corner cases. In this paper, we introduce VUI-UPSET, a novel approach that aims at adapting chatbot-testing approaches to VUI-testing. Both systems, indeed, provide a similar natural-language-based interface to users. We conducted an empirical study to understand how VUI-UPSET compares to existing approaches in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. Multiple authors analyzed 5,872 generated paraphrases, with a total of 13,310 manual evaluations required for such a process. Our results show that, while the DL-based tool integrated in the ADC generates a higher percentage of meaningful paraphrases compared to VUI-UPSET, VUI-UPSET generates more bug-revealing paraphrases. This allows developers to test more thoroughly their apps at the cost of discarding a higher number of irrelevant paraphrases.},
   author = {Emanuela Guglielmi and Giovanni Rosa and Simone Scalabrino and Gabriele Bavota and Rocco Oliveto},
   city = {New York, NY, USA},
   doi = {10.1145/3551349.3556934},
   isbn = {9781450394758},
   booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {NLP,software testing,voice user interfaces},
   publisher = {Association for Computing Machinery},
   title = {Sorry, I don’t Understand: Improving Voice User Interface Testing},
   url = {https://doi.org/10.1145/3551349.3556934},
   year = {2023}
}
@inproceedings{Wang2023,
   abstract = {Automated code generation is a longstanding challenge in both communities of software engineering and artificial intelligence. Currently, some works have started to investigate the functional correctness of code generation, where a code snippet is considered correct if it passes a set of test cases. However, most existing works still model code generation as text generation without considering program-specific information, such as functionally equivalent code snippets and test execution feedback. To address the above limitations, this paper proposes a method combining program analysis with deep learning for neural code generation, where functionally equivalent code snippets and test execution feedback will be considered at the training stage. Concretely, we firstly design several code transformation heuristics to produce different variants of the code snippet satisfying the same functionality. In addition, we employ the test execution feedback and design a test-driven discriminative task to train a novel discriminator, aiming to let the model distinguish whether the generated code is correct or not. The preliminary results on a newly published dataset demonstrate the effectiveness of our proposed framework for code generation. Particularly, in terms of the pass@1 metric, we achieve 8.81 and 11.53 gains compared with CodeGPT and CodeT5, respectively.},
   author = {Xin Wang and Xiao Liu and Pingyi Zhou and Qixia Liu and Jin Liu and Hao Wu and Xiaohui Cui},
   city = {New York, NY, USA},
   doi = {10.1145/3551349.3559549},
   isbn = {9781450394758},
   booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {Code Transformation,Execution Feedback,Multi-Task Learning,Neural Code Generation,Program Analysis},
   publisher = {Association for Computing Machinery},
   title = {Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation},
   url = {https://doi.org/10.1145/3551349.3559549},
   year = {2023}
}
@inproceedings{Pham2023,
   abstract = {The process of creating test cases from requirements written in natural language (NL) requires intensive human efforts and can be tedious, repetitive, and error-prone. Thus, many studies have attempted to automate that process by utilizing Natural Language Processing (NLP) approaches. Furthermore, with the advent of massive language models and transfer learning techniques, people have introduced various advancements in NLP-assisted software testing with promising results. More notably, in recent years, not only have researchers been engrossed in solving the above task, but many companies have also embedded the feature to translate from human language to test cases their products. This paper presents an overview of NLP-assisted solutions being used in both the literature and the software testing industry.},
   author = {Khang Pham and Vu Nguyen and Tien Nguyen},
   city = {New York, NY, USA},
   doi = {10.1145/3551349.3563241},
   isbn = {9781450394758},
   booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
   publisher = {Association for Computing Machinery},
   title = {Application of Natural Language Processing Towards Autonomous Software Testing},
   url = {https://doi.org/10.1145/3551349.3563241},
   year = {2023}
}
@article{Erata2023,
   abstract = {This article surveys the landscape of security verification approaches and techniques for computer systems at various levels: from a software-application level all the way to the physical hardware level. Different existing projects are compared, based on the tools used and security aspects being examined. Since many systems require both hardware and software components to work together to provide the system’s promised security protections, it is not sufficient to verify just the software levels or just the hardware levels in a mutually exclusive fashion. This survey especially highlights system levels that are verified by the different existing projects and presents to the readers the state of the art in hardware and software system security verification. Few approaches come close to providing full-system verification, and there is still much room for&nbsp;improvement.},
   author = {Ferhat Erata and Shuwen Deng and Faisal Zaghloul and Wenjie Xiong and Onur Demir and Jakub Szefer},
   city = {New York, NY, USA},
   doi = {10.1145/3564785},
   issn = {1550-4832},
   issue = {1},
   journal = {J. Emerg. Technol. Comput. Syst.},
   keywords = {Formal methods,hardware-level verification,model checkers,security verification,software-level verification,theorem provers},
   month = {1},
   publisher = {Association for Computing Machinery},
   title = {Survey of Approaches and Techniques for Security Verification of Computer Systems},
   volume = {19},
   url = {https://doi.org/10.1145/3564785},
   year = {2023}
}
@inproceedings{Evans2023,
   abstract = {The increase of disinformation in scientific news across a variety of domains has generated an urgency for a robust and generalizable approach to automated scientific claim verification (SCV). Available methods of SCV are limited in either domain adaptability or scalability. To facilitate building and evaluating more robust models on SCV we propose MSVEC, a multidomain dataset containing 200 pairs of verified scientific news claims with evidence research papers. To understand the capability of large language models on the SCV task, we evaluated GPT-3.5 against MSVEC. While methods of fact-checking exist for specific domains (e.g., political and health), the use of large language models exhibits better generalizability across multiple domains and is potentially compared with state-of-the-art models based on word embeddings. The data and software used and developed for this project are available at https://github.com/lamps-lab/msvec.},
   author = {Michael Evans and Dominik Soós and Ethan Landers and Jian Wu},
   city = {New York, NY, USA},
   doi = {10.1145/3565287.3617630},
   isbn = {9781450399265},
   booktitle = {Proceedings of the Twenty-Fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
   keywords = {benchmark datasets,large language models,machine learning,natural language processing},
   pages = {504-509},
   publisher = {Association for Computing Machinery},
   title = {MSVEC: A Multidomain Testing Dataset for Scientific Claim Verification},
   url = {https://doi.org/10.1145/3565287.3617630},
   year = {2023}
}
@article{Antonopoulos2023,
   abstract = {Relational verification encompasses information flow security, regression verification, translation validation for compilers, and more. Effective alignment of the programs and computations to be related facilitates use of simpler relational invariants and relational procedure specs, which in turn enables automation and modular reasoning. Alignment has been explored in terms of trace pairs, deductive rules of relational Hoare logics (RHL), and several forms of product automata. This article shows how a simple extension of Kleene Algebra with Tests (KAT), called BiKAT, subsumes prior formulations, including alignment witnesses for forall-exists properties, which brings to light new RHL-style rules for such properties. Alignments can be discovered algorithmically or devised manually but, in either case, their adequacy with respect to the original programs must be proved; an explicit algebra enables constructive proof by equational reasoning. Furthermore our approach inherits algorithmic benefits from existing KAT-based techniques and tools, which are applicable to a range of semantic models.},
   author = {Timos Antonopoulos and Eric Koskinen and Ton Chanh Le and Ramana Nagasamudram and David A Naumann and Minh Ngo},
   city = {New York, NY, USA},
   doi = {10.1145/3571213},
   issue = {POPL},
   journal = {Proc. ACM Program. Lang.},
   keywords = {Kleene algebra with tests,hyperproperties,program algebra,relational verification},
   month = {1},
   publisher = {Association for Computing Machinery},
   title = {An Algebra of Alignment for Relational Verification},
   volume = {7},
   url = {https://doi.org/10.1145/3571213},
   year = {2023}
}
@inproceedings{Morris2023,
   abstract = {Massive Open Online Courses (MOOCs) such as those offered by Coursera are popular ways for adults to gain important skills, advance their careers, and pursue their interests. Within these courses, students are often required to compose, submit, and peer review written essays, providing a valuable pedagogical experience for the student and a wealth of natural language data for the educational researcher. However, the scores provided by peers do not always reflect the actual quality of the text, generating questions about the reliability and validity of the scores. This study evaluates methods to increase the reliability of MOOC peer-review ratings through a series of validation tests on peer-reviewed essays. Reliability of reviewers was based on correlations between text length and essay quality. Raters were pruned based on score variance and the lexical diversity observed in their comments to create sub-sets of raters. Each subset was then used as training data to finetune distilBERT large language models to automatically score essay quality as a measure of validation. The accuracy of each language model for each subset was evaluated. We find that training language models on data subsets produced by more reliable raters based on a combination of score variance and lexical diversity produce more accurate essay scoring models. The approach developed in this study should allow for enhanced reliability of peer-reviewed scoring in MOOCS affording greater credibility within the systems.},
   author = {Wesley Morris and Scott Crossley and Langdon Holmes and Anne Trumbore},
   city = {New York, NY, USA},
   doi = {10.1145/3576050.3576098},
   isbn = {9781450398657},
   booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
   keywords = {moocs,natural language processing,rater reliability,transformers},
   pages = {315-323},
   publisher = {Association for Computing Machinery},
   title = {Using Transformer Language Models to Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs)},
   url = {https://doi.org/10.1145/3576050.3576098},
   year = {2023}
}
@inproceedings{Finnie-Ansley2023,
   abstract = {The introduction of OpenAI Codex sparked a surge of interest in the impact of generative AI models on computing education practices. Codex is also the underlying model for GitHub Copilot, a plugin which makes AI-generated code accessible to students through auto-completion in popular code editors. Research in this area, particularly on the educational implications, is nascent and has focused almost exclusively on introductory programming (or CS1) questions. Very recent work has shown that Codex performs considerably better on typical CS1 exam questions than most students. It is not clear, however, what Codex’s limits are with regard to more complex programming assignments and exams. In this paper, we present results detailing how Codex performs on more advanced CS2 (data structures and algorithms) exam questions taken from past exams. We compare these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students. We consider the implications of such tools for the future of undergraduate computing education.},
   author = {James Finnie-Ansley and Paul Denny and Andrew Luxton-Reilly and Eddie Antonio Santos and James Prather and Brett A Becker},
   city = {New York, NY, USA},
   doi = {10.1145/3576123.3576134},
   isbn = {9781450399418},
   booktitle = {Proceedings of the 25th Australasian Computing Education Conference},
   keywords = {AI,AlphaCode,CS1,CS2,Codex,DeepMind,GPT-3,GitHub,OpenAI,academic integrity,algorithms,artificial intelligence,code generation,copilot,data structures,deep learning,introductory programming,machine learning,neural networks,novice programming},
   pages = {97-104},
   publisher = {Association for Computing Machinery},
   title = {My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises},
   url = {https://doi.org/10.1145/3576123.3576134},
   year = {2023}
}
@inproceedings{He2023,
   abstract = {While enjoying the great achievements brought by deep learning (DL), people are also worried about the decision made by DL models, since the high degree of non-linearity of DL models makes the decision extremely difficult to understand. Consequently, attacks such as adversarial attacks are easy to carry out, but difficult to detect and explain, which has led to a boom in the research on local explanation methods for explaining model decisions. In this paper, we evaluate the faithfulness of explanation methods and find that traditional tests on faithfulness encounter the random dominance problem, i.e., the random selection performs the best, especially for complex data. To further solve this problem, we propose three trend-based faithfulness tests and empirically demonstrate that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefiting from the trend tests, we successfully assess the explanation methods on complex data for the first time, bringing unprecedented discoveries and inspiring future research. Downstream tasks also greatly benefit from the tests. For example, model debugging equipped with faithful explanation methods performs much better for detecting and correcting accuracy and security problems.},
   author = {Jinwen He and Kai Chen and Guozhu Meng and Jiangshan Zhang and Congyi Li},
   city = {New York, NY, USA},
   doi = {10.1145/3576915.3616605},
   isbn = {9798400700507},
   booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
   keywords = {deep learning,faithfulness,local explanation,security task},
   pages = {431-445},
   publisher = {Association for Computing Machinery},
   title = {Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing},
   url = {https://doi.org/10.1145/3576915.3616605},
   year = {2023}
}
@inproceedings{He2023-1,
   abstract = {Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1\% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3\% (or degraded to 36.8\%). Importantly, SVEN closely matches the original LMs in functional correctness.},
   author = {Jingxuan He and Martin Vechev},
   city = {New York, NY, USA},
   doi = {10.1145/3576915.3623175},
   isbn = {9798400700507},
   booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
   keywords = {ai safety,code generation,code security,large language models},
   pages = {1865-1879},
   publisher = {Association for Computing Machinery},
   title = {Large Language Models for Code: Security Hardening and Adversarial Testing},
   url = {https://doi.org/10.1145/3576915.3623175},
   year = {2023}
}
@inproceedings{Cankar2023,
   abstract = {Security represents one of the crucial concerns when it comes to DevOps methodology-empowered software development and service delivery process. Considering the adoption of Infrastructure as Code (IaC), even minor flaws could potentially cause fatal consequences, especially in sensitive domains such as healthcare and maritime applications. However, most of the existing solutions tackle either Static Application Security Testing (SAST) or run-time behavior analysis distinctly. In this paper, we propose a) IaC Scan Runner, an open-source solution developed in Python for inspecting a variety of state-of-the-art IaC languages in application design time and b) the run time anomaly detection tool called LOMOS. Both tools work in synergy and provide a valuable contribution to a DevSecOps tool set. The proposed approach is demonstrated and their results will be demonstrated on various case studies showcasing the capabilities of static analysis tool IaC Scan Runner combined with LOMOS - log analysis artificial intelligence-enabled framework.},
   author = {Matija Cankar and Nenad Petrovic and Joao Pita Costa and Ales Cernivec and Jan Antic and Tomaz Martincic and Dejan Stepec},
   city = {New York, NY, USA},
   doi = {10.1145/3578245.3584943},
   isbn = {9798400700729},
   booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
   keywords = {DAST,DevOps,DevSecOps,IaC,SAST,machine learning,natural language processing,self-supervised learning},
   pages = {201-205},
   publisher = {Association for Computing Machinery},
   title = {Security in DevSecOps: Applying Tools and Machine Learning to Verification and Monitoring Steps},
   url = {https://doi.org/10.1145/3578245.3584943},
   year = {2023}
}
@inproceedings{Deng2023,
   abstract = {In recent years, interest in directly involving end users in testing, auditing, and contesting AI systems has grown. The involvement of end users, especially from diverse backgrounds, can be essential to overcome AI developers’ blind spots and to surface issues that would otherwise go undetected prior to causing real-world harm. Emerging bodies of work in CSCW have begun to explore ways to engage end-users in testing and auditing AI systems, and to empower users to contest problematic or erroneous AI outputs. As this is a nascent area of work, we still know little about how to support effective user engagement. In this one-day workshop, we will bring together researchers and practitioners from academia, industry, and non-profit organizations to share ongoing efforts related to this workshop’s theme. Central to our discussions will be the challenges encountered in developing tools and processes to support user involvement, strategies to incentivize involvement, the asymmetric power dynamic between AI developers and end users, and the role of regulation in enhancing the accountability of AI developers and ameliorating potential burdens towards end-users. Overall, we hope the workshop will help shape the future of user engagement in building more responsible AI.},
   author = {Wesley Hanwen Deng and Michelle S Lam and Ángel Alexander Cabrera and Danaë Metaxa and Motahhare Eslami and Kenneth Holstein},
   city = {New York, NY, USA},
   doi = {10.1145/3584931.3611279},
   isbn = {9798400701290},
   booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
   keywords = {algorithm auditing,human-centered AI,responsible AI,usability testing},
   pages = {556-559},
   publisher = {Association for Computing Machinery},
   title = {Supporting User Engagement in Testing, Auditing, and Contesting AI},
   url = {https://doi.org/10.1145/3584931.3611279},
   year = {2023}
}
@article{Huang2023,
   abstract = {In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g.,&nbsp;67.1\% on ImageNet for 2\%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8\% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy.},
   author = {Yuheng Huang and Lei Ma and Yuanchun Li},
   city = {New York, NY, USA},
   doi = {10.1145/3591870},
   issn = {1049-331X},
   issue = {6},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Adversarial patch,certified accuracy,deep learning,neural networks,robustness certification,vision transformer},
   month = {9},
   publisher = {Association for Computing Machinery},
   title = {PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing},
   volume = {32},
   url = {https://doi.org/10.1145/3591870},
   year = {2023}
}
@inproceedings{Soares2023,
   abstract = {Artificial Intelligence (AI) is currently a hot topic. The explosion of AI technologies that create Synthetic Media (SM), such as Deep Fakes (DF), is worrisome, especially since they can easily spread false information over social networks. Our work explores a positive use case for SM (fake text, website and app design, audio, and videos) to aid in reconnaissance, vulnerability assessment, and exploitation during penetration testing. We contest that using SM during various penetration testing phases is useful for uncovering security weaknesses in an organization. Our work results in constructing a primary mind map of the various penetration testing phases and the potential uses of SM in each of them.},
   author = {Nathalia Soares and Steven Seiden and Ibrahim Baggili and Andrew Webb},
   city = {New York, NY, USA},
   doi = {10.1145/3595353.3595886},
   isbn = {9798400702037},
   booktitle = {Proceedings of the 2nd Workshop on Security Implications of Deepfakes and Cheapfakes},
   keywords = {Artificial Intelligence,DeepFake,Phishing,Social Engineering,Synthetic Media,Text Generation},
   pages = {1-10},
   publisher = {Association for Computing Machinery},
   title = {On the Application of Synthetic Media to Penetration Testing},
   url = {https://doi.org/10.1145/3595353.3595886},
   year = {2023}
}
@inproceedings{Zhang2024,
   abstract = {GUI test case migration is to migrate GUI test cases from a source app to a target app. The key of test case migration is widget matching. Recently, researchers have proposed various approaches by formulating widget matching as a matching task. However, since these matching approaches depend on static word embeddings without using contextual information to represent widgets and manually formulated matching functions, there are main limitations of these matching approaches when handling complex matching relations in apps. To address the limitations, we propose the first learning-based widget matching approach named TEMdroid (TEst Migration) for test case migration. Unlike the existing approaches, TEMdroid uses BERT to capture contextual information and learns a matching model to match widgets. Additionally, to balance the significant imbalance between positive and negative samples in apps, we design a two-stage training strategy where we first train a hard-negative sample miner to mine hard-negative samples, and further train a matching model using positive samples and mined hard-negative samples. Our evaluation on 34 apps shows that TEM-droid is effective in event matching (i.e., widget matching and target event synthesis) and test case migration. For event matching, TEM-droid's Top1 accuracy is 76\%, improving over 17\% compared to baselines. For test case migration, TEMdroid's F1 score is 89\%, also 7\% improvement compared to the baseline approach.},
   author = {Yakun Zhang and Wenjie Zhang and Dezhi Ran and Qihao Zhu and Chengfeng Dou and Dan Hao and Tao Xie and Lu Zhang},
   city = {New York, NY, USA},
   doi = {10.1145/3597503.3623322},
   isbn = {9798400702174},
   booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
   keywords = {GUI testing,deep learning,test migration},
   publisher = {Association for Computing Machinery},
   title = {Learning-based Widget Matching for Migrating GUI Test Cases},
   url = {https://doi.org/10.1145/3597503.3623322},
   year = {2024}
}
@inproceedings{Yang2024-6,
   abstract = {Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\%–54.4\%, and Top-5 results by 14.4\%-35.6\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.},
   author = {Aidan Z H Yang and Claire Le Goues and Ruben Martins and Vincent Hellendoorn},
   city = {New York, NY, USA},
   doi = {10.1145/3597503.3623342},
   isbn = {9798400702174},
   booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
   publisher = {Association for Computing Machinery},
   title = {Large Language Models for Test-Free Fault Localization},
   url = {https://doi.org/10.1145/3597503.3623342},
   year = {2024}
}
@inproceedings{Cao2024,
   abstract = {Graphical User Interface (GUI) testing is one of the primary approaches for testing mobile apps. Test scripts serve as the main carrier of GUI testing, yet they are prone to obsolescence when the GUIs change with the apps' evolution. Existing repair approaches based on GUI layouts or images prove effective when the GUI changes between the base and updated versions are minor, however, they may struggle with substantial changes. In this paper, a novel approach named COSER is introduced as a solution to repairing broken scripts, which is capable of addressing larger GUI changes compared to existing methods. COSER incorporates both external semantic information from the GUI elements and internal semantic information from the source code to provide a unique and comprehensive solution. The efficacy of COSER was demonstrated through experiments conducted on 20 Android apps, resulting in superior performance when compared to the state-of-the-art tools METER and GUIDER. In addition, a tool that implements the COSER approach is available for practical use and future research.},
   author = {Shaoheng Cao and Minxue Pan and Yu Pei and Wenhua Yang and Tian Zhang and Linzhang Wang and Xuandong Li},
   city = {New York, NY, USA},
   doi = {10.1145/3597503.3639108},
   isbn = {9798400702174},
   booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
   keywords = {Android testing,GUI test script repair,regression testing},
   publisher = {Association for Computing Machinery},
   title = {Comprehensive Semantic Repair of Obsolete GUI Test Scripts for Mobile Applications},
   url = {https://doi.org/10.1145/3597503.3639108},
   year = {2024}
}
@inproceedings{Wang2024-3,
   abstract = {In the wake of developments in the field of Natural Language Processing, Question Answering (QA) software has penetrated our daily lives. Due to the data-driven programming paradigm, QA software inevitably contains bugs, i.e., misbehaving in real-world applications. Current testing techniques for testing QA software include two folds, reference-based testing and metamorphic testing.This paper adopts a different angle to achieve testing for QA software: we notice that answers to questions would have inference relations, i.e., the answers to some questions could be logically inferred from the answers to other questions. If these answers on QA software do not satisfy the inference relations, an inference bug is detected. To generate the questions with the inference relations automatically, we propose a novel testing method Knowledge Graph driven Inference Testing (KGIT), which employs facts in the Knowledge Graph (KG) as the seeds to logically construct test cases containing questions and contexts with inference relations. To evaluate the effectiveness of KGIT, we conduct an extensive empirical study with more than 2.8 million test cases generated from the large-scale KG YAGO4 and three QA models based on the state-of-the-art QA model structure. The experimental results show that our method (a) could detect a considerable number of inference bugs in all three studied QA models and (b) is helpful in retraining QA models to improve their inference ability.},
   author = {Jun Wang and Yanhui Li and Zhifei Chen and Lin Chen and Xiaofang Zhang and Yuming Zhou},
   city = {New York, NY, USA},
   doi = {10.1145/3597503.3639109},
   isbn = {9798400702174},
   booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
   keywords = {inference rules,knowledge graph,question answering,software testing},
   publisher = {Association for Computing Machinery},
   title = {Knowledge Graph Driven Inference Testing for Question Answering Software},
   url = {https://doi.org/10.1145/3597503.3639109},
   year = {2024}
}
@inproceedings{Liu2024-1,
   abstract = {Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78\% bug detection rate, with 136\% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps.},
   author = {Zhe Liu and Chunyang Chen and Junjie Wang and Mengzhuo Chen and Boyu Wu and Zhilin Tian and Yuekai Huang and Jun Hu and Qing Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3597503.3639118},
   isbn = {9798400702174},
   booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
   keywords = {Android GUI testing,in-context learning,large language model},
   publisher = {Association for Computing Machinery},
   title = {Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model},
   url = {https://doi.org/10.1145/3597503.3639118},
   year = {2024}
}
@inproceedings{He2024-3,
   abstract = {Sequential decision-making processes (SDPs) are fundamental for complex real-world challenges, such as autonomous driving, robotic control, and traffic management. While recent advances in Deep Learning (DL) have led to mature solutions for solving these complex problems, SDMs remain vulnerable to learning unsafe behaviors, posing significant risks in safety-critical applications. However, developing a testing framework for SDMs that can identify a diverse set of crash-triggering scenarios remains an open challenge. To address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz testing approach for SDMs. CureFuzz proposes a curiosity mechanism that allows a fuzzer to effectively explore novel and diverse scenarios, leading to improved detection of crash-triggering scenarios. Additionally, we introduce a multi-objective seed selection technique to balance the exploration of novel scenarios and the generation of crash-triggering scenarios, thereby optimizing the fuzzing process. We evaluate CureFuzz on various SDMs and experimental results demonstrate that CureFuzz outperforms the state-of-the-art method by a substantial margin in the total number of faults and distinct types of crash-triggering scenarios. We also demonstrate that the crash-triggering scenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a valuable tool for testing SDMs and optimizing their performance.},
   author = {Junda He and Zhou Yang and Jieke Shi and Chengran Yang and Kisub Kim and Bowen Xu and Xin Zhou and David Lo},
   city = {New York, NY, USA},
   doi = {10.1145/3597503.3639149},
   isbn = {9798400702174},
   booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
   keywords = {deep learning,fuzz testing,sequential decision making},
   publisher = {Association for Computing Machinery},
   title = {Curiosity-Driven Testing for Sequential Decision-Making Process},
   url = {https://doi.org/10.1145/3597503.3639149},
   year = {2024}
}
@inproceedings{Su2024-1,
   abstract = {Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.},
   author = {Yanqi Su and Dianshu Liao and Zhenchang Xing and Qing Huang and Mulong Xie and Qinghua Lu and Xiwei Xu},
   city = {New York, NY, USA},
   doi = {10.1145/3597503.3639157},
   isbn = {9798400702174},
   booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
   keywords = {AI chain,exploratory testing,knowledge graph,prompt engineering},
   publisher = {Association for Computing Machinery},
   title = {Enhancing Exploratory Testing by Large Language Model and Knowledge Graph},
   url = {https://doi.org/10.1145/3597503.3639157},
   year = {2024}
}
@inproceedings{Liu2024-2,
   abstract = {Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32\% in activity coverage, and detects 31\% more bugs at a faster rate. Moreover, GPTDroid identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed.},
   author = {Zhe Liu and Chunyang Chen and Junjie Wang and Mengzhuo Chen and Boyu Wu and Xing Che and Dandan Wang and Qing Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3597503.3639180},
   isbn = {9798400702174},
   booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
   keywords = {automated GUI testing,large language model},
   publisher = {Association for Computing Machinery},
   title = {Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions},
   url = {https://doi.org/10.1145/3597503.3639180},
   year = {2024}
}
@inproceedings{Liu2023,
   abstract = {Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4\% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2\% of false positives have perfect (100\%) line coverage, 78.9\% have perfect branch coverage, and 32.5\% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.},
   author = {Kaibo Liu and Yudong Han and Jie M Zhang and Zhenpeng Chen and Federica Sarro and Mark Harman and Gang Huang and Yun Ma},
   city = {New York, NY, USA},
   doi = {10.1145/3597926.3598060},
   isbn = {9798400702211},
   booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Online judge platform,software testing,test assessment},
   pages = {334-346},
   publisher = {Association for Computing Machinery},
   title = {Who Judges the Judge: An Empirical Study on Online Judge Tests},
   url = {https://doi.org/10.1145/3597926.3598060},
   year = {2023}
}
@inproceedings{Shi2023,
   abstract = {Deep learning (DL) applications are prevalent nowadays as they can help with multiple tasks. DL libraries are essential for building DL applications. Furthermore, DL operators are the important building blocks of the DL libraries, that compute the multi-dimensional data (tensors). Therefore, bugs in DL operators can have great impacts. Testing is a practical approach for detecting bugs in DL operators. In order to test DL operators effectively, it is essential that the test cases pass the input validity check and are able to reach the core function logic of the operators. Hence, extracting the input validation constraints is required for generating high-quality test cases. Existing techniques rely on either human effort or documentation of DL library APIs to extract the constraints. They cannot extract complex constraints and the extracted constraints may differ from the actual code implementation.
To address the challenge, we propose ACETest, a technique to automatically extract input validation constraints from the code to build valid yet diverse test cases which can effectively unveil bugs in the core function logic of DL operators. For this purpose, ACETest can automatically identify the input validation code in DL operators, extract the related constraints and generate test cases according to the constraints. The experimental results on popular DL libraries, TensorFlow and PyTorch, demonstrate that ACETest can extract constraints with higher quality than state-of-the-art (SOTA) techniques. Moreover, ACETest is capable of extracting 96.4\% more constraints and detecting 1.95 to 55 times more bugs than SOTA techniques. In total, we have used ACETest to detect 108 previously unknown bugs on TensorFlow and PyTorch, with 87 of them confirmed by the developers. Lastly, five of the bugs were assigned with CVE IDs due to their security impacts.},
   author = {Jingyi Shi and Yang Xiao and Yuekang Li and Yeting Li and Dongsong Yu and Chendong Yu and Hui Su and Yufeng Chen and Wei Huo},
   city = {New York, NY, USA},
   doi = {10.1145/3597926.3598088},
   isbn = {9798400702211},
   booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Constraint Extraction,Deep Learning Library Testing,Symbolic Execution,Test Generation},
   pages = {690-702},
   publisher = {Association for Computing Machinery},
   title = {ACETest: Automated Constraint Extraction for Testing Deep Learning Operators},
   url = {https://doi.org/10.1145/3597926.3598088},
   year = {2023}
}
@inproceedings{Lau2023,
   abstract = {Recent studies have proposed the use of Text-To-Speech (TTS) systems to automatically synthesise speech test cases on a scale and uncover a large number of failures in ASR systems. However, the failures uncovered by synthetic test cases may not reflect the actual performance of an ASR system when it transcribes human audio, which we refer to as false alarms. Given a failed test case synthesised from TTS systems, which consists of TTS-generated audio and the corresponding ground truth text, we feed the human audio stating the same text to an ASR system. If human audio can be correctly transcribed, an instance of a false alarm is detected. In this study, we investigate false alarm occurrences in five popular ASR systems using synthetic audio generated from four TTS systems and human audio obtained from two commonly used datasets. Our results show that the least number of false alarms is identified when testing Deepspeech, and the number of false alarms is the highest when testing Wav2vec2. On average, false alarm rates range from 21\% to 34\% in all five ASR systems. Among the TTS systems used, Google TTS produces the least number of false alarms (17\%), and Espeak TTS produces the highest number of false alarms (32\%) among the four TTS systems. Additionally, we build a false alarm estimator that flags potential false alarms, which achieves promising results: a precision of 98.3\%, a recall of 96.4\%, an accuracy of 98.5\%, and an F1 score of 97.3\%. Our study provides insight into the appropriate selection of TTS systems to generate high-quality speech to test ASR systems. Additionally, a false alarm estimator can be a way to minimise the impact of false alarms and help developers choose suitable test inputs when evaluating ASR systems. The source code used in this paper is publicly available on GitHub at https://github.com/julianyonghao/FAinASRtest.},
   author = {Julia Kaiwen Lau and Kelvin Kai Wen Kong and Julian Hao Yong and Per Hoong Tan and Zhou Yang and Zi Qian Yong and Joshua Chern Wey Low and Chun Yong Chong and Mei Kuan Lim and David Lo},
   city = {New York, NY, USA},
   doi = {10.1145/3597926.3598126},
   isbn = {9798400702211},
   booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Automated Speech Recognition,False Alarms,Software Testing},
   pages = {1169-1181},
   publisher = {Association for Computing Machinery},
   title = {Synthesizing Speech Test Cases with Text-to-Speech? An Empirical Study on the False Alarms in Automated Speech Recognition Testing},
   url = {https://doi.org/10.1145/3597926.3598126},
   year = {2023}
}
@inproceedings{Shrestha2023,
   abstract = {MATLAB/Simulink is a de-facto standard tool in several safety-critical industries such as automotive, aerospace, healthcare, and industrial automation for system modeling and analysis, compiling models to code, and deploying code to embedded hardware. On one hand, testing cyber-physical system (CPS) development tools such as MathWorks’ Simulink is important as a bug in the toolchain may propagate to the artifacts they produce. On the other hand, it is equally important to understand modeling practices and model evolution to support engineers and scientists as they are widely used in design, simulation, and verification of CPS models. Existing work in this area is limited by two main factors, i.e., (1) inefficiencies of state-of-the-art testing schemes in finding critical tool-chain bugs and (2) the lack of a reusable corpus of public Simulink models. In my thesis, I propose to (1) curate a large reusable corpus of Simulink models to help understand modeling practices and model evolution and (2) leverage such a corpus with deep-learning based language models to test the toolchain.},
   author = {Sohil Lal Shrestha},
   city = {New York, NY, USA},
   doi = {10.1145/3597926.3605233},
   isbn = {9798400702211},
   booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Cyber-physical system development,GPT-2,Simulink,deep learning,mining software repositories,model evolution,open-source,programming language modeling,tool chain bugs},
   pages = {1541-1545},
   publisher = {Association for Computing Machinery},
   title = {Harnessing Large Language Models for Simulink Toolchain Testing and Developing Diverse Open-Source Corpora of Simulink Models for Metric and Evolution Analysis},
   url = {https://doi.org/10.1145/3597926.3605233},
   year = {2023}
}
@inproceedings{Gong2023,
   abstract = {Random-based approaches and heuristics are commonly used in kernel concurrency testing due to the massive scale of modern kernels and corresponding interleaving space. The lack of accurate and scalable approaches to analyze concurrent kernel executions makes existing testing approaches heavily rely on expensive dynamic executions to measure the effectiveness of a new test. Unfortunately, the high cost incurred by dynamic executions limits the breadth of the exploration and puts latency pressure on finding effective concurrent test inputs and schedules, hindering the overall testing effectiveness.This paper proposes Snowcat, a kernel concurrency testing framework that generates effective test inputs and schedules using a learned kernel block-coverage predictor. Using a graph neural network, the coverage predictor takes a concurrent test input and scheduling hints and outputs a prediction on whether certain important code blocks will be executed. Using this predictor, Snowcat can skip concurrent tests that are likely to be fruitless and prioritize the promising ones for actual dynamic execution.After testing the Linux kernel for over a week, Snowcat finds ~17\% more potential data races, by prioritizing tests of more fruitful schedules than existing work would have chosen. Snowcat can also find effective test inputs that expose new concurrency bugs with higher probability (1.4~2.6), or reproduce known bugs more quickly (15) than state-of-art testing tools. More importantly, Snowcat is shown to be more efficient at reaching a desirable level of race coverage in the continuous setting, as the Linux kernel evolves from version to version. In total, Snowcat discovered 17 new concurrency bugs in Linux kernel 6.1, of which 13 are confirmed and 6 are fixed.},
   author = {Sishuai Gong and Dinglan Peng and Deniz Altnbüken and Pedro Fonseca and Petros Maniatis},
   city = {New York, NY, USA},
   doi = {10.1145/3600006.3613148},
   isbn = {9798400702297},
   booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
   keywords = {concurrency programming,kernel concurrency bugs,operating systems security,software testing and debugging},
   pages = {35-51},
   publisher = {Association for Computing Machinery},
   title = {Snowcat: Efficient Kernel Concurrency Testing using a Learned Coverage Predictor},
   url = {https://doi.org/10.1145/3600006.3613148},
   year = {2023}
}
@inproceedings{Zhu2023,
   abstract = {In this paper, Formal Language Processing (FLP) is explicitly defined as processing generalized formal language problems via Machine Learning (ML) including Deep Learning (DL). Temporal Logic (TL) Model Checking (MC) and TL Satisfiability Checking (SC) can be considered as the two formal language problems. How to deal with them via ML? First, an online way for approximate Linear Temporal Logic (LTL) MC using ML is pioneered. Second, an algorithmic framework for approximate LTL-MC based on Graph Neural Network (GNN) and Recursive Neural Network (RecNN) is designed. Third, the approximate LTL-SC technique is analyzed and discussed. On the basis of it, an early exploration aiming to FLP automatic verification of temporal logic occurs.},
   author = {Weijun Zhu and Yang Liu},
   city = {New York, NY, USA},
   doi = {10.1145/3603165.3607422},
   isbn = {9798400702334},
   booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2023},
   keywords = {Formal language processing,Machine learning,Model checking,Temporal logic},
   pages = {106-107},
   publisher = {Association for Computing Machinery},
   title = {Approximate Automatic Verification and Formal Language Processing},
   url = {https://doi.org/10.1145/3603165.3607422},
   year = {2023}
}
@inproceedings{Kaesmayr2023,
   abstract = {Digitalization affects public servants’ job demands and resources. The current paper investigates which measures managers can employ to optimize digital work on administrative procedures. Building on a previous study on the impact of digitalization on work experiences and performance in public agencies and on a literature research regarding leadership instruments, we derived two sets of measures on job design and digital communication management. The proposed measures were validated conducting expert interviews with actual and prospective managers from various municipal agencies. Overall, our analyses revealed heterogenous patterns regarding the prevalence of the proposed measures. Experts reported various potential benefits and challenges as well as suitable recommendations regarding their implementations which indicates that the proposed measures could indeed contribute to facilitating digital work on administrative procedures. Finally, the practical implications as well as limitations of our approach are discussed.},
   author = {Julia Kaesmayr and Sandra Mirlieb and Michael Schorn and Anna Steidle},
   city = {New York, NY, USA},
   doi = {10.1145/3603304.3603351},
   isbn = {9798400700064},
   booktitle = {Proceedings of the Central and Eastern European EDem and EGov Days 2023},
   keywords = {Competence,Implementation,Motivation,Personal Resources,Personnel Management},
   pages = {140-144},
   publisher = {Association for Computing Machinery},
   title = {Validating Personnel Management Interventions to Elicit Digital Competence and Affinity in Public Servants – A Qualitative Approach},
   url = {https://doi.org/10.1145/3603304.3603351},
   year = {2023}
}
@inproceedings{Ferrando2023,
   abstract = {Chatbots are here to stay, and are going to be deployed in various application domains. Unfortunately, amongst them, there are safety-critical ones. Thus, we need a way to guarantee our chatbots will always behave as expected. In this paper, we propose RV4Rasa, a Runtime Verification framework that monitors whether a given chatbot deviates from its expected behaviour, when the latter is formalised as an interaction protocol between the end-user and the chatbot. We present RV4Rasa, its engineering, and its instantiation to monitor chatbots implemented using the Rasa framework. After presenting RV4Rasa's structure, we report experiments that we carried out in a simulated robotic scenario, where a chatbot is used to support the design of a factory workfloor.},
   author = {Angelo Ferrando and Andrea Gatti and Viviana Mascardi},
   city = {New York, NY, USA},
   doi = {10.1145/3605159.3605855},
   isbn = {9798400702495},
   booktitle = {Proceedings of the 6th International Workshop on Verification and Monitoring at Runtime Execution},
   keywords = {ChatBot,Rasa,Runtime Verification},
   pages = {1-8},
   publisher = {Association for Computing Machinery},
   title = {RV4Rasa: A Formalism-Agnostic Runtime Verification Framework for Verifying ChatBots in Rasa},
   url = {https://doi.org/10.1145/3605159.3605855},
   year = {2023}
}
@inproceedings{Azeroual2023,
   abstract = {The volume of data in companies and in the private sector is already gigantic today. Mobile devices such as smartphones constantly collect data on all possible environmental conditions. When surfing the Internet, everyone leaves an endless digital trail. The Internet of Things (IoT) promises comprehensive networking of all everyday devices and production tools that surround people. Nevertheless, the modern knowledge society has to face the question of whether we are really actively using all the data or whether useful knowledge has increased as a result. Answering this question is not trivial. It is true that today's opportunities for exploring data, for transforming data into information and thus for gaining knowledge from it, are greater than ever. But it is also true that this new knowledge, which consists of the hidden connections in data, does not appear in our mind's eye on its own. We must explore it to bring it to the surface which is related to recognizing patterns in the world of data. In the last step, these patterns must be correctly interpreted. Predictive analytics (PA) is going exactly in this direction. They are currently one of the most important application areas of big data and are seen as the most actionable form of business intelligence (BI). Predictive analytics can be used for a variety of purposes, from predicting customer behaviour in sales and marketing to determining risk profiles for financing. Another widely known application is credit reporting, used by financial institutions to determine the likelihood that customers will repay future loans on time. It can also be used when working with big data in predicting user behaviour and opinion. In this regard, the purpose of this paper is to develop a predictive analytics-driven decision framework based on machine learning and data mining methods and techniques. To test it, we conducted an experiment for predicting sentiments and emotions in social media posts, as well as discussed topics and extracted keywords.},
   author = {Otmane Azeroual and Radka Nacheva and Anastasija Nikiforova and Uta Störl and Amel Fraisse},
   city = {New York, NY, USA},
   doi = {10.1145/3606305.3606309},
   isbn = {9798400700477},
   booktitle = {Proceedings of the 24th International Conference on Computer Systems and Technologies},
   pages = {42-53},
   publisher = {Association for Computing Machinery},
   title = {Predictive Analytics intelligent decision-making framework and testing it through sentiment analysis on Twitter data},
   url = {https://doi.org/10.1145/3606305.3606309},
   year = {2023}
}

@inproceedings{Happe2023,
   abstract = {The field of software security testing, more specifically penetration testing, requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential use of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of AI sparring partners.},
   author = {Andreas Happe and Jürgen Cito},
   city = {New York, NY, USA},
   doi = {10.1145/3611643.3613083},
   isbn = {9798400703270},
   booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {large language models,penetration testing,security testing},
   pages = {2082-2086},
   publisher = {Association for Computing Machinery},
   title = {Getting pwn’d by AI: Penetration Testing with Large Language Models},
   url = {https://doi.org/10.1145/3611643.3613083},
   year = {2023}
}
@inproceedings{Su2023,
   abstract = {This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.},
   author = {Chia-Yi Su and Aakash Bansal and Vijayanta Jain and Sepideh Ghanavati and Collin McMillan},
   city = {New York, NY, USA},
   doi = {10.1145/3611643.3613090},
   isbn = {9798400703270},
   booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {deduplication,java,language model,research tools},
   pages = {2152-2156},
   publisher = {Association for Computing Machinery},
   title = {A Language Model of Java Methods with Train/Test Deduplication},
   url = {https://doi.org/10.1145/3611643.3613090},
   year = {2023}
}
@inproceedings{Mandal2023,
   abstract = {Prior work has developed numerous systems that test the security and safety of smart homes. For these systems to be applicable in practice, it is necessary to test them with realistic scenarios that represent the use of the smart home, i.e., home automation, in the wild. This demo paper presents the technical details and usage of Helion, a system that uses n-gram language modeling to learn the regularities in user-driven programs, i.e., routines developed for the smart home, and predicts natural scenarios of home automation, i.e., event sequences that reflect realistic home automation usage. We demonstrate the HelionHA platform, developed by integrating Helion with the popular Home Assistant smart home platform. HelionHA allows an end-to-end exploration of Helion’s scenarios by executing them as test cases with real and virtual smart home devices.},
   author = {Prianka Mandal and Sunil Manandhar and Kaushal Kafle and Kevin Moran and Denys Poshyvanyk and Adwait Nadkarni},
   city = {New York, NY, USA},
   doi = {10.1145/3611643.3613095},
   isbn = {9798400703270},
   booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Home Assistant,Home Automation,Language Models,Trigger-Action Programming},
   pages = {2147-2151},
   publisher = {Association for Computing Machinery},
   title = {Helion: Enabling Natural Testing of Smart Homes},
   url = {https://doi.org/10.1145/3611643.3613095},
   year = {2023}
}
@inproceedings{Cao2023,
   abstract = {Coreference resolution (CR) is a task to resolve different expressions (e.g., named entities, pronouns) that refer to the same real-world entity/event. It is a core natural language processing (NLP) component that underlies and empowers major downstream NLP applications such as machine translation, chatbots, and question-answering. Despite its broad impact, the problem of testing CR systems has rarely   been studied. A major difficulty is the shortage of a labeled dataset   for testing. While it is possible to feed arbitrary sentences as test inputs to a CR system, a test oracle that captures their expected test outputs (coreference relations) is hard to define automatically. To address the challenge, we propose Crest, an automated testing methodology for CR systems. Crest uses constituency and dependency relations to construct pairs of test inputs subject to the same coreference. These relations can be leveraged to define the metamorphic relation for metamorphic testing. We compare Crest with five state-of-the-art test generation baselines on two popular CR systems, and apply them to generate tests from 1,000 sentences randomly sampled from CoNLL-2012, a popular dataset for coreference resolution. Experimental results show that Crest outperforms baselines significantly. The issues reported by Crest are all true   positives (i.e., 100\% precision), compared with 63\% to 75\% achieved by the baselines.},
   author = {Jialun Cao and Yaojie Lu and Ming Wen and Shing-Chi Cheung},
   city = {New York, NY, USA},
   doi = {10.1145/3611643.3616258},
   isbn = {9798400703270},
   booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Coreference resolution testing,Metamorphic testing,SE4AI},
   pages = {107-119},
   publisher = {Association for Computing Machinery},
   title = {Testing Coreference Resolution Systems without Labeled Test Sets},
   url = {https://doi.org/10.1145/3611643.3616258},
   year = {2023}
}
@inproceedings{Yu2023,
   abstract = {Named entity recognition (NER) systems have seen rapid progress in recent years due to the development of deep neural networks. These systems are widely used in various natural language processing applications, such as information extraction, question answering, and sentiment analysis. However, the complexity and intractability of deep neural networks can make NER systems unreliable in certain circumstances, resulting in incorrect predictions. For example, NER systems may misidentify female names as chemicals or fail to recognize the names of minority groups, leading to user dissatisfaction. To tackle this problem, we introduce TIN, a novel, widely applicable approach for automatically testing and repairing various NER systems. The key idea for automated testing is that the NER predictions of the same named entities under similar contexts should be identical. The core idea for automated repairing is that similar named entities should have the same NER prediction under the same context. We use TIN to test two SOTA NER models and two commercial NER APIs, i.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues reported by TIN and find that 702 are erroneous issues, leading to high precision (85.0\%-93.4\%) across four categories of NER errors: omission, over-labeling, incorrect category, and range error. For automated repairing, TIN achieves a high error reduction rate (26.8\%-50.6\%) over the four systems under test, which successfully repairs 1,056 out of the 1,877 reported NER errors.},
   author = {Boxi Yu and Yiyan Hu and Qiuyang Mang and Wenhan Hu and Pinjia He},
   city = {New York, NY, USA},
   doi = {10.1145/3611643.3616295},
   isbn = {9798400703270},
   booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {AI software,Metamorphic testing,named entity recognition,software repairing},
   pages = {883-894},
   publisher = {Association for Computing Machinery},
   title = {Automated Testing and Improvement of Named Entity Recognition Systems},
   url = {https://doi.org/10.1145/3611643.3616295},
   year = {2023}
}
@inproceedings{Ye2023,
   abstract = {Random test case generation, or fuzzing, is a viable means for uncovering compiler bugs. Unfortunately, compiler fuzzing can be time-consuming and inefficient with purely randomly generated test cases due to the complexity of modern compilers. We present COMFUZZ, a focused compiler fuzzing framework. COMFUZZ aims to improve compiler fuzzing efficiency by focusing on testing components and language features that are likely to trigger compiler bugs. Our key insight is human developers tend to make common and repeat errors across compiler implementations; hence, we can leverage the previously reported buggy-exposing test cases of a programming language to test a new compiler implementation. To this end, COMFUZZ employs deep learning to learn a test program generator from open-source projects hosted on GitHub. With the machine-generated test programs in place, COMFUZZ then leverages a set of carefully designed mutation rules to improve the coverage and bug-exposing capabilities of the test cases. We evaluate COMFUZZ on 11 compilers for JS and Java programming languages. Within 260 hours of automated testing runs, we discovered 33 unique bugs across nine compilers, of which 29 have been confirmed and 22, including an API documentation defect, have already been fixed by the developers. We also compared COMFUZZ to eight prior fuzzers on four evaluation metrics. In a 24-hour comparative test, COMFUZZ uncovers at least 1.5 more bugs than the state-of-the-art baselines.},
   author = {Guixin Ye and Tianmin Hu and Zhanyong Tang and Zhenye Fan and Shin Hwei Tan and Bo Zhang and Wenxiang Qian and Zheng Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3611643.3616332},
   isbn = {9798400703270},
   booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Compiler,Deep learning,Fuzzing,Guided testing,Historical bug},
   pages = {1127-1139},
   publisher = {Association for Computing Machinery},
   title = {A Generative and Mutational Approach for Synthesizing Bug-Exposing Test Cases to Guide Compiler Fuzzing},
   url = {https://doi.org/10.1145/3611643.3616332},
   year = {2023}
}
@inproceedings{Gu2023,
   abstract = {Modern optimizing compilers are among the most complex software systems humans build. One way to identify subtle compiler bugs is fuzzing. Both the quantity and the quality of testcases are crucial to the performance of fuzzing. Traditional testcase-generation methods, such as Csmith and YARPGen, have been proven successful at discovering compiler bugs. However, such generated testcases have limited coverage and quantity. In this paper, we present a code generation method for compiler testing based on LLM to maximize the quality and quantity of the generated code. In particular, to avoid undefined behavior and syntax errors in generated testcases, we design a filter strategy to clean the source code, preparing a high-quality dataset for the model training. Besides, we present a seed schedule strategy to improve code generation. We apply the method to test the Golang compiler and the result shows that our pipeline outperforms previous methods both qualitatively and quantitatively. It produces testcases with an average coverage of 3.38\%, in contrast to the testcases generated by GoFuzz, which have an average coverage of 0.44\%. Moreover, among all the generated testcases, only 2.79\% exhibited syntax errors, and none displayed undefined behavior.},
   author = {Qiuhan Gu},
   city = {New York, NY, USA},
   doi = {10.1145/3611643.3617850},
   isbn = {9798400703270},
   booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Code generation,Compiler testing,Go language,Large model},
   pages = {2201-2203},
   publisher = {Association for Computing Machinery},
   title = {LLM-Based Code Generation Method for Golang Compiler Testing},
   url = {https://doi.org/10.1145/3611643.3617850},
   year = {2023}
}
@inproceedings{Sohrawardi2024,
   abstract = {The evolving landscape of manipulated media, including the threat of deepfakes, has made information verification a daunting challenge for journalists. Technologists have developed tools to detect deepfakes, but these tools can sometimes yield inaccurate results, raising concerns about inadvertently disseminating manipulated content as authentic news. This study examines the impact of unreliable deepfake detection tools on information verification. We conducted role-playing exercises with 24 US journalists, immersing them in complex breaking-news scenarios where determining authenticity was challenging. Through these exercises, we explored questions regarding journalists’ investigative processes, use of a deepfake detection tool, and decisions on when and what to publish. Our findings reveal that journalists are diligent in verifying information, but sometimes rely too heavily on results from deepfake detection tools. We argue for more cautious release of such tools, accompanied by proper training for users to mitigate the risk of unintentionally propagating manipulated content as real news.},
   author = {Saniat Javid Sohrawardi and Y Kelly Wu and Andrea Hickerson and Matthew Wright},
   city = {New York, NY, USA},
   doi = {10.1145/3613904.3641973},
   isbn = {9798400703300},
   booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
   keywords = {deepfake,journalism,role-play,verification},
   publisher = {Association for Computing Machinery},
   title = {Dungeons and Deepfakes: Using scenario-based role-play to study journalists' behavior towards using AI-based verification tools for video content},
   url = {https://doi.org/10.1145/3613904.3641973},
   year = {2024}
}
@inproceedings{Arawjo2024,
   abstract = {Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.},
   author = {Ian Arawjo and Chelse Swoopes and Priyan Vaithilingam and Martin Wattenberg and Elena L Glassman},
   city = {New York, NY, USA},
   doi = {10.1145/3613904.3642016},
   isbn = {9798400703300},
   booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
   keywords = {auditing,language models,prompt engineering,toolkits,visual programming environments},
   publisher = {Association for Computing Machinery},
   title = {ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing},
   url = {https://doi.org/10.1145/3613904.3642016},
   year = {2024}
}
@inproceedings{Belghith2024,
   abstract = {As generative AI rapidly enters everyday life, educational interventions for teaching about AI need to cater to how young people, in particular middle schoolers who are at a critical age for reasoning skills and identity formation, conceptualize and interact with AI. We conducted nine focus groups with 24 middle school students to elicit their interests, conceptions of, and approaches to a popular generative AI tool, ChatGPT. We highlight a) personally and culturally-relevant topics to this population, b) three distinct approaches in students’ open-ended interactions with ChatGPT: AI testing-oriented, AI socializing-oriented, and content exploring-oriented, and 3) an improved understanding of youths’ conceptions and misconceptions of generative AI. While misconceptions highlight gaps in understanding what generative AI is and how it works, most learners show interest in learning about what AI is and what it can do. We discuss the implications of these conceptions for designing AI literacy interventions in museums.},
   author = {Yasmine Belghith and Atefeh Mahdavi Goloujeh and Brian Magerko and Duri Long and Tom Mcklin and Jessica Roberts},
   city = {New York, NY, USA},
   doi = {10.1145/3613904.3642332},
   isbn = {9798400703300},
   booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
   keywords = {AI literacy,ChatGPT,Child-AI Interaction,Conceptions of AI,Conversational Agents (CAs),Generative AI,Informal Learning,Large Language Models (LLMs)},
   publisher = {Association for Computing Machinery},
   title = {Testing, Socializing, Exploring: Characterizing Middle Schoolers’ Approaches to and Conceptions of ChatGPT},
   url = {https://doi.org/10.1145/3613904.3642332},
   year = {2024}
}
@inproceedings{Adam2023,
   abstract = {High-Performance Computing (HPC) is currently facing significant challenges. The hardware pressure has become increasingly difficult to manage due to the lack of parallel abstractions in applications. As a result, parallel programs must undergo drastic evolution to effectively exploit underlying hardware parallelism. Failure to do so results in inefficient code. In this constrained environment, parallel runtimes play a critical role, and their testing becomes crucial. This paper focuses on the MPI interface and leverages the MPI binding tools to develop a multi-language test suite for MPI. By doing so and building on previous work from the Forum document editors, we implement a systematic testing of MPI symbols in the context of the Parallel Computing Validation System (PCVS), which is an HPC validation platform dedicated to running and managing test suites at scale. We first describe PCVS, then outline the process of generating the MPI API test suite, and finally, run these tests at scale. All data sets, code generators, and implementations are made available in open-source to the community. We also set up a dedicated website showcasing the results, which self-updates thanks to the Spack package manager.},
   author = {Julien Adam and Jean-Baptiste Besnard and Paul Canat and Hugo Taboada and Adrien Roussel and Marc Pérache and Julien Jaeger and Sameer Shende},
   city = {New York, NY, USA},
   doi = {10.1145/3615318.3615329},
   isbn = {9798400709135},
   booktitle = {Proceedings of the 30th European MPI Users' Group Meeting},
   keywords = {HPC,MPI,api,test,validation},
   publisher = {Association for Computing Machinery},
   title = {Generating and Scaling a Multi-Language Test-Suite for MPI},
   url = {https://doi.org/10.1145/3615318.3615329},
   year = {2023}
}
@inproceedings{Karakas2023,
   abstract = {The issue of fairness testing in machine learning models has become popular due to rising concerns about potential bias and discrimination, as these models continue to permeate end-user applications. However, achieving an accurate and reliable measurement of the fairness performance of machine learning models remains a substantial challenge. Representative sampling plays a pivotal role in ensuring accurate fairness assessments and providing insight into the underlying dynamics of data, unlike biased or random sampling approaches. In our study, we introduce our approach, namely RSFair, which adopts the representative sampling method to comprehensively evaluate the fairness performance of a trained machine learning model. Our research findings on two datasets indicate that RSFair yields more accurate and reliable results, thus improving the efficiency of subsequent search steps, and ultimately the fairness performance of the model. With the usage of Orthogonal Matching Pursuit (OMP) and K-Singular Value Decomposition (K-SVD) algorithms for representative sampling, RSFair significantly improves the detection of discriminatory inputs by 76\% and the fairness performance by 53\% compared to other search-based approaches in the literature.},
   author = {Umutcan Karakas and Ayse Tosun},
   city = {New York, NY, USA},
   doi = {10.1145/3617555.3617871},
   isbn = {9798400703751},
   booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
   keywords = {fairness testing,machine learning,representative sampling},
   pages = {54-63},
   publisher = {Association for Computing Machinery},
   title = {Automated Fairness Testing with Representative Sampling},
   url = {https://doi.org/10.1145/3617555.3617871},
   year = {2023}
}
@inproceedings{Tihanyi2023,
   abstract = {This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. We have associated the identified vulnerabilities with Common Weakness Enumeration (CWE) numbers. We make the source code available for the 112,000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms. Our study unveiled that according to ESBMC, 51.24\% of the programs generated by GPT-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.},
   author = {Norbert Tihanyi and Tamas Bisztray and Ridhi Jain and Mohamed Amine Ferrag and Lucas C Cordeiro and Vasileios Mavroeidis},
   city = {New York, NY, USA},
   doi = {10.1145/3617555.3617874},
   isbn = {9798400703751},
   booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
   keywords = {Artificial Intelligence,Dataset,Formal Verification,Large Language Models,Software Security,Vulnerability Classification},
   pages = {33-43},
   publisher = {Association for Computing Machinery},
   title = {The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification},
   url = {https://doi.org/10.1145/3617555.3617874},
   year = {2023}
}
@article{Biagiola2023,
   abstract = {Deep Learning (DL) techniques help software developers thanks to their ability to learn from historical information which is useful in several program analysis and testing tasks (e.g., malware detection, fuzz testing, bug-finding, and type-checking). DL-based software systems are also increasingly adopted in safety-critical domains, such as autonomous driving, medical diagnosis, and aircraft collision avoidance systems. In particular, testing the correctness and reliability of DL-based systems is paramount, since a failure of such systems would cause a significant safety risk for the involved people and/or environment. The 4th International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest 2023) was co-located with the 45th International Conference on Software Engineering (ICSE), with the goal of targeting research at the intersection of software engineering and deep learning and devise novel approaches and tools to ensure the interpretability and dependability of software systems that depends on DL components.},
   author = {Matteo Biagiola and Nicolás Cardozo and Donghwan Shin and Foutse Khomh and Andrea Stocco and Vincenzo Riccio},
   city = {New York, NY, USA},
   doi = {10.1145/3617946.3617953},
   issn = {0163-5948},
   issue = {4},
   journal = {SIGSOFT Softw. Eng. Notes},
   month = {10},
   pages = {39-40},
   publisher = {Association for Computing Machinery},
   title = {Summary of the Fourth International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest 2023)},
   volume = {48},
   url = {https://doi.org/10.1145/3617946.3617953},
   year = {2023}
}
@inproceedings{Miao2024,
   abstract = {This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree-based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8 for distributed LLM inference and by 2.6-3.5 for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/},
   author = {Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Zhengxin Zhang and Rae Ying Yee Wong and Alan Zhu and Lijie Yang and Xiaoxiang Shi and Chunan Shi and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},
   city = {New York, NY, USA},
   doi = {10.1145/3620666.3651335},
   isbn = {9798400703867},
   booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
   keywords = {large language model serving,speculative decoding,token tree verification},
   pages = {932-949},
   publisher = {Association for Computing Machinery},
   title = {SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification},
   url = {https://doi.org/10.1145/3620666.3651335},
   year = {2024}
}
@inproceedings{daVeiga2023,
   abstract = {The advent of generative AI artworks has paved the way for ground-breaking explorations in the realm of digital creativity. This article delves into the multifaceted dimensions of G.O.D., an abbreviation for the art project Generative Ominous Dataset. G.O.D. aims at critically engaging with contemporary AI generative image systems and their intricate interplay with copyright issues, artistic autonomy, and the ethical implications of data collection, unravelling its conceptual underpinnings and its implications for the broader discourse on artificial intelligence, artistic agency, and the evolving contours of digital art. G.O.D. is a generative artwork, entirely coded in Processing, and developed within a/r/cography, a creative research methodology. G.O.D. scrutinizes and questions the ethics of contemporary text-to-image AI-based systems, such as Midjourney, DALL-E, or Firefly. These systems have been at the centre of controversies concerning the datasets used for their training, which encompass online sourced copyrighted materials, without authorization or attribution, masking questionable approaches with technological dazzlement. Many artists and authors find their works repurposed by these systems for the mass production of digital derivatives. G.O.D. aims at critically exposing art audiences to these concerns.},
   author = {Pedro da Veiga},
   city = {New York, NY, USA},
   doi = {10.1145/3623462.3623475},
   isbn = {9798400708367},
   booktitle = {Proceedings of the 20th International Conference on Culture and Computer Science: Code and Materiality},
   publisher = {Association for Computing Machinery},
   title = {Generative Ominous Dataset: Testing the Current Public Perception of Generative Art},
   url = {https://doi.org/10.1145/3623462.3623475},
   year = {2023}
}
@article{Breuer2024,
   abstract = {Evaluating retrieval performance without editorial relevance judgments is challenging, but instead, user interactions can be used as relevance signals. Living labs offer a way for small-scale platforms to validate information retrieval systems with real users. If enough user interaction data are available, then click models can be parameterized from historical sessions to evaluate systems before exposing users to experimental rankings. However, interaction data are sparse in living labs, and little is studied about how click models can be validated for reliable user simulations when click data are available in moderate amounts.This work introduces an evaluation approach for validating synthetic usage data generated by click models in data-sparse human-in-the-loop environments like living labs. We ground our methodology on the click model’s estimates about a system ranking compared to a reference ranking for which the relative performance is known. Our experiments compare different click models and their reliability and robustness as more session log data become available. In our setup, simple click models can reliably determine the relative system performance with already 20 logged sessions for 50 queries. In contrast, more complex click models require more session data for reliable estimates, but they are a better choice in simulated interleaving experiments when enough session data are available. While it is easier for click models to distinguish between more diverse systems, it is harder to reproduce the system ranking based on the same retrieval algorithm with different interpolation weights. Our setup is entirely open, and we share the code to reproduce the experiments.},
   author = {Timo Breuer and Norbert Fuhr and Philipp Schaer},
   city = {New York, NY, USA},
   doi = {10.1145/3623640},
   issn = {1936-1955},
   issue = {1},
   journal = {J. Data and Information Quality},
   keywords = {Synthetic usage data,click signals,living labs,system evaluation},
   month = {3},
   publisher = {Association for Computing Machinery},
   title = {Validating Synthetic Usage Data in Living Lab Environments},
   volume = {16},
   url = {https://doi.org/10.1145/3623640},
   year = {2024}
}
@inproceedings{Guerino2023,
   abstract = {Context: Software testing is a very relevant step in quality assurance, but developers frequently overlook it. We pursued testing automation to minimize the impact of missing test cases in a software project. Problem: However, for Python programs, there are not many tools able to fully automate the generation of unit test sets, and the one available demands studies to provide evidence of the quality of the generated test set. Solution: This work aims to evaluate the quality of different unit test generation algorithms for Python, implemented in a tool named Pynguin. Method: In the analysis of the selected programs, the Pynguin test generation tool is executed with each of its algorithms, including random, as a way to generate complete unit test sets. Then, we evaluate each generated test set’s efficacy, efficiency, and cost. We use four different fault models, implemented by four mutation testing tools, to measure efficacy. We use line and branch coverage to measure efficiency, the number of test cases, and test set execution time to measure cost. Summary of Results: We identified that RANDOM test set performed worst concerning all evaluated aspects, DYNAMOSA and MOSA, the two algorithms that generate the best test sets regarding efficacy, efficiency, and cost. By combining all Pynguin smart algorithms (DYNAMOSA, MIO, MOSA, WHOLE-SUITE), the resultant test set overcomes the individual test sets efficiency by around 1\%, for coverage and efficacy by 4.5\% on average, concerning previous mutation score, at a reasonable cost, without a test set minimization.},
   author = {Lucca Guerino and Auri Vincenzi},
   city = {New York, NY, USA},
   doi = {10.1145/3624032.3624034},
   isbn = {9798400716294},
   booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
   keywords = {automated test generation,coverage testing,experimental software engineering,mutation testing,software testing,testing tools},
   pages = {5-14},
   publisher = {Association for Computing Machinery},
   title = {An Experimental Study Evaluating Cost, Adequacy, and Effectiveness of Pynguin's Test Sets},
   url = {https://doi.org/10.1145/3624032.3624034},
   year = {2023}
}
@inproceedings{Marques2023,
   abstract = {The software testing process is important to ensure quality, especially in mobile applications that have characteristics such as platform diversity, hardware limitations, portability, frequent updates, among others. Software companies need to deliver quickly with increasingly complex functionalities; therefore, the testing process must be efficient and avoid bottlenecks, such as the creation of test cases. Among the solutions found in the literature, the state-of-the-art tool DRL-MOBTEST aims to assist in the automatic generation of test cases for mobile applications using deep reinforcement learning. The experiments show promising results; however, the tool has some limitations, such as generating duplicate and less readable tests. In this article, we present SelectNLTest, a module developed to identify and remove similar test scripts and transcribe the test cases generated by the tool using Natural Language Processing techniques. This allows for the removal of similar tests and improves the readability and understanding of the generated test cases for professionals in the field. The results of the experiments showed that, in 10 Android applications used in the comparative analysis, the proposed module reduced the number of test cases by 58.3\% while maintaining code coverage and application functionality.},
   author = {João Paulo Marques and Mônica Lima and Bruno Souza and Eloise Miranda and André Santos and Eliane Collins},
   city = {New York, NY, USA},
   doi = {10.1145/3624032.3624043},
   isbn = {9798400716294},
   booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
   keywords = {automatic test generation,coverage analysis,deep reinforcement learning,natural language},
   pages = {77-85},
   publisher = {Association for Computing Machinery},
   title = {SelectNLTest - Selection and natural language rewriting of test cases generated by the DRL-MOBTEST tool},
   url = {https://doi.org/10.1145/3624032.3624043},
   year = {2023}
}
@inproceedings{Drake2024,
   abstract = {We present the best practices and collaborative methods used during the DARPA SHADE program, exploring the use of AI agents to advise diplomatic negotiations. In this accelerated 18-month program, we created an environment for seven research performers to collaborate in the development of an AI Gym. This AI Gym provided an area in which AI developed by the various teams could compete, negotiate, and explain their intentions as they played the game of Diplomacy, allowing us to evaluate their abilities against other digital and human agents.},
   author = {Justin Drake and Niall Gaffney and Nathan Freeman and Erik Ferlanti and John Fonner},
   city = {New York, NY, USA},
   doi = {10.1145/3626203.3670579},
   isbn = {9798400704192},
   booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing},
   keywords = {AI Gym,Collaborative Development,Diplomacy,Explainable AI},
   publisher = {Association for Computing Machinery},
   title = {Human-powered AI Gym: Lessons Learned as the Test and Evaluation Team for the DARPA SHADE Program: Human-powered AI Gym},
   url = {https://doi.org/10.1145/3626203.3670579},
   year = {2024}
}
@inproceedings{Fernandez2024,
   abstract = {As AI-generated code promises to become an increasingly relied upon tool for software developers, there is a temptation to call for significant changes to early computer science curricula. A move from syntax-focused topics in CS1 toward abstraction and high-level application design seems motivated by the new large language models (LLMs) recently made available. In this position paper however, we advocate for an approach more informed by the AI itself - teaching early CS learners not only how to use the tools but also how to better understand them. Novice programmers leveraging AI-code-generation without proper understanding of syntax or logic can create "black box" code with significant security vulnerabilities. We outline methods for integrating basic AI knowledge and traditional software verification steps into CS1 along with LLMs, which will better prepare students for software development in professional settings.},
   author = {Amanda S Fernandez and Kimberly A Cornell},
   city = {New York, NY, USA},
   doi = {10.1145/3626252.3630817},
   isbn = {9798400704239},
   booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
   keywords = {ai,artificial intelligence,code generation,copilot,cs1,gpt-4,introductory programming,large language model,llm,machine learning,novice programmers,programming,prompt engineering,secure code,software verification},
   pages = {345-351},
   publisher = {Association for Computing Machinery},
   title = {CS1 with a Side of AI: Teaching Software Verification for Secure Code in the Era of Generative AI},
   url = {https://doi.org/10.1145/3626252.3630817},
   year = {2024}
}
@inproceedings{Chen2024-4,
   abstract = {Dataset search, or more specifically, ad hoc dataset retrieval which is a trending specialized IR task, has received increasing attention in both academia and industry. While methods and systems continue evolving, existing test collections for this task exhibit shortcomings, particularly suffering from lexical bias in pooling and limited to keyword-style queries for evaluation. To address these limitations, in this paper, we construct ACORDAR 2.0, a new test collection for this task which is also the largest to date. To reduce lexical bias in pooling, we adapt dense retrieval models to large structured data, using them to find an extended set of semantically relevant datasets to be annotated. To diversify query forms, we employ a large language model to rewrite keyword queries into high-quality question-style queries. We use the test collection to evaluate popular sparse and dense retrieval models to establish a baseline for future studies. The test collection and source code are publicly available.},
   author = {Qiaosheng Chen and Weiqing Luo and Zixian Huang and Tengteng Lin and Xiaxia Wang and Ahmet Soylu and Basil Ell and Baifan Zhou and Evgeny Kharlamov and Gong Cheng},
   city = {New York, NY, USA},
   doi = {10.1145/3626772.3657866},
   isbn = {9798400704314},
   booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {ad hoc dataset retrieval,dataset search,rdf,test collection},
   pages = {303-312},
   publisher = {Association for Computing Machinery},
   title = {ACORDAR 2.0: A Test Collection for Ad Hoc Dataset Retrieval with Densely Pooled Datasets and Question-Style Queries},
   url = {https://doi.org/10.1145/3626772.3657866},
   year = {2024}
}
@inproceedings{Singhal2023,
   abstract = {Before implementing a function, programmers are encouraged to write a suite of test cases that specify its intended behaviour on several inputs. A suite of tests is thorough if any buggy implementation fails at least one of these tests. We posit that as the proportion of code generated by Large Language Models (LLMs) grows, so must the ability of students to create test suites that are thorough enough to detect subtle bugs in such code. Our paper makes two contributions. First, we demonstrate how difficult it can be to create thorough tests for LLM-generated code by evaluating 27&nbsp;test suites from a public dataset (EvalPlus). Second, by identifying deficiencies in these test suites, we propose strategies for improving the ability of students to develop thorough test suites for LLM-generated code.},
   author = {Shreya Singhal and Viraj Kumar},
   city = {New York, NY, USA},
   doi = {10.1145/3627217.3627238},
   isbn = {9798400708404},
   booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
   pages = {108-111},
   publisher = {Association for Computing Machinery},
   title = {Creating Thorough Tests for AI-Generated Code is Hard},
   url = {https://doi.org/10.1145/3627217.3627238},
   year = {2023}
}
@article{Shoufan2023,
   abstract = {With the immense interest in ChatGPT worldwide, education has seen a mix of both excitement and skepticism. To properly evaluate its impact on education, it is crucial to understand how far it can help students without prior knowledge answer assessment questions. This study aims to address this question as well as the impact of the question type. We conducted multiple experiments with computer engineering students (experiment group: n=41 to 56), who were asked to use ChatGPT to answer previous test questions before learning about the related topics. Their scores were then compared with the scores of previous-term students who answered the same questions in a quiz or exam setting (control group: n=24 to 61). The results showed a wide range of effect sizes, from -2.55 to 1.23, depending on the question type and content. The experiment group performed best answering code analysis and conceptual questions but struggled with code completion and questions that involved images. However, the performance in code generation tasks was inconsistent. Overall, the ChatGPT group’s answers lagged slightly behind the control group’s answers with an effect size of -0.16. We conclude that ChatGPT, at least in the field of this study, is not yet ready to rely on by students who do not have sufficient background to evaluate generated answers. We suggest that educators try using ChatGPT and educate students on effective questioning techniques and how to assess the generated responses. This study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.},
   author = {Abdulhadi Shoufan},
   city = {New York, NY, USA},
   doi = {10.1145/3628162},
   issue = {4},
   journal = {ACM Trans. Comput. Educ.},
   keywords = {ChatGPT,large language models},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {Can Students without Prior Knowledge Use ChatGPT to Answer Test Questions? An Empirical Study},
   volume = {23},
   url = {https://doi.org/10.1145/3628162},
   year = {2023}
}
@inproceedings{Cardoso2023,
   abstract = {The industry of large-scale software for mobile devices, such as the Android operating system, presents significant challenges regarding software validation and testing. This is due to the need to test on various devices, operating system versions, connections, and different hardware configurations. As a result, the manual creation of test cases can be a time-consuming process, and test cases can become outdated with updates in the Android version. To tackle these challenges, automatic test case generation emerges as an effective solution to streamline test creation and updates. In this context, Artificial Intelligence (AI) techniques, such as Deep Reinforcement Learning (DRL), have been explored to optimize this process and ensure adequate coverage of system requirements. This study evaluated the performance of the DRL state-of-the-art tool for test case generation DRL-MOBTEST [3] in an industry scenario context to generate test cases for Android functional applications (apps). The tool was performed in nine native apps (clock, maps, calculator, wallpaper, calendar, contacts, YouTube, drive, and files) regarding the functionalities coverage. The results showed a coverage range of 74.43\%, and we compared it with the random Android SDK tool Monkey in five applications, revealing a trend of 63.52\% improvement. The DRL-MOBTEST tool achieved the coverage of basic application paths through the creation of different test input types, such as symbols, numbers, and letters. It enables professionals to focus on complex scenarios and improve software quality across different devices and hardware configurations. However, it’s worth noting that human supervision is still necessary despite the advances offered by automated tools.},
   author = {Ana Paula Cardoso and Cleicy Priscilla Santos and Eliane Collins and Kelen Lima and Pablo Quiroga and Marlon Griego},
   city = {New York, NY, USA},
   doi = {10.1145/3629479.3629503},
   isbn = {9798400707865},
   booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
   keywords = {AI,Android,Automatic Test Generation,Reinforcement Learning,Test Automation},
   pages = {228-235},
   publisher = {Association for Computing Machinery},
   title = {Evaluation of Automatic Test Case Generation for the Android Operating System using Deep Reinforcement Learning},
   url = {https://doi.org/10.1145/3629479.3629503},
   year = {2023}
}

@inproceedings{Pornphol2024,
   abstract = {The potential of using large language model artificial intelligence systems to generate program codes for application development is significant. Database codes in SQL (Structured Query Language), which is the standard relational database language, can be generated by such systems. Generative AI systems know database languages syntax through their training data and the text patterns from various sources that include SQL queries and related text. Thus, the generated codes may not be perfect and need verification before usage. This paper verifies the relational completeness of the SQL codes generated by ChatGPT, one of the most widely used large language model systems. Relational algebra operators are used for the relational complete verification. An equivalent relational calculus statement is generated for each SQL and relational algebra statement. The results confirmed that ChatGPT has the ability to generate relational complete SQL, relational algebra, and relational calculus codes.},
   author = {Putsadee Pornphol and Suphamit Chittayasothorn},
   city = {New York, NY, USA},
   doi = {10.1145/3634814.3634817},
   isbn = {9798400708534},
   booktitle = {Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference},
   keywords = {ChatGPT,Relational Algebra,Relational Calculus,SQL},
   pages = {17-22},
   publisher = {Association for Computing Machinery},
   title = {Verification of Relational Database Languages Codes Generated by ChatGPT},
   url = {https://doi.org/10.1145/3634814.3634817},
   year = {2024}
}

@inproceedings{Kim2024-3,
   abstract = {The widespread adoption of REST APIs, coupled with their growing complexity and size, has led to the need for automated REST API testing tools. Current tools focus on the structured data in REST API specifications but often neglect valuable insights available in unstructured natural-language descriptions in the specifications, which leads to suboptimal test coverage. Recently, to address this gap, researchers have developed techniques that extract rules from these human-readable descriptions and query knowledge bases to derive meaningful input values. However, these techniques are limited in the types of rules they can extract and prone to produce inaccurate results. This paper presents RESTGPT, an innovative approach that leverages the power and intrinsic context-awareness of Large Language Models (LLMs) to improve REST API testing. RESTGPT takes as input an API specification, extracts machine-interpretable rules, and generates example parameter values from natural-language descriptions in the specification. It then augments the original specification with these rules and values. Our evaluations indicate that RESTGPT outperforms existing techniques in both rule extraction and value generation. Given these promising results, we outline future research directions for advancing REST API testing through LLMs.},
   author = {Myeongsoo Kim and Tyler Stennett and Dhruv Shah and Saurabh Sinha and Alessandro Orso},
   city = {New York, NY, USA},
   doi = {10.1145/3639476.3639769},
   isbn = {9798400705007},
   booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
   keywords = {OpenAPI specification analysis,large language models for testing},
   pages = {37-41},
   publisher = {Association for Computing Machinery},
   title = {Leveraging Large Language Models to Improve REST API Testing},
   url = {https://doi.org/10.1145/3639476.3639769},
   year = {2024}
}
@inproceedings{Sapozhnikov2024,
   abstract = {Writing software tests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces Test-Spark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (https://github.com/JetBrains-Research/TestSpark), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. Demo video: https://youtu.be/0F4PrxWfiXo},
   author = {Arkadii Sapozhnikov and Mitchell Olsthoorn and Annibale Panichella and Vladimir Kovalenko and Pouria Derakhshanfar},
   city = {New York, NY, USA},
   doi = {10.1145/3639478.3640024},
   isbn = {9798400705021},
   booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
   keywords = {intellij idea plugin,large language models,unit test generation},
   pages = {30-34},
   publisher = {Association for Computing Machinery},
   title = {TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion},
   url = {https://doi.org/10.1145/3639478.3640024},
   year = {2024}
}
@inproceedings{Xue2024-2,
   abstract = {Despite the promise of automation, general-purpose Large Language Models (LLMs) face difficulties in generating complete and accurate test cases from informal software requirements, primarily due to challenges in interpreting unstructured text and producing diverse, relevant scenarios. This paper argues that incorporating domain knowledge significantly improves LLM performance in test case generation. We report on the successful deployment of our LLM-powered tool, LLM4Fin, in the FinTech domain, showcasing the crucial role of domain knowledge in addressing the aforementioned challenges. We demonstrate two methods for integrating domain knowledge: implicit incorporation through model fine-tuning, and explicit incorporation with algorithm design. This combined approach delivers remarkable results, achieving up to 98.18\% improvement in test scenario coverage and reducing generation time from 20 minutes to 7 seconds.},
   author = {Zhiyi Xue and Liangguo Li and Senyue Tian and Xiaohong Chen and Pingping Li and Liangyu Chen and Tingting Jiang and Min Zhang},
   city = {New York, NY, USA},
   doi = {10.1145/3639478.3643087},
   isbn = {9798400705021},
   booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
   pages = {314-315},
   publisher = {Association for Computing Machinery},
   title = {Domain Knowledge is All You Need: A Field Deployment of LLM-Powered Test Case Generation in FinTech Domain},
   url = {https://doi.org/10.1145/3639478.3643087},
   year = {2024}
}
@inproceedings{Plein2024,
   abstract = {Tests suites are a key ingredient in various software automation tasks. Recently, various studies [4] have demonstrated that they are paramount in the adoption of latest innovations in software engineering, such as automated program repair (APR) [3]. Test suites are unfortunately often too scarce in software development projects. Generally, they are provided for regression testing, while new bugs are discovered by users who then describe them informally in bug reports. In recent literature, a new trend of research in APR has attempted to leverage bug reports in generate-and-validate pipelines for program repair. Even in such cases, when an APR tool generates a patch candidate, if test cases are unavailable, developers must manually validate the patch, leading to a threat to validity.},
   author = {Laura Plein and Wendkûuni C Ouédraogo and Jacques Klein and Tegawendé F Bissyandé},
   city = {New York, NY, USA},
   doi = {10.1145/3639478.3643119},
   isbn = {9798400705021},
   booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
   pages = {360-361},
   publisher = {Association for Computing Machinery},
   title = {Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models},
   url = {https://doi.org/10.1145/3639478.3643119},
   year = {2024}
}
@inproceedings{Fakhoury2024,
   abstract = {We introduce a novel workflow, TiCoder, designed to enhance the trust and accuracy of LLM-based code generation through interactive and guided intent formalization. TiCoder partially formalizes ambiguous intent in natural language prompts by generating a set of tests to distinguish common divergent behaviours in generated code suggestions. We evaluate the code generation accuracy improvements provided by TiCoder at scale across four competitive LLMs, and evaluate the cost-benefit trade off of evaluating tests surfaced by TiCoder through a user study with 15 participants.},
   author = {Sarah Fakhoury and Aaditya Naik and Georgios Sakkas and Saikat Chakraborty and Madan Musuvathi and Shuvendu Lahiri},
   city = {New York, NY, USA},
   doi = {10.1145/3639478.3643525},
   isbn = {9798400705021},
   booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
   pages = {390-391},
   publisher = {Association for Computing Machinery},
   title = {Exploring the Effectiveness of LLM based Test-driven Interactive Code Generation: User Study and Empirical Evaluation},
   url = {https://doi.org/10.1145/3639478.3643525},
   year = {2024}
}
@inproceedings{Morales2024,
   abstract = {Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose LangBiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach.},
   author = {Sergio Morales and Robert Clarisó and Jordi Cabot},
   city = {New York, NY, USA},
   doi = {10.1145/3640310.3674093},
   isbn = {9798400705045},
   booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
   keywords = {Bias,Domain-Specific Language,Ethics,Large Language Models,Model-Driven Engineering,Red Teaming,Testing},
   pages = {203-213},
   publisher = {Association for Computing Machinery},
   title = {A DSL for Testing LLMs for Fairness and Bias},
   url = {https://doi.org/10.1145/3640310.3674093},
   year = {2024}
}
@article{Zhang2024,
   abstract = {Machine translation systems have been widely adopted in our daily life, making life easier and more convenient. Unfortunately, erroneous translations may result in severe consequences, such as financial losses. This requires to improve the accuracy and the reliability of machine translation systems. However, it is challenging to test machine translation systems because of the complexity and intractability of the underlying neural models. To tackle these challenges, we propose a novel metamorphic testing approach by syntactic tree pruning (STP) to validate machine translation systems. Our key insight is that a pruned sentence should have similar crucial semantics compared with the original sentence. Specifically, STP (1) proposes a core semantics-preserving pruning strategy by basic sentence structures and dependency relations on the level of syntactic tree representation, (2) generates source sentence pairs based on the metamorphic relation, and (3) reports suspicious issues whose translations break the consistency property by a bag-of-words model. We further evaluate STP on two state-of-the-art machine translation systems (i.e., Google Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs. The results show that STP accurately finds 5,073 unique erroneous translations in Google Translate and 5,100 unique erroneous translations in Bing Microsoft Translator (400\% more than state-of-the-art techniques), with 64.5\% and 65.4\% precision, respectively. The reported erroneous translations vary in types and more than 90\% of them are not found by state-of-the-art techniques. There are 9,393 erroneous translations unique to STP, which is 711.9\% more than state-of-the-art techniques. Moreover, STP is quite effective in detecting translation errors for the original sentences with a recall reaching 74.0\%, improving state-of-the-art techniques by 55.1\% on average.},
   author = {Quanjun Zhang and Juan Zhai and Chunrong Fang and Jiawei Liu and Weisong Sun and Haichuan Hu and Qingyu Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3640329},
   issn = {1049-331X},
   issue = {5},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Software testing,machine translation,metamorphic testing},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {Machine Translation Testing via Syntactic Tree Pruning},
   volume = {33},
   url = {https://doi.org/10.1145/3640329},
   year = {2024}
}
@article{Liu2024-3,
   abstract = {The relationship between states (status of a system) and modes (capabilities of a system) used to describe system requirements is often poorly defined. The unclear relationship could make systems of interest out of control because of the out of boundaries of the systems caused by the newly added modes. Formally modeling and verifying requirements can clarify the relationship, making the system safer. To this end, an innovative approach to analyzing requirements is proposed. The MoSt language (a Domain Specific Language implemented on the Xtext framework) is firstly designed for requirements modeling and a model validator is realized to check requirements statically. A code generator is then provided to realize the automatic model transformation from the MoSt model to a NuSMV model, laying the foundation for the dynamic checks of requirements through symbolic model checking. Next, a NuSMV runner is designed to connect the NuSMV with the validator to automate the whole dynamic checks. The grammar, the model validator, the code generator, and the NuSMV runner are finally integrated into a publicly available Eclipse-based tool. Two case studies have been employed to illustrate the feasibility of our approach. For each case study, we injected 14 errors. The results show that the static and dynamic checks can successfully detect all the errors.},
   author = {Yinling Liu and Jean-Michel Bruel},
   city = {New York, NY, USA},
   doi = {10.1145/3640822},
   issn = {0934-5043},
   issue = {2},
   journal = {Form. Asp. Comput.},
   keywords = {States and modes,domain specific language,model checking,requirements modeling and verification},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {Modeling and Verification of Natural Language Requirements based on States and Modes},
   volume = {36},
   url = {https://doi.org/10.1145/3640822},
   year = {2024}
}
@inproceedings{Zhang2024,
   abstract = {Fuzzy testing is one of the most popular vulnerability mining techniques recently, it plays a huge role in exploiting software security vulnerabilities and improving software security. Fuzzy testing mainly performs specific variations on the collected seeds to obtain a large number of test cases that can be used to execute the target program and trigger potential crashes in the program. However, traditional fuzzy testing generally suffers from a low level of test automation and fewer types of vulnerabilities detected. Aiming at the above problems, the application of machine learning techniques to fuzzy testing has become a hot research topic in academia. However, some recent studies still have problems, such as low edge coverage and poor generalization ability. Therefore, this paper proposes a strongly directed fuzz testing method based on attention mechanism and we name the fuzzer as AMNeuzz. AMNeuzz uses neural networks combined with attention mechanisms to construct an automatic sample generation model, which is trained so that the model learns the intrinsic formatting features of the samples, thus being able to automatically generate test samples that conform to certain syntactic specifications to quickly examine program paths that may have vulnerabilities, and this improves efficiency. In addition, the performance of the fuzzers is improved by improving Neuzz's gradient strategy. The final experimental results show that the AMNeuzz method proposed in this paper can achieve higher edge coverage than NEUZZ under the same time overhead.},
   author = {Lei Zhang and Binbin Wang and Chang Liu and Mi Wen and Yan Zhang and Liangliang Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3641181.3641182},
   isbn = {9798400709319},
   booktitle = {Proceedings of the 2024 10th International Conference on Computing and Data Engineering},
   keywords = {attention mechanism,fuzzing,machine learning,neural network},
   pages = {105-110},
   publisher = {Association for Computing Machinery},
   title = {AMNeuzz: A Strongly Directed Fuzz Testing Method Based on Attention Mechanism},
   url = {https://doi.org/10.1145/3641181.3641182},
   year = {2024}
}
@inproceedings{Olsthoorn2024,
   abstract = {Over the last decades, various tools (e.g., AUSTIN and EvoSuite) have been developed to automate the process of unit-level test case generation. Most of these tools are designed for statically-typed languages, such as C and Java. However, as is shown in recent Stack Overflow developer surveys, the popularity of dynamically-typed languages, such as JavaScript and Python, has been increasing and is dominating the charts. Only recently, tools for automated test case generation of dynamically-typed languages have started to emerge (e.g., Pynguin for Python). However, to the best of our knowledge, there is no tool that focuses on automated test case generation for server-side JavaScript. To this aim, we introduce SynTest-JavaScript, a user-friendly tool for automated unit-level test case generation for (server-side) JavaScript. To showcase the effectiveness of SynTest-JavaScript, we empirically evaluate it on five large open-source JavaScript projects and one artificial one.},
   author = {Mitchell Olsthoorn and Dimitri Stallenberg and Annibale Panichella},
   city = {New York, NY, USA},
   doi = {10.1145/3643659.3643928},
   isbn = {9798400705625},
   booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
   keywords = {fuzzing,javascript,search-based software testing,software testing,syntest,test case generation},
   pages = {21-24},
   publisher = {Association for Computing Machinery},
   title = {Syntest-JavaScript: Automated Unit-Level Test Case Generation for JavaScript},
   url = {https://doi.org/10.1145/3643659.3643928},
   year = {2024}
}
@article{Hu2024,
   abstract = {This article presents a comprehensive survey on test optimization in deep neural network&nbsp;(DNN) testing. Here, test optimization refers to testing with low data labeling effort. We analyzed 90 papers, including 43 from the software engineering (SE) community, 32 from the machine learning (ML) community, and 15 from other communities. Our study: (i) unifies the problems as well as terminologies associated with low-labeling cost testing, (ii) compares the distinct focal points of SE and ML communities, and (iii) reveals the pitfalls in existing literature. Furthermore, we highlight the research opportunities in this domain.},
   author = {Qiang Hu and Yuejun Guo and Xiaofei Xie and Maxime Cordy and Lei Ma and Mike Papadakis and Yves Le Traon},
   city = {New York, NY, USA},
   doi = {10.1145/3643678},
   issn = {1049-331X},
   issue = {4},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {DNN testing,Test optimization,low-labeling cost},
   month = {4},
   publisher = {Association for Computing Machinery},
   title = {Test Optimization in DNN Testing: A Survey},
   volume = {33},
   url = {https://doi.org/10.1145/3643678},
   year = {2024}
}
@article{Jiang2024-3,
   abstract = {Large language models have gained significant popularity and are often provided as a service (i.e., LLMaaS).  Companies like OpenAI and Google provide online APIs of LLMs to allow downstream users to create innovative applications.  Despite its popularity, LLM safety and quality assurance is a well-recognized concern in the real world, requiring extra efforts for testing these LLMs.  Unfortunately, while end-to-end services like ChatGPT have garnered rising attention in terms of testing, the LLMaaS embeddings have comparatively received less scrutiny.  We state the importance of testing and uncovering problematic individual embeddings without considering downstream applications.  The abstraction and non-interpretability of embedded vectors, combined with the black-box inaccessibility of LLMaaS, make testing a challenging puzzle.  This paper proposes COSTELLO, a black-box approach to reveal potential defects in abstract embedding vectors from LLMaaS by contrastive testing.  Our intuition is that high-quality LLMs can adequately capture the semantic relationships of the input texts and properly represent their relationships in the high-dimensional space.  For the given interface of LLMaaS and seed inputs, COSTELLO can automatically generate test suites and output words with potential problematic embeddings.  The idea is to synthesize contrastive samples with guidance, including positive and negative samples, by mutating seed inputs.  Our synthesis guide will leverage task-specific properties to control the mutation procedure and generate samples with known partial relationships in the high-dimensional space.  Thus, we can compare the expected relationship (oracle) and embedding distance (output of LLMs) to locate potential buggy cases.  We evaluate COSTELLO on 42 open-source (encoder-based) language models and two real-world commercial LLMaaS.  Experimental results show that COSTELLO can effectively detect semantic violations, where more than 62\% of violations on average result in erroneous behaviors (e.g., unfairness) of downstream applications.},
   author = {Weipeng Jiang and Juan Zhai and Shiqing Ma and Xiaoyu Zhang and Chao Shen},
   city = {New York, NY, USA},
   doi = {10.1145/3643767},
   issue = {FSE},
   journal = {Proc. ACM Softw. Eng.},
   keywords = {Contrastive Testing,Embeddings,LLMaaS},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings},
   volume = {1},
   url = {https://doi.org/10.1145/3643767},
   year = {2024}
}
@article{Ryan2024,
   abstract = {Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage.

In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26\% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.},
   author = {Gabriel Ryan and Siddhartha Jain and Mingyue Shang and Shiqi Wang and Xiaofei Ma and Murali Krishna Ramanathan and Baishakhi Ray},
   city = {New York, NY, USA},
   doi = {10.1145/3643769},
   issue = {FSE},
   journal = {Proc. ACM Softw. Eng.},
   keywords = {Large Language Models,Test Generation},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM},
   volume = {1},
   url = {https://doi.org/10.1145/3643769},
   year = {2024}
}
@inproceedings{Piya2024,
   abstract = {In today's society, we are becoming increasingly dependent on software systems. However, we also constantly witness the negative impacts of buggy software. Program synthesis aims to improve software correctness by automatically generating the program given an outline of the expected behavior. For decades, program synthesis has been an active research field, with recent approaches looking to incorporate Large Language Model. This paper explores the concept of LLM4TDD, where we guide Large Language Models to generate code iteratively using a test-driven development methodology. We conduct an empirical evaluation using ChatGPT and coding problems from LeetCode to investigate the impact of different test, prompt and problem attributes on the efficacy of LLM4TDD.},
   author = {Sanyogita Piya and Allison Sullivan},
   city = {New York, NY, USA},
   doi = {10.1145/3643795.3648382},
   isbn = {9798400705793},
   booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
   pages = {14-21},
   publisher = {Association for Computing Machinery},
   title = {LLM4TDD: Best Practices for Test Driven Development Using Large Language Models},
   url = {https://doi.org/10.1145/3643795.3648382},
   year = {2024}
}
@inproceedings{Bhatia2024,
   abstract = {Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT's performance, achieving a much higher coverage.*These authors contributed equally.},
   author = {Shreya Bhatia and Tarushi Gandhi and Dhruv Kumar and Pankaj Jalote},
   city = {New York, NY, USA},
   doi = {10.1145/3643795.3648396},
   isbn = {9798400705793},
   booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
   keywords = {ChatGPT,generative AI,large language models,unit test generation},
   pages = {54-61},
   publisher = {Association for Computing Machinery},
   title = {Unit Test Generation using Generative AI: A Comparative Performance Analysis of Autogeneration Tools},
   url = {https://doi.org/10.1145/3643795.3648396},
   year = {2024}
}
@inproceedings{Khandker2024,
   abstract = {Despite the widespread deployment of 5G technologies, there exists a critical gap in security testing for 5G Standalone (SA) devices. Existing methods, largely manual and labor-intensive, are ill-equipped to fully uncover the state of security in the implementations of 5G SA protocols and standards on devices, severely limiting the ability to conduct comprehensive evaluations. To address this issue, in this work, we introduce a novel, open-source framework that automates the security testing process for 5G SA devices. By leveraging enhanced functionalities of 5G SA core and Radio Access Network (RAN) software, our framework offers a streamlined approach to generating, executing, and evaluating test cases, specifically focusing on the Non-Access Stratum layer. Our application of this framework across multiple 5G SA devices provides in-depth security insights, significantly improving testing efficiency and breadth.},
   author = {Syed Khandker and Michele Guerra and Evangelos Bitsikas and Roger Piqueras Jover and Aanjhan Ranganathan and Christina Pöpper},
   city = {New York, NY, USA},
   doi = {10.1145/3643833.3656141},
   isbn = {9798400705823},
   booktitle = {Proceedings of the 17th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
   keywords = {5g,automated testing,open5gs,security,srsran},
   pages = {89-100},
   publisher = {Association for Computing Machinery},
   title = {ASTRA-5G: Automated Over-the-Air Security Testing and Research Architecture for 5G SA Devices},
   url = {https://doi.org/10.1145/3643833.3656141},
   year = {2024}
}
@inproceedings{Zhu2024,
   abstract = {Code completion is an important feature in Integrated Development Environments (IDEs). These years, researchers have been making efforts for intelligent code completion. However, existing work on intelligent code completion either only considered production code, or did not distinguish between production code and test code. It is unclear how effective existing completion models are for test code completion, nor whether we can further improve it. In this work, we focus on the completion of test code. We first find through experiments that completion models for production code are suboptimal for test code completion. Then we analyze the specific characteristics of test code, and observe that test code has inter- and intra-project similarities, and a strong relationship with its focal class and other production classes depending on the focal class (i.e., focal-related code). By incorporating test code from other projects to fine-tune existing models, we leverage the inter-project similarity of test code to improve the completion of tokens specific to test code. By introducing a local component and constructing existing test code as well as the focal-related code in the project as references, we enhance existing code completion models with the intra-project similarity and the focal-related code of test code. Experiments show that each characteristic of test code we exploit can bring substantial improvement to test code completion and our integrated framework outperforms other baseline frameworks. Compared to the base completion model, on token-level completion, our optimal model for test code completion relatively improves all-token and identifier completion accuracy by 7.68\% and 19.96\%, respectively; on line-level completion, it relatively improves edit-distance similarity and exact-match metrics by 8.89\% and 22.82\%, respectively. Moreover, we perform error analysis and point out potential directions for future work.},
   author = {Tingwei Zhu and Zhongxin Liu and Tongtong Xu and Ze Tang and Tian Zhang and Minxue Pan and Xin Xia},
   city = {New York, NY, USA},
   doi = {10.1145/3643916.3644421},
   isbn = {9798400705861},
   booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
   keywords = {code completion,retrieval augmentation,test code},
   pages = {137-148},
   publisher = {Association for Computing Machinery},
   title = {Exploring and Improving Code Completion for Test Code},
   url = {https://doi.org/10.1145/3643916.3644421},
   year = {2024}
}
@inproceedings{ElHaji2024,
   abstract = {Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot's test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations.Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28\% of the tests generated by Copilot are passing tests; 54.72\% of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45\% of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.},
   author = {Khalid El Haji and Carolin Brandt and Andy Zaidman},
   city = {New York, NY, USA},
   doi = {10.1145/3644032.3644443},
   isbn = {9798400705885},
   booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
   pages = {45-55},
   publisher = {Association for Computing Machinery},
   title = {Using GitHub Copilot for Test Generation in Python: An Empirical Study},
   url = {https://doi.org/10.1145/3644032.3644443},
   year = {2024}
}
@inproceedings{Canizares2024,
   abstract = {Conversational agents - or chatbots - are increasingly used as the user interface to many software services. While open-domain chatbots like ChatGPT excel in their ability to chat about any topic, task-oriented conversational agents are designed to perform goal-oriented tasks (e.g., booking or shopping) guided by a dialogue-based user interaction, which is explicitly designed. Like any kind of software system, task-oriented conversational agents need to be properly tested to ensure their quality. For this purpose, some tools permit defining and executing conversation test cases. However, there are currently no established means to assess the coverage of the design of a task-oriented agent by a test suite, or mechanisms to automate quality test case generation ensuring the agent coverage.To attack this problem, we propose test coverage criteria for task-oriented conversational agents, and define coverage-based strategies to synthesise test scenarios, some oriented to test case reduction. We provide an implementation of the criteria and the strategies that is independent of the agent development platform. Finally, we report on their evaluation on open-source Dialogflow and Rasa agents, and a comparison against a state-of-the-art testing tool. The experiment shows benefits in terms of test generation correctness, increased coverage and reduced testing time.},
   author = {Pablo C Canizares and Daniel Ávila and Sara Perez-Soler and Esther Guerra and Juan De Lara},
   city = {New York, NY, USA},
   doi = {10.1145/3644032.3644456},
   isbn = {9798400705885},
   booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
   keywords = {task-oriented conversational agents,test suite generation,testing},
   pages = {23-33},
   publisher = {Association for Computing Machinery},
   title = {Coverage-based Strategies for the Automated Synthesis of Test Scenarios for Conversational Agents},
   url = {https://doi.org/10.1145/3644032.3644456},
   year = {2024}
}
@inproceedings{Li2024-7,
   abstract = {Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.},
   author = {Ziyu Li and Donghwan Shin},
   city = {New York, NY, USA},
   doi = {10.1145/3644815.3644946},
   isbn = {9798400705915},
   booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
   keywords = {large language models,mutation analysis,software engineering},
   pages = {150-159},
   publisher = {Association for Computing Machinery},
   title = {Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs},
   url = {https://doi.org/10.1145/3644815.3644946},
   year = {2024}
}
@inproceedings{Rasool2024,
   abstract = {Large language models (LLMs) enable state-of-the-art semantic capabilities to be added to software systems such as semantic search of unstructured documents and text generation. However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience. To address this problem, semantic caches are used to check for answers to similar queries (that may have been phrased differently) without hitting the LLM service. Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system. Adopting semantic cache techniques usually requires testing the effectiveness of a semantic cache (accurate cache hits and misses) which requires a labelled test set of similar queries and responses which is often unavailable. In this paper, we present VaryGen, an approach for using LLMs for test input generation that produces similar questions from unstructured text documents. Our novel approach uses the reasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise subtle variations to queries, and 3) evaluate the synthesised test dataset. We evaluated our approach in the domain of a student question and answer system by qualitatively analysing 100 generated queries and result pairs, and conducting an empirical case study with an open source semantic cache. Our results show that query pairs satisfy human expectations of similarity and our generated data demonstrates failure cases of a semantic cache. Additionally, we also evaluate our approach on Qasper dataset. This work is an important first step into test input generation for semantic applications and presents considerations for practitioners when calibrating a semantic cache.},
   author = {Zafaryab Rasool and Scott Barnett and David Willie and Stefanus Kurniawan and Sherwin Balugo and Srikanth Thudumu and Mohamed Abdelrazek},
   city = {New York, NY, USA},
   doi = {10.1145/3644815.3644948},
   isbn = {9798400705915},
   booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
   keywords = {large language model,query evaluation,question answering,semantic cache,test input generation},
   pages = {160-165},
   publisher = {Association for Computing Machinery},
   title = {LLMs for Test Input Generation for Semantic Applications},
   url = {https://doi.org/10.1145/3644815.3644948},
   year = {2024}
}
@inproceedings{Ma2024,
   abstract = {Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regression testing for evolving LLM APIs. We argue that regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.},
   author = {Wanqin Ma and Chenyang Yang and Christian Kästner},
   city = {New York, NY, USA},
   doi = {10.1145/3644815.3644950},
   isbn = {9798400705915},
   booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
   keywords = {large language models (LLM),regression testing},
   pages = {166-171},
   publisher = {Association for Computing Machinery},
   title = {(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs},
   url = {https://doi.org/10.1145/3644815.3644950},
   year = {2024}
}
@inproceedings{Wang2024-4,
   abstract = {Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation.This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM-augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51\% on our tests compared to the original datasets, emphasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM-augmented applications.},
   author = {Guanyu Wang and Yuekang Li and Yi Liu and Gelei Deng and Tianlin Li and Guosheng Xu and Yang Liu and Haoyu Wang and Kailong Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3650105.3652297},
   isbn = {9798400706097},
   booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
   keywords = {augmented generation,metamorphic testing,vector matching},
   pages = {12-23},
   publisher = {Association for Computing Machinery},
   title = {MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation},
   url = {https://doi.org/10.1145/3650105.3652297},
   year = {2024}
}
@article{Coppola2024,
   abstract = {In this paper we report the outcomes of the 1st and 2nd edition of the International Workshop on Gamification in Software Development, Verification, and Validation (Gamify 2022 and Gamify 2023) which were held as part of the 30th and 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022, in Singapore, November 17, 2022 and ESEC/FSE 2023, online workshop, December 4, 2023).},
   author = {Riccardo Coppola and Luca Ardito and Maurizio Leotta},
   city = {New York, NY, USA},
   doi = {10.1145/3650142.3650151},
   issn = {0163-5948},
   issue = {2},
   journal = {SIGSOFT Softw. Eng. Notes},
   month = {4},
   pages = {27-30},
   publisher = {Association for Computing Machinery},
   title = {Gamify: Gamification in Software Development, Verification,and Validation},
   volume = {49},
   url = {https://doi.org/10.1145/3650142.3650151},
   year = {2024}
}
@inproceedings{Hu2024,
   abstract = {High-level programming models like Q# significantly simplify the complexity of programming for quantum computing. These models are supported by a set of foundation libraries for code development. However, errors can occur in the library implementation, and one common root cause is the lack of or incomplete checks on properties like values, length, and quantum states of inputs passed to user-facing subroutines. This paper presents Upbeat, a fuzzing tool to generate random test cases for bugs related to input checking in Q# libraries. Upbeat develops an automated process to extract constraints from the API documentation and the developer implemented input-checking statements. It leverages open-source Q# code samples to synthesize test programs. It frames the test case generation as a constraint satisfaction problem for classical computing and a quantum state model for quantum computing to produce carefully generated subroutine inputs to test if the input-checking mechanism is appropriately implemented. Under 100 hours of automated test runs, Upbeat has successfully identified 16 bugs in API implementations and 4 documentation errors. Of these, 14 have been confirmed, and 12 have been fixed by the library developers.},
   author = {Tianmin Hu and Guixin Ye and Zhanyong Tang and Shin Hwei Tan and Huanting Wang and Meng Li and Zheng Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3652120},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Fuzzing,Quantum computing,Software testing},
   pages = {186-198},
   publisher = {Association for Computing Machinery},
   title = {UPBEAT: Test Input Checks of Q# Quantum Libraries},
   url = {https://doi.org/10.1145/3650212.3652120},
   year = {2024}
}

@inproceedings{Yang2024-1,
   abstract = {Large Language Models (LLMs) have showcased their remarkable capabilities in diverse domains, encompassing natural language understanding, translation, and even code generation. The potential for LLMs to generate harmful content is a significant concern. This risk necessitates rigorous testing and comprehensive evaluation of LLMs to ensure safe and responsible use. However, extensive testing of LLMs requires substantial computational resources, making it an expensive endeavor. Therefore, exploring cost-saving strategies during the testing phase is crucial to balance the need for thorough evaluation with the constraints of resource availability. To address this, our approach begins by transferring the moderation knowledge from an LLM to a small model. Subsequently, we deploy two distinct strategies for generating malicious queries: one based on a syntax tree approach, and the other leveraging an LLM-based method. Finally, our approach incorporates a sequential filter-test process designed to identify test cases that are prone to eliciting toxic responses. By doing so, we significantly curtail unnecessary or unproductive interactions with LLMs, thereby streamlining the testing process. Our research evaluated the efficacy of DistillSeq across four LLMs: GPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the observed attack success rates on these LLMs stood at 31.5\% for GPT-3.5, 21.4\% for GPT-4.0, 28.3\% for Vicuna-13B, and 30.9\% for Llama-13B. However, upon the application of DistillSeq, these success rates notably increased to 58.5\%, 50.7\%, 52.5\%, and 54.4\%, respectively. This translated to an average escalation in attack success rate by a factor of 93.0\% when compared to scenarios without the use of DistillSeq. Such findings highlight the significant enhancement DistillSeq offers in terms of reducing the time and resource investment required for effectively testing LLMs.},
   author = {Mingke Yang and Yuqi Chen and Yi Liu and Ling Shi},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680304},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Automated Testing,Knowledge Distillation,Large Language Models},
   pages = {578-589},
   publisher = {Association for Computing Machinery},
   title = {DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation},
   url = {https://doi.org/10.1145/3650212.3680304},
   year = {2024}
}
@inproceedings{He2024-1,
   abstract = {The remarkable capability of large language models (LLMs) in































































generating high-quality code has drawn increasing attention































































in the software testing community.































































However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate, complete tests































































since they were trained on code snippets collected without































































differentiating between code for testing and for other purposes.































































In this paper, we present a large-scale dataset, UniTSyn, which can enhance LLMs for Unit Test Synthesis.































































Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified.































































By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics, which tend to be fragile and difficult to scale.































































Containing 2.7 million focal-test pairs across five mainstream programming languages, it can enhance the test generation ability of LLMs.































































Our experiments demonstrate that,































































by building an autoregressive LLM based on UniTSyn,































































we can achieve significant benefits in learning and understanding unit test representations,































































resulting in improved generation accuracy and code coverage































































across all the evaluated programming languages.},
   author = {Yifeng He and Jiabo Huang and Yuyang Rong and Yiwen Guo and Ethan Wang and Hao Chen},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680342},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Large language models,dataset,software testing,test case generation},
   pages = {1061-1072},
   publisher = {Association for Computing Machinery},
   title = {UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing},
   url = {https://doi.org/10.1145/3650212.3680342},
   year = {2024}
}
@inproceedings{Kim2024-1,
   abstract = {Assembler is a critical component of the compiler toolchain, which has been less tested than the other components. Unfortunately, current grammar-based fuzzing techniques suffer from several challenges when testing assemblers. First, each different assembler accepts different grammar rules and syntaxes, and there are no existing assembly grammar specifications. Second, not every assembler is open-source, which makes it difficult to extract grammar rules from the source code. While existing black-box grammar inference approaches are applicable to such closed-source assemblers, they suffer from the scalability issue, which renders them impractical for testing assemblers. To address these challenges, we propose a novel way to test assemblers by automatically inferring their grammar rules with only a few queries to the target assemblers by leveraging their error messages. The key insight is that assembly error messages often deliver useful information to infer the underlying grammar rules. We have implemented our technique in a tool named AsFuzzer, and evaluated it on 4 real-world assemblers including Clang-integrated assembler (Clang), GNU assembler (GAS), Intel’s assembler (ICC), and Microsoft macro assembler (MASM). With AsFuzzer, we have successfully found 497 buggy instruction opcodes for six popular architectures, and reported them to the developers.},
   author = {Hyungseok Kim and Soomin Kim and Jungwoo Lee and Sang Kil Cha},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680345},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {assembler testing,compiler testing,grammar inference},
   pages = {1099-1111},
   publisher = {Association for Computing Machinery},
   title = {AsFuzzer: Differential Testing of Assemblers with Error-Driven Grammar Inference},
   url = {https://doi.org/10.1145/3650212.3680345},
   year = {2024}
}
@inproceedings{Shin2024,
   abstract = {Recently, deep learning-based test case generation approaches have been proposed to automate the generation of unit test cases. In this study, we leverage Transformer-based code models to generate unit tests with the help of Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a relatively small language model trained on source code data, and fine-tune it on the test generation task. Then, we apply domain adaptation to each target project data to learn project-specific knowledge (project-level DA). We use the Methods2test dataset to fine-tune CodeT5 for the test generation task and the Defects4j dataset for project-level domain adaptation and evaluation. We compare our approach with (a) CodeT5 fine-tuned on the test generation without DA, (b) the A3Test tool, and (c) GPT-4 on five projects from the Defects4j dataset. The results show that tests generated using DA can increase the line coverage by 18.62\%, 19.88\%, and 18.02\% and mutation score by 16.45\%, 16.01\%, and 12.99\% compared to the above (a), (b), and (c) baselines, respectively. The overall results show consistent improvements in metrics such as parse rate, compile rate, BLEU, and CodeBLEU. In addition, we show that our approach can be seen as a complementary solution alongside existing search-based test generation tools such as EvoSuite, to increase the overall coverage and mutation scores with an average of 34.42\% and 6.8\%, for line coverage and mutation score, respectively.},
   author = {Jiho Shin and Sepehr Hashtroudi and Hadi Hemmati and Song Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680354},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Code Model,Domain Adaption,GPT,LLM,Test generation,Transformers},
   pages = {1211-1222},
   publisher = {Association for Computing Machinery},
   title = {Domain Adaptation for Code Model-Based Unit Test Case Generation},
   url = {https://doi.org/10.1145/3650212.3680354},
   year = {2024}
}
@inproceedings{Go2024,
   abstract = {Deep learning library is important in AI systems. Recently, many works have been proposed to ensure its reliability.         They often model inputs of tensor operations as constraints to guide the generation of test cases.        However, these constraints may narrow the search space, resulting in incomplete testing.        This paper introduces a complementary set-guided refinement that can enhance the completeness of constraints.        The basic idea is to see if the complementary set of constraints yields valid test cases. If so, the original constraint is incomplete and needs refinement.         Based on this idea, we design an automatic constraint refinement tool, DeepConstr, which adopts a genetic algorithm to refine constraints for better completeness.        We evaluated it on two DL libraries, PyTorch and TensorFlow.        DeepConstr discovered 84 unknown bugs, out of which 72 were confirmed, with 51 fixed.         Compared to state-of-the-art fuzzers, DeepConstr increased coverage for 43.44\% of operators supported by NNSmith, and 59.16\% of operators supported by NeuRI.},
   author = {Gwihwan Go and Chijin Zhou and Quan Zhang and Xiazijian Zou and Heyuan Shi and Yu Jiang},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680364},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {DL library,Fuzzing,Large Language Model},
   pages = {1338-1350},
   publisher = {Association for Computing Machinery},
   title = {Towards More Complete Constraints for Deep Learning Library Testing via Complementary Set Guided Refinement},
   url = {https://doi.org/10.1145/3650212.3680364},
   year = {2024}
}
@inproceedings{Chen2024-5,
   abstract = {Test flakiness, a non-deterministic behavior of builds irrelevant to code changes, is a major and continuing impediment to deliver- ing reliable software. The very few techniques for the automated repair of test flakiness are specifically crafted to repair either Order- Dependent (OD) or Implementation-Dependent (ID) flakiness. They are also all symbolic approaches, i.e., they leverage program analy- sis to detect and repair known test flakiness patterns and root causes, failing to generalize. To bridge the gap, we propose FlakyDoctor, a neuro-symbolic technique that combines the power of LLMs— generalizability—and program analysis—soundness—to fix different types of test flakiness.

Our extensive evaluation using 873 confirmed flaky tests (332 OD and 541 ID) from 243 real-world projects demonstrates the ability of FlakyDoctor in repairing flakiness, achieving 57\% (OD) and 59\% (ID) success rate. Comparing to three alternative flakiness repair approaches, FlakyDoctor can repair 8\% more ID tests than DexFix, 12\% more OD flaky tests than ODRepair, and 17\% more OD flaky tests than iFixFlakies. Regardless of underlying LLM, the non-LLM components of FlakyDoctor contribute to 12–31 \% of the overall performance, i.e., while part of the FlakyDoctor power is from using LLMs, they are not good enough to repair flaky tests in real-world projects alone. What makes the proposed technique superior to related research on test flakiness mitigation specifically and program repair, in general, is repairing 79 previously unfixed flaky tests in real-world projects. We opened pull requests for all cases with corresponding patches; 19 of them were accepted and merged at the time of submission.},
   author = {Yang Chen and Reyhaneh Jabbarvand},
   city = {New York, NY, USA},
   doi = {10.1145/3650212.3680369},
   isbn = {9798400706127},
   booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Large Language Models,Program Repair,Test Flakiness},
   pages = {1402-1414},
   publisher = {Association for Computing Machinery},
   title = {Neurosymbolic Repair of Test Flakiness},
   url = {https://doi.org/10.1145/3650212.3680369},
   year = {2024}
}
@inproceedings{Hu2024,
   abstract = {Profiting by the rapid development of computer science and technology, deep neural networks have been widely used in security-related fields such as face recognition, automatic driving, medical diagnosis and decision-making reasoning, and there is an urgent need for testers to conduct comprehensive and in-depth testing of these software to ensure their quality and security. However, intelligent software based on neural networks is fundamentally different from traditional software. In recent years, more and more researchers have shifted their attention from traditional software testing to intelligent software testing, and a series of evaluation criteria, test frameworks, and test case generation methods, etc. have been proposed for deep neural network models. This paper summarises and concludes the existing research from the perspectives of testing techniques based on test adequacy theory, testing techniques based on traditional testing theory and testing techniques based on adversarial samples. Finally, it summarises and looks forward to deep neural network testing and points out the problems in deep neural network testing, in order to provide some thoughts for researchers in related fields.},
   author = {Qianchao Hu and Feng Wang and Binglin Liu and Haitian Liu},
   city = {New York, NY, USA},
   doi = {10.1145/3650215.3650237},
   isbn = {9798400709449},
   booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application},
   pages = {113-119},
   publisher = {Association for Computing Machinery},
   title = {Research on Deep Neural Network Testing Techniques},
   url = {https://doi.org/10.1145/3650215.3650237},
   year = {2024}
}
@article{Chen2024-6,
   abstract = {Unfair behaviors of Machine Learning (ML) software have garnered increasing attention and concern among software engineers. To tackle this issue, extensive research has been dedicated to conducting fairness testing of ML software, and this article offers a comprehensive survey of existing studies in this field. We collect 100 papers and organize them based on the testing workflow (i.e., how to test) and testing components (i.e., what to test). Furthermore, we analyze the research focus, trends, and promising directions in the realm of fairness testing. We also identify widely adopted datasets and open-source tools for fairness testing.},
   author = {Zhenpeng Chen and Jie M Zhang and Max Hort and Mark Harman and Federica Sarro},
   city = {New York, NY, USA},
   doi = {10.1145/3652155},
   issn = {1049-331X},
   issue = {5},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Machine learning,analysis,fairness testing,survey,trends},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {Fairness Testing: A Comprehensive Survey and Analysis of Trends},
   volume = {33},
   url = {https://doi.org/10.1145/3652155},
   year = {2024}
}
@inproceedings{Zavada2024,
   abstract = {In the field of model-based systems engineering, there is an increasing demand for the application of formal methods. However, this requires expertise in formal methods, which cannot be expected from systems engineers. While several attempts have been made to bridge this gap, there are still open questions. (1) With the trend shifting towards ontological languages, systems are modeled as classes of 4D occurrences, rather than a 3D system evolving with time, which hinders the application of state-of-the-art model checking algorithms. (2) Ontological reasoning cannot handle the state space explosion problem, and can even make it harder for verifiers to operate efficiently. (3) When operationalizing ontological languages, we need to validate the conformance of the two semantics, even in the presence of optimizations. (4) On top of all, these challenges must be solved for every new engineering language, version, or variant. In this paper, we propose a new approach to address the aforementioned challenges. To validate its feasibility, we present a prototype tool and evaluate it on a SysML model.},
   author = {Ármin Zavada and Kristóf Marussy and Vince Molnár},
   city = {New York, NY, USA},
   doi = {10.1145/3652620.3686251},
   isbn = {9798400706226},
   booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
   keywords = {declarative interpretation,formal verification,kernel modeling language,metaprogramming,model-based systems engineering,operational libraries,semantic libraries},
   pages = {311-317},
   publisher = {Association for Computing Machinery},
   title = {From Transpilers to Semantic Libraries: Formal Verification With Pluggable Semantics},
   url = {https://doi.org/10.1145/3652620.3686251},
   year = {2024}
}
@article{Guglielmi2024,
   abstract = {Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers for building custom apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows the user to use natural language commands to perform actions. Testing such apps is not trivial: The same command can be expressed in different semantically equivalent ways. In this article, we introduce VUI-UPSET, an approach that adapts chatbot-testing approaches to VUI-testing. We conducted an empirical study to understand how VUI-UPSET compares to two state-of-the-art approaches (i.e., a chatbot testing technique and ChatGPT) in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. To this aim, we analyzed 14,898 generated paraphrases for 40 Alexa Skills. Our results show that VUI-UPSET generates more bug-revealing paraphrases than the two baselines with, however, ChatGPT being the approach generating the highest percentage of correct paraphrases. We also tried to use the generated paraphrases to improve the skills. We tried to include in the voice interaction models of the skills (i) only the bug-revealing paraphrases, (ii) all the valid paraphrases. We observed that including only bug-revealing paraphrases is sometimes not sufficient to make all the tests pass.},
   author = {Emanuela Guglielmi and Giovanni Rosa and Simone Scalabrino and Gabriele Bavota and Rocco Oliveto},
   city = {New York, NY, USA},
   doi = {10.1145/3654438},
   issn = {1049-331X},
   issue = {6},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {NLP;,Voice user interfaces,software testing},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {Help Them Understand: Testing and Improving Voice User Interfaces},
   volume = {33},
   url = {https://doi.org/10.1145/3654438},
   year = {2024}
}
@inproceedings{Kazemitabaar2024,
   abstract = {LLM-powered tools like ChatGPT Data Analysis, have the potential to help users tackle the challenging task of data analysis programming, which requires expertise in data processing, programming, and statistics. However, our formative study (n=15) uncovered serious challenges in verifying AI-generated results and steering the AI (i.e., guiding the AI system to produce the desired output). We developed two contrasting approaches to address these challenges. The first (Stepwise) decomposes the problem into step-by-step subgoals with pairs of editable assumptions and code until task completion, while the second (Phasewise) decomposes the entire problem into three editable, logical phases: structured input/output assumptions, execution plan, and code. A controlled, within-subjects experiment (n=18) compared these systems against a conversational baseline. Users reported significantly greater control with the Stepwise and Phasewise systems, and found intervention, correction, and verification easier, compared to the baseline. The results suggest design guidelines and trade-offs for AI-assisted data analysis tools.},
   author = {Majeed Kazemitabaar and Jack Williams and Ian Drosos and Tovi Grossman and Austin Zachary Henley and Carina Negreanu and Advait Sarkar},
   city = {New York, NY, USA},
   doi = {10.1145/3654777.3676345},
   isbn = {9798400706288},
   booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
   keywords = {AI Agents,Copilot,Data Analysis,Data Science Assistant,Generative AI,Human-AI Interaction,Large Language Models},
   publisher = {Association for Computing Machinery},
   title = {Improving Steering and Verification in AI-Assisted Data Analysis with Interactive Task Decomposition},
   url = {https://doi.org/10.1145/3654777.3676345},
   year = {2024}
}
@article{Xu2024-3,
   abstract = {Metamorphic Testing (MT) alleviates the oracle problem by defining oracles based on metamorphic relations (MRs) that govern multiple related inputs and their outputs. However, designing MRs is challenging, as it requires domain-specific knowledge. This hinders the widespread adoption of MT. We observe that developer-written test cases can embed domain knowledge that encodes MRs. Such encoded MRs could be synthesized for testing not only their original programs but also other programs that share similar functionalities.In this article, we propose MR-Scout to automatically synthesize MRs from test cases in open-source software (OSS) projects. MR-Scout first discovers MR-encoded test cases (MTCs), and then synthesizes the encoded MRs into parameterized methods (called codified MRs), and filters out MRs that demonstrate poor quality for new test case generation. MR-Scout discovered over 11,000 MTCs from 701 OSS projects. Experimental results show that over 97\% of codified MRs are of high quality for automated test case generation, demonstrating the practical applicability of MR-Scout. Furthermore, codified-MRs-based tests effectively enhance the test adequacy of programs with developer-written tests, leading to 13.52\% and 9.42\% increases in line coverage and mutation score, respectively. Our qualitative study shows that 55.76\% to 76.92\% of codified MRs are easily comprehensible for developers.},
   author = {Congying Xu and Valerio Terragni and Hengcheng Zhu and Jiarong Wu and Shing-Chi Cheung},
   city = {New York, NY, USA},
   doi = {10.1145/3656340},
   issn = {1049-331X},
   issue = {6},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Software testing,automated test case generation,metamorphic relation,metamorphic testing},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {MR-Scout: Automated Synthesis of Metamorphic Relations from Existing Test Cases},
   volume = {33},
   url = {https://doi.org/10.1145/3656340},
   year = {2024}
}
@article{Chen2024-7,
   abstract = {Deep learning (DL) is a critical tool for real-world applications, and comprehensive testing of DL models is vital to ensure their quality before deployment. However, recent studies have shown that even subtle deviations in DL operators can result in catastrophic consequences, underscoring the importance of rigorous testing of these components. Unlike testing other DL system components, operator analysis poses unique challenges due to complex inputs and uncertain outputs. The existing DL operator testing approach has limitations in terms of testing efficiency and error localization. In this paper, we propose Meta, a novel operator testing framework based on metamorphic testing that automatically tests and assists bug location based on metamorphic relations (MRs). Meta distinguishes itself in three key ways: (1) it considers both parameters and input tensors to detect operator errors, enabling it to identify both implementation and precision errors; (2) it uses MRs to guide the generation of more effective inputs (i.e., tensors and parameters) in less time; (3) it assists the precision error localization by tracing the error to the input level of the operator based on MR violations. We designed 18 MRs for testing 10 widely used DL operators. To assess the effectiveness of Meta, we conducted experiments on 13 released versions of 5 popular DL libraries. Our results revealed that Meta successfully detected 41 errors, including 14 new ones that were reported to the respective platforms and 8 of them are confirmed/fixed. Additionally, Meta demonstrated high efficiency, outperforming the baseline by detecting ∼2 times more errors of the baseline. Meta is open-sourced and available at https://github.com/TDY-raedae/Medi-Test.},
   author = {Jinyin Chen and Chengyu Jia and Yunjie Yan and Jie Ge and Haibin Zheng and Yao Cheng},
   city = {New York, NY, USA},
   doi = {10.1145/3660796},
   issue = {FSE},
   journal = {Proc. ACM Softw. Eng.},
   keywords = {Deep Learning,Deep Learning Operator,Defect Location,Metamorphic Testing},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {A Miss Is as Good as A Mile: Metamorphic Testing for Deep Learning Operators},
   volume = {1},
   url = {https://doi.org/10.1145/3660796},
   year = {2024}
}

@inproceedings{Mattis2024,
   abstract = {Feedback during programming is desirable, but its usefulness depends on immediacy and relevance to the task. Unit and regression testing are practices to ensure programmers can obtain feedback on their changes; however, running a large test suite is rarely fast, and only a few results are relevant. Identifying tests relevant to a change can help programmers in two ways: upcoming issues can be detected earlier during programming, and relevant tests can serve as examples to help programmers understand the code they are editing. In this work, we describe an approach to evaluate how well large language models (LLMs) and embedding models can judge the relevance of a test to a change. We construct a dataset by applying faulty variations of real-world code changes and measuring whether the model could nominate the failing tests beforehand. We found that, while embedding models perform best on such a task, even simple information retrieval models are surprisingly competitive. In contrast, pre-trained LLMs are of limited use as they focus on confounding aspects like coding styles. We argue that the high computational cost of AI models is not always justified, and tool developers should also consider non-AI models for code-related retrieval and recommendation tasks. Lastly, we generalize from unit tests to live examples and outline how our approach can benefit live programming environments.},
   author = {Toni Mattis and Lukas Böhme and Eva Krebs and Martin C Rinard and Robert Hirschfeld},
   city = {New York, NY, USA},
   doi = {10.1145/3660829.3660837},
   isbn = {9798400706349},
   booktitle = {Companion Proceedings of the 8th International Conference on the Art, Science, and Engineering of Programming},
   keywords = {embedding models,generative ai,large language models,test prioritization,testing},
   pages = {32-40},
   publisher = {Association for Computing Machinery},
   title = {Faster Feedback with AI? A Test Prioritization Study},
   url = {https://doi.org/10.1145/3660829.3660837},
   year = {2024}
}
@inproceedings{Leotta2024-1,
   abstract = {Automated testing is vital for ensuring the reliability of web applications. This paper presents a preliminary study on leveraging artificial intelligence (AI) models, specifically ChatGPT and Github Copilot, to generate test scripts for web end-to-end testing. Through experimentation, we evaluated the feasibility and effectiveness of AI language models in generating test scripts based on natural language descriptions of user interactions with web applications. Our preliminary results show that AI-based generation generally provides an advantage over fully manual test scripts development. Starting from test cases clearly defined in Gherkin, a reduction in development time is always observable. In some cases, this reduction is statistically significant (e.g., Manual vs. a particular use of ChatGPT). These results are valid provided that the tester has some skills in manual test script development and is therefore able to modify the code produced by the AI-generation tools. This study contributes to the exploration of AI-driven solutions in web test scripts generation and lays the foundation for future research in this domain.},
   author = {Maurizio Leotta and Hafiz Zeeshan Yousaf and Filippo Ricca and Boni Garcia},
   city = {New York, NY, USA},
   doi = {10.1145/3661167.3661192},
   isbn = {9798400717017},
   booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
   keywords = {ChatGPT,E2E Testing,Empirical Study.,GitHub Copilot,LLM,Selenium WebDriver,Test Automation},
   pages = {339-344},
   publisher = {Association for Computing Machinery},
   title = {AI-Generated Test Scripts for Web E2E Testing with ChatGPT and Copilot: A Preliminary Study},
   url = {https://doi.org/10.1145/3661167.3661192},
   year = {2024}
}
@inproceedings{Esposito2024,
   abstract = {Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications. Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision. Aim: We aim to comprehensively evaluate SASTTs, setting a reliable benchmark for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives. Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase. It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis. Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities. In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall. Conclusions: Our findings suggest that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions.},
   author = {Matteo Esposito and Valentina Falaschi and Davide Falessi},
   city = {New York, NY, USA},
   doi = {10.1145/3661167.3661199},
   isbn = {9798400717017},
   booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
   keywords = {Common Vulnerability Exposure,Common Weakness Enumeration,Security Assessment Tool,Static Application Security Testing},
   pages = {69-78},
   publisher = {Association for Computing Machinery},
   title = {An Extensive Comparison of Static Application Security Testing Tools},
   url = {https://doi.org/10.1145/3661167.3661199},
   year = {2024}
}
@inproceedings{Gmez-Abajo2024,
   abstract = {Conversational agents, or chatbots, are increasingly used to access all sorts of services using natural language. While open-domain chatbots – like ChatGPT – can converse on any topic, task-oriented chatbots – the focus of this paper – are designed for specific tasks, like booking a flight, obtaining customer support, or setting an appointment. Like any other software, task-oriented chatbots need to be properly tested, usually by defining and executing test scenarios (i.e., sequences of user-chatbot interactions). However, there is currently a lack of methods to quantify the completeness and strength of such test scenarios, which can lead to low-quality tests, and hence to buggy chatbots. To fill this gap, we propose adapting mutation testing (MuT) for task-oriented chatbots. To this end, we introduce a set of mutation operators that emulate faults in chatbot designs, an architecture that enables MuT on chatbots built using heterogeneous technologies, and a practical realisation as an Eclipse plugin. Moreover, we evaluate the applicability, effectiveness and efficiency of our approach on open-source chatbots, with promising results.},
   author = {Pablo Gómez-Abajo and Sara Pérez-Soler and Pablo C Cañizares and Esther Guerra and Juan de Lara},
   city = {New York, NY, USA},
   doi = {10.1145/3661167.3661220},
   isbn = {9798400717017},
   booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
   keywords = {Botium,Dialogflow,Mutation testing,Rasa,Task-oriented chatbots},
   pages = {232-241},
   publisher = {Association for Computing Machinery},
   title = {Mutation Testing for Task-Oriented Chatbots},
   url = {https://doi.org/10.1145/3661167.3661220},
   year = {2024}
}
@article{Wu2024,
   abstract = {With the ever-increasing hardware design complexity comes the realization that efforts required for hardware verification increase at an even faster rate. Driven by the push from the desired verification productivity boost and the pull from leap-ahead capabilities of machine learning (ML), recent years have witnessed the emergence of exploiting ML-based techniques to improve the efficiency of hardware verification. In this article, we present a panoramic view of how ML-based techniques are embraced in hardware design verification, from formal verification to simulation-based verification, from academia to industry, and from current progress to future prospects. We envision that the adoption of ML-based techniques will pave the road for more scalable, more intelligent, and more productive hardware verification.},
   author = {Nan Wu and Yingjie Li and Hang Yang and Hanqiu Chen and Steve Dai and Cong Hao and Cunxi Yu and Yuan Xie},
   city = {New York, NY, USA},
   doi = {10.1145/3661308},
   issn = {1084-4309},
   issue = {4},
   journal = {ACM Trans. Des. Autom. Electron. Syst.},
   keywords = {Hardware verification,formal verification,machine learning,simulation-based verification},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {Survey of Machine Learning for Software-assisted Hardware Design Verification: Past, Present, and Prospect},
   volume = {29},
   url = {https://doi.org/10.1145/3661308},
   year = {2024}
}
@inproceedings{Zheng2024,
   abstract = {The integration of machine learning into cyber-physical systems (CPS) promises enhanced efficiency and autonomous capabilities, revolutionizing fields like autonomous vehicles and telemedicine. This evolution necessitates a shift in the software development life cycle, where data and learning are pivotal. Traditional verification and validation methods are inadequate for these AI-driven systems. This study focuses on the challenges in ensuring safety in learning-enabled CPS. It emphasizes the role of testing as a primary method for verification and validation, critiques current methodologies, and advocates for a more rigorous approach to assure formal safety.},
   author = {Xi Zheng and Aloysius K Mok and Ruzica Piskac and Yong Jae Lee and Bhaskar Krishnamachari and Dakai Zhu and Oleg Sokolsky and Insup Lee},
   city = {New York, NY, USA},
   doi = {10.1145/3663529.3663779},
   isbn = {9798400706585},
   booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
   keywords = {AI-based Systems,LLM-based Testing,automata-learning,model-based testing},
   pages = {467-471},
   publisher = {Association for Computing Machinery},
   title = {Testing Learning-Enabled Cyber-Physical Systems with Large-Language Models: A Formal Approach},
   url = {https://doi.org/10.1145/3663529.3663779},
   year = {2024}
}

@inproceedings{Zhang2024,
   abstract = {Nowadays, the testing of large-scale microservices could produce an enormous number of test alarms daily. Manually diagnosing these alarms is time-consuming and laborious for the testers. Automatic fault diagnosis with fault classification and localization can help testers efficiently handle the increasing volume of failed test cases. However, the current methods for diagnosing test alarms struggle to deal with the complex and frequently updated microservices. In this paper, we introduce SynthoDiag, a novel fault diagnosis framework for test alarms in microservices through multi-source logs (execution logs, trace logs, and test case information) organized with a knowledge graph. An Entity Fault Association and Position Value (EFA-PV) algorithm is proposed to localize the fault-indicative log entries. Additionally, an efficient block-based differentiation approach is used to filter out fault-irrelevant entries in the test cases, significantly improving the overall performance of fault diagnosis. At last, SynthoDiag is systematically evaluated with a large-scale real-world dataset from a top-tier global cloud service provider, Huawei Cloud, which provides services for more than three million users. The results show the Micro-F1 and Macro-F1 scores improvement of SynthoDiag over baseline methods in fault classification are 21\% and 30\%, respectively, and its top-5 accuracy of fault localization is 81.9\%, significantly surpassing the previous methods.},
   author = {Shenglin Zhang and Jun Zhu and Bowen Hao and Yongqian Sun and Xiaohui Nie and Jingwen Zhu and Xilin Liu and Xiaoqian Li and Yuchi Ma and Dan Pei},
   city = {New York, NY, USA},
   doi = {10.1145/3663529.3663833},
   isbn = {9798400706585},
   booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
   keywords = {Execution Logs,Fault Diagnosis,Microservice,Test Case,Trace Logs},
   pages = {115-125},
   publisher = {Association for Computing Machinery},
   title = {Fault Diagnosis for Test Alarms in Microservices through Multi-source Data},
   url = {https://doi.org/10.1145/3663529.3663833},
   year = {2024}
}
@inproceedings{Alshahwan2024-1,
   abstract = {TestGen automatically generates unit tests, carved from serialized observations of complex objects, observed during app execution.  We describe the development and deployment of TestGen at Meta.   In particular, we focus on the scalability challenges overcome during development in order to deploy observation-based test carving at scale in industry.  So far, TestGen has landed 518 tests into production, which have been executed 9,617,349 times in continuous integration, finding 5,702 faults.   Meta is currently in the process of more widespread deployment.  Our evaluation reveals that, when carving its observations from 4,361 reliable end-to-end tests, TestGen was able to generate tests for at least 86\% of the classes covered by end-to-end tests.   Testing on 16 Kotlin Instagram app-launch-blocking tasks demonstrated that the TestGen tests would have trapped 13 of these before they became launch blocking.},
   author = {Nadia Alshahwan and Mark Harman and Alexandru Marginean and Rotem Tal and Eddy Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3663529.3663838},
   isbn = {9798400706585},
   booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
   keywords = {Automated test generation,test carving,unit testing},
   pages = {173-184},
   publisher = {Association for Computing Machinery},
   title = {Observation-Based Unit Test Generation at Meta},
   url = {https://doi.org/10.1145/3663529.3663838},
   year = {2024}
}
@inproceedings{Alshahwan2024,
   abstract = {This paper describes Meta’s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests.     TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination.    We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms.     In an evaluation on Reels and Stories products for Instagram,     75\% of TestGen-LLM’s test cases built correctly, 57\% passed reliably, and 25\% increased coverage.    During Meta’s Instagram and Facebook test-a-thons, it improved 11.5\% of all classes to which it was applied, with 73\% of its recommendations being accepted for production deployment by Meta software engineers.    We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.},
   author = {Nadia Alshahwan and Jubin Chheda and Anastasia Finogenova and Beliz Gokkaya and Mark Harman and Inna Harper and Alexandru Marginean and Shubho Sengupta and Eddy Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3663529.3663839},
   isbn = {9798400706585},
   booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
   keywords = {Automated Test Generation,Genetic Improvement,LLMs,Large Language Models,Unit Testing},
   pages = {185-196},
   publisher = {Association for Computing Machinery},
   title = {Automated Unit Test Improvement using Large Language Models at Meta},
   url = {https://doi.org/10.1145/3663529.3663839},
   year = {2024}
}

@inproceedings{Paduraru2024,
   abstract = {Challenges related to game quality, whether occurring during initial release or after updates, can result in player dissatisfaction, media scrutiny, and potential financial setbacks. These issues may stem from factors like software bugs, performance bottlenecks, or security vulnerabilities. Despite these challenges, game developers often rely on manual playtesting, highlighting the need for more robust and automated processes in game development.  This research explores the application of Large Language Models (LLMs) for automating unit test creation in game development, with a specific focus on strongly typed programming languages like C++ and C#, widely used in the industry. The study centers around fine-tuning Code Llama, an advanced code generation model, to address common scenarios encountered in game development, including game engines and specific APIs or backends. Although the prototyping and evaluations primarily occurred within the Unity game engine, the proposed methods can be adapted to other internal or publicly available solutions. The evaluation outcomes demonstrate the effectiveness of these methods in enhancing existing unit test suites or automatically generating new tests based on natural language descriptions of class contexts and targeted methods.},
   author = {Ciprian Paduraru and Alin Stefanescu and Augustin Jianu},
   city = {New York, NY, USA},
   doi = {10.1145/3663532.3664466},
   isbn = {9798400706745},
   booktitle = {Proceedings of the 1st ACM International Workshop on Foundations of Applied Software Engineering for Games},
   keywords = {game development,large language models,unit testing},
   pages = {7-13},
   publisher = {Association for Computing Machinery},
   title = {Unit Test Generation using Large Language Models for Unity Game Development},
   url = {https://doi.org/10.1145/3663532.3664466},
   year = {2024}
}
@inproceedings{Chang2024,
   abstract = {Image editing is an iterative process that requires precise visual evaluation and manipulation for the output to match the editing intent. However, current image editing tools do not provide accessible interaction nor sufficient feedback for blind and low vision individuals to achieve this level of control. To address this, we developed EditScribe, a prototype system that makes object-level image editing actions accessible using natural language verification loops powered by large multimodal models. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including a summary of visual changes, AI judgement, and updated general and object descriptions. The user can ask follow-up questions to clarify and probe into the edits or verification feedback, before performing another edit. In a study with ten blind or low-vision users, we found that EditScribe supported participants to perform and verify image edit actions non-visually. We observed different prompting strategies from participants, and their perceptions on the various types of verification feedback. Finally, we discuss the implications of leveraging natural language verification loops to make visual authoring non-visually accessible.},
   author = {Ruei-Che Chang and Yuxuan Liu and Lotus Zhang and Anhong Guo},
   city = {New York, NY, USA},
   doi = {10.1145/3663548.3675599},
   isbn = {9798400706776},
   booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
   keywords = {Accessibility,assistive technology,blind,creativity support tools,generative AI,image editing,low vision,visual authoring},
   publisher = {Association for Computing Machinery},
   title = {EditScribe: Non-Visual Image Editing with Natural Language Verification Loops},
   url = {https://doi.org/10.1145/3663548.3675599},
   year = {2024}
}
@inproceedings{Hlk2024,
   abstract = {Modern malware increasingly deploys network covert channels to prevent detection or bypass firewalls. Unfortunately, the early discovery of protocol fields and functional behaviors of traffic that can be abused to conceal information is very challenging. In this perspective, fuzz testing could help to face the tight relationship between the used hiding scheme and the targeted protocol trait. Even if fuzzing is a well-established practice to reveal implementation issues, bugs, or unhandled behaviors, it has never been considered to assess the “susceptilibility” of protocols to covert communications. Therefore, this paper explores the use of basic fuzzing techniques to quantify how ubiquitous HTTP conversations can be manipulated by an attacker to create a network covert channel. To this aim, we developed an ad-hoc random fuzzer, which mutates a reference HTTP request to simulate the presence of various cloaking attempts. To evaluate the feasibility of our idea, we conducted a thorough test campaign considering three different covert channels hidden in traffic exchanged with 1,000 real Web destinations. Results indicate that fuzzing should be considered a valid technique to investigate how HTTP can be altered to cloak data.},
   author = {Kai Hölk and Wojciech Mazurczyk and Marco Zuppelli and Luca Caviglione},
   city = {New York, NY, USA},
   doi = {10.1145/3664476.3664493},
   isbn = {9798400717185},
   booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
   keywords = {Covert Channels,Fuzz Testing,HTTP,Information Hiding},
   publisher = {Association for Computing Machinery},
   title = {Investigating HTTP Covert Channels Through Fuzz Testing},
   url = {https://doi.org/10.1145/3664476.3664493},
   year = {2024}
}
@article{Sun2024,
   abstract = {Machine translation is integral to international communication and extensively employed in diverse human-related applications. Despite remarkable progress, fairness issues persist within current machine translation systems. In this article, we propose FairMT, an automated fairness testing approach tailored for machine translation systems. FairMT operates on the assumption that translations of semantically similar sentences, containing protected attributes from distinct demographic groups, should maintain comparable meanings. It comprises three key steps: (1) test input generation, producing inputs covering various demographic groups; (2) test oracle generation, identifying potential unfair translations based on semantic similarity measurements; and (3) regression, discerning genuine fairness issues from those caused by low-quality translation. Leveraging FairMT, we conduct an empirical study on three leading machine translation systems–Google Translate, T5, and Transformer. Our investigation uncovers up to 832, 1,984, and 2,627 unfair translations across the three systems, respectively. Intriguingly, we observe that fair translations tend to exhibit superior translation performance, challenging the conventional wisdom of a fairness-performance tradeoff prevalent in the fairness literature.},
   author = {Zeyu Sun and Zhenpeng Chen and Jie Zhang and Dan Hao},
   city = {New York, NY, USA},
   doi = {10.1145/3664608},
   issn = {1049-331X},
   issue = {6},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Fairness testing,machine translation,metamorphic testing,protected attributes},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {Fairness Testing of Machine Translation Systems},
   volume = {33},
   url = {https://doi.org/10.1145/3664608},
   year = {2024}
}
@inproceedings{Kulsum2024,
   abstract = {Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored.















In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously generated patches.















To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14\% and 7.6\% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair.},
   author = {Ummay Kulsum and Haotian Zhu and Bowen Xu and Marcelo d'Amorim},
   city = {New York, NY, USA},
   doi = {10.1145/3664646.3664770},
   isbn = {9798400706851},
   booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
   keywords = {Automated Vulnerability Repair,Large Language Models},
   pages = {103-111},
   publisher = {Association for Computing Machinery},
   title = {A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback},
   url = {https://doi.org/10.1145/3664646.3664770},
   year = {2024}
}
@inproceedings{KouemoNgassom2024,
   abstract = {LLM-based assistants, such as GitHub Copilot and ChatGPT, have the potential to generate code that fulfills a programming task described in a natural language description, referred to as a prompt. The widespread accessibility of these assistants enables users with diverse backgrounds to generate code and integrate it into software projects. However, studies show that code generated by LLMs is prone to bugs and may miss various corner cases in task specifications. Presenting such buggy code to users can impact their reliability and trust in LLM-based assistants. Moreover, significant efforts are required by the user to detect and repair any bug present in the code, especially if no test cases are available. In this study, we propose a self-refinement method aimed at improving the reliability of code generated by LLMs by minimizing the number of bugs before execution, without human intervention, and in the absence of test cases. Our approach is based on targeted Verification Questions (VQs) to identify potential bugs within the initial code. These VQs target various nodes within the Abstract Syntax Tree (AST) of the initial code, which have the potential to trigger specific types of bug patterns commonly found in LLM-generated code. Finally, our method attempts to repair these potential bugs by re-prompting the LLM with the targeted VQs and the initial code. Our evaluation, based on programming tasks in the CoderEval dataset, demonstrates that our proposed method outperforms state-of-the-art methods by decreasing the number of targeted errors in the code between 21\% to 62\% and improving the number of executable code instances to 13\%.},
   author = {Sylvain Kouemo Ngassom and Arghavan Moradi Dakhel and Florian Tambon and Foutse Khomh},
   city = {New York, NY, USA},
   doi = {10.1145/3664646.3664772},
   isbn = {9798400706851},
   booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
   keywords = {Code Generation,Hallucination,Large Language Model,Reliability,Software Development},
   pages = {122-130},
   publisher = {Association for Computing Machinery},
   title = {Chain of Targeted Verification Questions to Improve the Reliability of Code Generated by LLMs},
   url = {https://doi.org/10.1145/3664646.3664772},
   year = {2024}
}
@article{Feng2024,
   abstract = {Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker, which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker, we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs’ response latency and energy consumption by 325\% to 3,244\% and 344\% to 3,616\%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).},
   author = {Xiaoning Feng and Xiaohong Han and Simin Chen and Wei Yang},
   city = {New York, NY, USA},
   doi = {10.1145/3664812},
   issn = {1049-331X},
   issue = {7},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Machine learning,large language model,software testing},
   month = {8},
   publisher = {Association for Computing Machinery},
   title = {LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models},
   volume = {33},
   url = {https://doi.org/10.1145/3664812},
   year = {2024}
}
@inproceedings{Ayalasomayajula2024,
   abstract = {As the complexity of System-on-Chips (SoCs) increases, ensuring their security presents escalating challenges. Formal property verification is one of the most robust methods to model and check security behaviors using model checkers. However, the generation of these security properties is a labor-intensive endeavor. Large language models (LLMs) have been applied in multiple fields due to their excellent ability to understand natural language. Hence, this paper presents a novel framework that utilizes LLMs to automate the generation of security properties directly from Register Transfer Level (RTL) designs. By extracting critical features and security assets from both the design specifications and RTL, the framework systematically produces tailored security properties for specific hardware designs. These properties are systematically cataloged in a security property database, providing an essential resource for ongoing and future hardware verification efforts. The effectiveness of this innovative framework is validated through its application to various open-source hardware designs, confirming its ability to significantly improve SoC security verification by efficiently generating robust security properties.},
   author = {Avinash Ayalasomayajula and Rui Guo and Jingbo Zhou and Sujan Kumar Saha and Farimah Farahmandi},
   city = {New York, NY, USA},
   doi = {10.1145/3670474.3685967},
   isbn = {9798400706998},
   booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
   keywords = {Formal Property Verification,Large Language Models,Security Property Generation,System-on-Chip Security},
   publisher = {Association for Computing Machinery},
   title = {LASP: LLM Assisted Security Property Generation for SoC Verification},
   url = {https://doi.org/10.1145/3670474.3685967},
   year = {2024}
}
@inproceedings{Wang2024-5,
   abstract = {The emergence of Large Language Models (LLMs) has impacted our perspective on applying Machine Learning (ML) in semiconductor test. This paper shares our experience in leveraging the power of LLMs to build an AI agent for test data analytics. We advocate for an end-to-end approach where the Knowledge Graph (KG) plays a central role. Using wafermap analytics as an example, we highlight the key ideas behind developing the LLM-assisted AI agent named IEA-Plot, and discuss its practical applications.},
   author = {Li-C. Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3670474.3685974},
   isbn = {9798400706998},
   booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
   keywords = {Knowledge Graph,Large Language Model,Machine Learning,Test Data Analytics},
   publisher = {Association for Computing Machinery},
   title = {LLM-Assisted Analytics in Semiconductor Test (Invited)},
   url = {https://doi.org/10.1145/3670474.3685974},
   year = {2024}
}
@inproceedings{Francisco2024,
   abstract = {The complexity in design rule description and coding is drastically increasing as technology nodes advance. This complexity makes the process of implementing the physical verification (PV) rule checks more time-consuming and susceptible to human error, creating the need to explore alternate methods to improve the runset creation process. The work presented proposes a generative AI solution that uses Large Language Models (LLMs) to interpret rule descriptions and generate design rule check decks (runsets) in a language that a PV tool can interpret. The LLM is fine-tuned with existing design rule manuals and runsets. After post-processing the LLM output, the presented solution can generate rules implementation with up to 97\% accuracy. The proposed solution can be used as a runset writer Co-Pilot to help develop the new physical verification runsets.},
   author = {Luis Francisco and Srini Arikati},
   city = {New York, NY, USA},
   doi = {10.1145/3670474.3685976},
   isbn = {9798400706998},
   booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
   keywords = {AI,Design Rule Checking,GenAI,LLMs,Physical Verification,Runset Creation},
   publisher = {Association for Computing Machinery},
   title = {LLM Based Physical Verification Runset Generator},
   url = {https://doi.org/10.1145/3670474.3685976},
   year = {2024}
}
@article{Mcintosh2024,
   abstract = {In response to diverse perspectives on artificial general intelligence (AGI), ranging from potential safety and ethical concerns to more extreme views about the threats it poses to humanity, this research presents a generic method to gauge the reasoning capabilities of artificial intelligence (AI) models as a foundational step in evaluating safety measures. Recognizing that AI reasoning measures cannot be wholly automated, due to factors such as cultural complexity, we conducted an extensive examination of five commercial generative pre-trained transformers (GPTs), focusing on their comprehension and interpretation of culturally intricate contexts. Utilizing our novel “Reasoning and Value Alignment Test,” we assessed the GPT models’ ability to reason in complex situations and grasp local cultural subtleties. Our findings have indicated that, although the models have exhibited high levels of human-like reasoning, significant limitations remained, especially concerning the interpretation of cultural contexts. This article also explored potential applications and use-cases of our Test, underlining its significance in AI training, ethics compliance, sensitivity auditing, and AI-driven cultural consultation. We concluded by emphasizing its broader implications in the AGI domain, highlighting the necessity for interdisciplinary approaches, wider accessibility to various GPT models, and a profound understanding of the interplay between GPT reasoning and cultural sensitivity.},
   author = {Timothy R Mcintosh and Tong Liu and Teo Susnjak and Paul Watters and Malka N Halgamuge},
   city = {New York, NY, USA},
   doi = {10.1145/3670691},
   issn = {2160-6455},
   issue = {3},
   journal = {ACM Trans. Interact. Intell. Syst.},
   keywords = {AI model limitations,AI training and assessment,Large language model (LLM),cross-cultural AI applications,cultural sensitivity,reasoning and value alignment test},
   month = {8},
   publisher = {Association for Computing Machinery},
   title = {A Reasoning and Value Alignment Test to Assess Advanced GPT Reasoning},
   volume = {14},
   url = {https://doi.org/10.1145/3670691},
   year = {2024}
}
@inproceedings{Xiao2024-3,
   abstract = {Search-based unit test generation methods have been considered effective and widely applied, and Large Language Models (LLMs) have also demonstrated their powerful generation ability. Therefore, some scholars have proposed using LLMs to enhance search-based unit test generation methods and have preliminarily confirmed that LLMs can help alleviate the problem of test coverage plateaus. However, it is still unclear when and how LLMs should intervene in the time-consuming test generation process. This paper explores the application of LLMs at various stages of search-based test generation (SBTG) (including the initial stage, the test generation period, and the test coverage plateaus), as well as strategies for controlling the frequency of LLM intervention. A comprehensive empirical study was conducted on 486 Python benchmark modules from 27 projects. The experimental results show that 1) LLM intervention has a positive effect at any stage, whether to improve coverage over a fixed period or to reduce the time to reach a specific coverage; 2) a reasonable intervention frequency is crucial for LLMs to have a positive effect on SBTG. This work can better help understand when and how LLMs should be applied in SBTG and provide valuable suggestions for developers in practice.},
   author = {Danni Xiao and Yimeng Guo and Yanhui Li and Lin Chen},
   city = {New York, NY, USA},
   doi = {10.1145/3671016.3674813},
   isbn = {9798400707056},
   booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
   keywords = {Large Language Model,Search-based Testing,Unit Test},
   pages = {71-80},
   publisher = {Association for Computing Machinery},
   title = {Optimizing Search-Based Unit Test Generation with Large Language Models: An Empirical Study},
   url = {https://doi.org/10.1145/3671016.3674813},
   year = {2024}
}
@article{Lou2024,
   abstract = {In recent years, Automated Program Repair (APR) has been extensively studied in academia and even drawn wide attention from the industry. However, APR techniques can be extremely time consuming since (1) a large number of patches can be generated for a given bug, and (2) each patch needs to be executed on the original tests to ensure its correctness. In the literature, various techniques (e.g., based on learning, mining, and constraint solving) have been proposed/studied to reduce the number of patches. Intuitively, every patch can be treated as a software revision during regression testing; thus, traditional Regression Test Selection (RTS) techniques can be leveraged to only execute the tests affected by each patch (as the other tests would keep the same outcomes) to further reduce patch execution time. However, few APR systems actually adopt RTS and there is still a lack of systematic studies demonstrating the benefits of RTS and the impact of different RTS strategies on APR. To this end, this article presents the first extensive study of widely used RTS techniques at different levels (i.e., class/method/statement levels) for 12 state-of-the-art APR systems on over 2M patches. Our study reveals various practical guidelines for bridging the gap between APR and regression testing, including: (1) the number of patches widely used for measuring APR efficiency can incur skewed conclusions, and the use of inconsistent RTS configurations can further skew the conclusions; (2) all studied RTS techniques can substantially improve APR efficiency and should be considered in future APR work; (3) method- and statement-level RTS outperform class-level RTS substantially and should be preferred; (4) RTS techniques can substantially outperform state-of-the-art test prioritization techniques for APR, and combining them can further improve APR efficiency; and (5) traditional Regression Test Prioritization (RTP) widely studied in regression testing performs even better than APR-specific test prioritization when combined with most RTS techniques. Furthermore, we also present the detailed impact of different patch categories and patch validation strategies on our findings.},
   author = {Yiling Lou and Jun Yang and Samuel Benton and Dan Hao and Lin Tan and Zhenpeng Chen and Lu Zhang and Lingming Zhang},
   city = {New York, NY, USA},
   doi = {10.1145/3672450},
   issn = {1049-331X},
   issue = {7},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Test selection,patch validation,program repair},
   month = {9},
   publisher = {Association for Computing Machinery},
   title = {When Automated Program Repair Meets Regression Testing—An Extensive Study on Two Million Patches},
   volume = {33},
   url = {https://doi.org/10.1145/3672450},
   year = {2024}
}

@article{Lee2024,
   abstract = {Natural language processing (NLP) has gained widespread adoption in the development of real-world applications. However, the black-box nature of neural networks in NLP applications poses a challenge when evaluating their performance, let alone ensuring it. Recent research has proposed testing techniques to enhance the trustworthiness of NLP-based applications. However, most existing works use a single, aggregated metric (i.e., accuracy) which is difficult for users to assess NLP model performance on fine-grained aspects, such as LCs. To address this limitation, we present ALiCT, an automated testing technique for validating NLP applications based on their LCs. ALiCT takes user-specified LCs as inputs and produces diverse test suite with test oracles for each of given LC. We evaluate ALiCT on two widely adopted NLP tasks, sentiment analysis and hate speech detection, in terms of diversity, effectiveness, and consistency. Using Self-BLEU and syntactic diversity metrics, our findings reveal that ALiCT generates test cases that are 190\% and 2213\% more diverse in semantics and syntax, respectively, compared to those produced by state-of-the-art techniques. In addition, ALiCT is capable of producing a larger number of NLP model failures in 22 out of 25 LCs over the two NLP applications.},
   author = {Jaeseong Lee and Simin Chen and Austin Mordahl and Cong Liu and Wei Yang and Shiyi Wei},
   city = {New York, NY, USA},
   doi = {10.1145/3672455},
   issn = {1049-331X},
   issue = {7},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {LC,Software testing,hate speech detection,sentiment analysis},
   month = {9},
   publisher = {Association for Computing Machinery},
   title = {Automated Testing Linguistic Capabilities of NLP Models},
   volume = {33},
   url = {https://doi.org/10.1145/3672455},
   year = {2024}
}
@inproceedings{Shi2024-2,
   abstract = {Implementing automation testing is difficult and as a consequence there is a growing desire for semi-automated software testing systems with humans in the loop. Leveraging the growth of LLMs, recent research has demonstrated LLMs’ potential to improve performance on test generation, reporting, and bug triaging. However, relatively little work has explored the interactivity issues that emerge in semi-automated LLM-assisted software test case development. To fill this gap, we present two user studies (N1 = 16, N2 = 24) that investigate productivity, creativity, and user attention in three semi-automated LLM-assisted interaction strategies: (1) pre-emptive prompting; (2) buffered response; and (3) guided input. We find that pre-emptively prompting the user significantly enhances branch coverage and task creativity by more than 30\% while reducing user’s off-task idle time by up to 48.7\%. We conclude by suggesting concrete research directions applying mixed-initiative principles for LLM-based interactive systems for semi-automated software testing.},
   author = {Billy Shi and Per Ola Kristensson},
   city = {New York, NY, USA},
   doi = {10.1145/3672539.3686341},
   isbn = {9798400707186},
   booktitle = {Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
   publisher = {Association for Computing Machinery},
   title = {Pay Attention! Human-Centric Improvements of LLM-based Interfaces for Assisting Software Test Case Development},
   url = {https://doi.org/10.1145/3672539.3686341},
   year = {2024}
}
@article{Savage2024,
   abstract = {Modern large language models appear to be able to fool the Turing Test. Will we know if they are becoming as intelligent as people?},
   author = {Neil Savage},
   city = {New York, NY, USA},
   doi = {10.1145/3673427},
   issn = {0001-0782},
   issue = {9},
   journal = {Commun. ACM},
   month = {8},
   pages = {10-12},
   publisher = {Association for Computing Machinery},
   title = {Beyond Turing: Testing LLMs for Intelligence},
   volume = {67},
   url = {https://doi.org/10.1145/3673427},
   year = {2024}
}

@inproceedings{Jones2024,
   abstract = {Background: Software Package Registries (SPRs) are an integral part of the software supply chain. These collaborative platforms unite contributors, users, and code for streamlined package management. Prior work has characterized the SPRs associated with traditional software, such as NPM (JavaScript) and PyPI (Python). Pre-Trained Model (PTM) Registries are an emerging class of SPR of increasing importance, because they support the deep learning supply chain. A growing body of empirical research has examined PTM registries from various angles, such as vulnerabilities, reuse processes, and evolution. However, no synthesis provides a systematic understanding of current knowledge. Furthermore, much of the existing research includes non-quantified qualitative observations. Aims: First, we aim to provide a systematic knowledge synthesis. Second, we quantify qualitative claims. Methods: We conducted a systematic literature review (SLR). We then observed that some of the claims are qualitative, lacking quantitative evidence. We identify quantifiable metrics associated with those claims, and measure in order to substantiate these claims. Results: We identify 12 claims about PTM reuse on the HuggingFace platform, 4 of which lack quantitative support. We tested 3 of these claims through a quantitative analysis, and directly compare the fourth with traditional software. Our most notable findings are: (1) PTMs have a significantly higher turnover rate than traditional software, indicating more rapid evolution; and (2) There is a strong correlation between documentation quality and PTM popularity. Conclusions: Our findings validate several qualitative research claims with concrete metrics, confirming prior research. Our measures motivate further research on the dynamics of PTM reuse.},
   author = {Jason Jones and Wenxin Jiang and Nicholas Synovic and George Thiruvathukal and James Davis},
   city = {New York, NY, USA},
   doi = {10.1145/3674805.3686665},
   isbn = {9798400710476},
   booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
   keywords = {Empirical Software Engineering,Literature Review,Pre-Trained Models,Software Package Registries,Software Supply Chain},
   pages = {13-24},
   publisher = {Association for Computing Machinery},
   title = {What do we know about Hugging Face? A systematic literature review and quantitative validation of qualitative claims},
   url = {https://doi.org/10.1145/3674805.3686665},
   year = {2024}
}
@inproceedings{Qi2024,
   abstract = {With the development of software testing technology, Large Language Model (LLM) driven testing method have gradually become an emerging trend in the field of software testing. This paper presents a comprehensive review of LLM-based testing techniques. The results of 19 studies using LLM to optimize testing techniques are analyzed from the perspective of software testing. This paper discusses in detail how to use LLM to optimize test techniques for generating automated test code and generating diverse input in software test tasks. It also summarizes the challenges and opportunities faced by this field. The above conclusions can identify the shortcomings of LLM-based software testing technology and the direction of future research.},
   author = {Fei Qi and Yingnan Hou and Ning Lin and Shanshan Bao and Nuo Xu},
   city = {New York, NY, USA},
   doi = {10.1145/3675249.3675298},
   isbn = {9798400718267},
   booktitle = {Proceedings of the 2024 International Conference on Computer and Multimedia Technology},
   keywords = {LLM,Pre-trained Large Language Model,Software Testing Techniques},
   pages = {280-284},
   publisher = {Association for Computing Machinery},
   title = {A Survey of Testing Techniques Based on Large Language Models},
   url = {https://doi.org/10.1145/3675249.3675298},
   year = {2024}
}
@article{Xie2024,
   abstract = {With the wide application of machine translation, the testing of Machine Translation Systems (MTSs) has attracted much attention. Recent works apply Metamorphic Testing (MT) to address the oracle problem in MTS testing. Existing MT methods for MTS generally follow the workflow of input transformation and output relation comparison, which generates a follow-up input sentence by mutating the source input and compares the source and follow-up output translations to detect translation errors, respectively. These methods use various input transformations to generate the test case pairs and have successfully triggered numerous translation errors. However, they have limitations in performing fine-grained and rigorous output relation comparison and thus may report many false alarms and miss many true errors. In this article, we propose a word closure-based output comparison method to address the limitations of the existing MTS MT methods. We first propose word closure as a new comparison unit, where each closure includes a group of correlated input and output words in the test case pair. Word closures suggest the linkages between the appropriate fragment in the source output translation and its counterpart in the follow-up output for comparison. Next, we compare the semantics on the level of word closure to identify the translation errors. In this way, we perform a fine-grained and rigorous semantic comparison for the outputs and thus realize more effective violation identification. We evaluate our method with the test cases generated by five existing input transformations and the translation outputs from three popular MTSs. Results show that our method significantly outperforms the existing works in violation identification by improving the precision and recall and achieving an average increase of 29.9\% in F1 score. It also helps to increase the F1 score of translation error localization by 35.9\%.},
   author = {Xiaoyuan Xie and Shuo Jin and Songqiang Chen and Shing-Chi Cheung},
   city = {New York, NY, USA},
   doi = {10.1145/3675396},
   issn = {1049-331X},
   issue = {8},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Machine translation,deep learning testing,metamorphic testing,word closure},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {Word Closure-Based Metamorphic Testing for Machine Translation},
   volume = {33},
   url = {https://doi.org/10.1145/3675396},
   year = {2024}
}
@inproceedings{Lachaux2024,
   abstract = {The integration of socially assistive robots (SARs) in healthcare has the potential to revolutionize physical health assessments by providing standardized, reliable, and engaging methods for evaluating functional mobility and muscle strength. This paper presents the development and testing of a SAR system designed specifically for standardized physical assessments in patients with rare diseases with mobility-related impairements such as ataxias and muscular dystrophies. Utilizing the TEMI robot, our study focused on three standardized tests: the 30-Second Chair Stand Test, the 10-Meter Walk Test, and the Grip Strength Test. Preliminary results from lab experiments with healthy subjects indicate strong correlations between manual and robotic measurements, particularly for knee angles and walk times, demonstrating the system’s accuracy and consistency. However, challenges were still noted, like the need for more interactive procedures and clearer instructions. These findings highlight both the potential and the hurdles in deploying SARs for physical assessments. This research contributes to the broader field of health informatics and robotic-assisted interventions, offering insights into the design, development, and testing of SAR systems for clinical use, setting the stage for their eventual integration into clinical practice.},
   author = {Killian Lachaux and Élodie Gagnon and Florentin Thullier and Claudia Maltais and Julien Maitre and Kévin Bouchard and Cynthia Gagnon and Élise Duchesne and Sébastien Gaboury},
   city = {New York, NY, USA},
   doi = {10.1145/3677525.3678657},
   isbn = {9798400710940},
   booktitle = {Proceedings of the 2024 International Conference on Information Technology for Social Good},
   keywords = {Autonomous Health Monitoring Systems,Rare Diseases,Robot-Assisted Health Evaluation,Socially Assistive Robots (SARs),Standardized Physical Assessments},
   pages = {163-171},
   publisher = {Association for Computing Machinery},
   title = {Preparing for the Clinical Stage : Lab Development and Testing of Socially Assistive Robot for Physical Health Assessments},
   url = {https://doi.org/10.1145/3677525.3678657},
   year = {2024}
}
@inproceedings{Garcia2024,
   abstract = {Automated testing is crucial in software development to ensure that applications perform as intended. However, generating automated End-to-End (E2E) tests can be time-consuming and challenging, especially for junior developers. This study investigates the use of ChatGPT, a popular Generative Artificial Intelligence (GenAI) model, as an assistant in developing automated E2E test scripts for Android apps. We present an empirical study that compares the effort required to create E2E test scripts and the resulting reliability of these tests using two treatments: manually and assisted by ChatGPT. We used Gherkin, a domain-specific language that allows non-technical practitioners to define test scenarios using a human-readable syntax. Our findings indicate that using ChatGPT significantly reduces the time required to develop automated test scripts without compromising the reliability of the scripts. Statistical analysis shows a notable reduction in development time for the ChatGPT-assisted group compared to the manual group, with a large effect size. While the reliability of the tests did not show a significant difference between the two groups, the results suggest practical benefits in terms of efficiency.},
   author = {Boni Garc\'\{\i\}a and Maurizio Leotta and Filippo Ricca and Jim Whitehead},
   city = {New York, NY, USA},
   doi = {10.1145/3678719.3685691},
   isbn = {9798400711091},
   booktitle = {Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation},
   keywords = {Android,E2E Automated Testing,Empirical Study,GenAI},
   pages = {5-11},
   publisher = {Association for Computing Machinery},
   title = {Use of ChatGPT as an Assistant in the End-to-End Test Script Generation for Android Apps},
   url = {https://doi.org/10.1145/3678719.3685691},
   year = {2024}
}
@inproceedings{Bergsmann2024,
   abstract = {Gherkin is a domain-specific language for describing test scenarios in natural language, which are the basis for automated acceptance testing. The emergence of Large Language Models (LLMs) has opened up new possibilities for processing such test specifications and for generating executable test code. This paper investigates the feasibility of employing LLMs to execute Gherkin test specifications utilizing the AutoGen multi-agent framework. Our findings show that our LLM agent system is able to automatically run the given test scenarios by autonomously exploring the system under test, generating executable test code on the fly, and evaluating execution results. We observed high success rates for executing simple as well as more complex test scenarios, but we also identified difficulties regarding failure scenarios and fault detection.},
   author = {Severin Bergsmann and Alexander Schmidt and Stefan Fischer and Rudolf Ramler},
   city = {New York, NY, USA},
   doi = {10.1145/3678719.3685692},
   isbn = {9798400711091},
   booktitle = {Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation},
   keywords = {Domain Specific Language,LLMs,Test Automation},
   pages = {12-15},
   publisher = {Association for Computing Machinery},
   title = {First Experiments on Automated Execution of Gherkin Test Specifications with Collaborating LLM Agents},
   url = {https://doi.org/10.1145/3678719.3685692},
   year = {2024}
}
@inproceedings{Grassini2024,
   abstract = {The present article introduces and implements an initial validation for the Perceived Artificial Intelligence Literacy Questionnaire (PAILQ-6), a brief tool designed to assess individuals' self-perceived AI literacy. Amidst the growing integration of AI in various aspects of life and its ethical implications, understanding AI becomes crucial for effective interaction with AI technologies. The PAILQ-6 emerges in response to the need for an accessible instrument that evaluates general AI literacy without compromising on clarity or depth, suitable for both academic and practical applications. This paper presents the development process of the PAILQ-6, consisting of six items derived from established components of AI literacy, structured as a seven-point Likert scale for easy administration and digital compatibility. The validation study was conducted from data of a gender-balanced sample of 232 UK adults. The article demonstrates the PAILQ-6's reliability and validity through exploratory factor analysis, showing a two-factor structure. The findings reveal the scale's good internal consistency and convergent validity. The study highlights demographic predictors of AI literacy perceptions, indicating a possible gender disparity and the positive influence of higher education on perceived AI competency.},
   author = {Simone Grassini},
   city = {New York, NY, USA},
   doi = {10.1145/3679318.3685359},
   isbn = {9798400709661},
   booktitle = {Proceedings of the 13th Nordic Conference on Human-Computer Interaction},
   keywords = {Artificial Intelligence,Literacy,Psychology,Questionnaire},
   publisher = {Association for Computing Machinery},
   title = {A Psychometric Validation of the PAILQ-6: Perceived Artificial Intelligence Literacy Questionnaire},
   url = {https://doi.org/10.1145/3679318.3685359},
   year = {2024}
}
@article{Openja2024,
   abstract = {Background: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively). Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies. Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow. Method: We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems. Results: Our findings reveal several key insights: 1.) The most common testing strategies, accounting for less than 40\%, are Grey-box and White-box methods, such as Negative Testing, Oracle Approximation, and Statistical Testing. 2.) A wide range of  (17)  ML properties are tested, out of which only 20\% to 30\% are frequently tested, including Consistency, Correctness, and Efficiency. 3.) Bias and Fairness is more tested in Recommendation (6\%) and CV (3.9\%) systems, while Security and Privacy is tested in CV (2\%), Application Platforms (0.9\%), and NLP (0.5\%). 4.) We identified 13 types of testing methods, such as Unit Testing, Input Testing, and Model Testing. Conclusions: This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.},
   author = {Moses Openja and Foutse Khomh and Armstrong Foundjem and Zhen Ming (Jack) Jiang and Mouna Abidi and Ahmed E Hassan},
   city = {New York, NY, USA},
   doi = {10.1145/3680463},
   issn = {1049-331X},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Deep learning,ML properties,Machine learning,Machine learning workflow,Software Testing,Test types/ Types of testing,Testing methods,Testing strategies},
   month = {7},
   note = {Just Accepted},
   publisher = {Association for Computing Machinery},
   title = {An empirical study of testing machine learning in the wild},
   url = {https://doi.org/10.1145/3680463},
   year = {2024}
}
@inproceedings{Qiu2024,
   abstract = {Theoretically the performance of AI chips large model training performance mainly depends on the specification parameters of the chip and quantity, in practice, the performance of inference cards is also related to the inference optimization technology of the chip. In response to this issue of AI chips large model training performance, we conducted researches on the specification information of current mainstream chip manufacturers. On the basis of studying existing evaluation indicators, a performance measurement scheme was proposed by using MFU as an important performance indicator of AI chips large model training performance, and five products were tested. The experimental results indicate that this scheme can effectively quantitatively evaluate the training performance of AI chips.},
   author = {Hongfei Qiu and Pengcheng Jiang and Zhiqin Huang},
   city = {New York, NY, USA},
   doi = {10.1145/3687311.3687405},
   isbn = {9798400709920},
   booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
   pages = {525-529},
   publisher = {Association for Computing Machinery},
   title = {Research on AI Chip Large Model Train Testing Technology},
   url = {https://doi.org/10.1145/3687311.3687405},
   year = {2024}
}

@article{Li2024-1,
   abstract = {Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations — coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations.    To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth.    Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7\% to 59.8\%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.    To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.},
   author = {Ningke Li and Yuekang Li and Yi Liu and Ling Shi and Kailong Wang and Haoyu Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3689776},
   issue = {OOPSLA2},
   journal = {Proc. ACM Program. Lang.},
   keywords = {Hallucination,Large Language Model,Software Testing},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models},
   volume = {8},
   url = {https://doi.org/10.1145/3689776},
   year = {2024}
}
@inproceedings{Tang2024,
   abstract = {Handling long tail corner cases is a major challenge faced by autonomous vehicles (AVs). While large language models (LLMs) hold great potentials to handle the corner cases with excellent generalization and explanation capabilities and received increasing research interest on application to autonomous driving, there are still technical barriers to be tackled, such as strict model performance and huge computing resource requirements of LLMs, which are difficult to be met locally at AVs. In this paper, we investigate a new approach of applying remote or edge LLMs to support autonomous driving. With this approach connected autonomous vehicles (CAVs) send driving assistance requests to the LLMs. LLMs deployed at the edge of the networks or remote clouds process the requests and generate driving assistance instructions for the CAVs. A key issue for such LLM assisted driving system is the assessment of LLMs on their understanding of driving theory and skills, ensuring they are qualified to undertake safety critical driving assistance tasks for CAVs. As there is no published work on assessing LLM of driving theory and skills, we design and run driving theory tests for several proprietary LLM models (OpenAI GPT models, Baidu Ernie and Ali QWen) and open-source LLM models (Tsinghua MiniCPM-2B and MiniCPM-Llama3-V2.5) with more than 500 multiple-choices theory test questions. These questions are close to the official UK driving theory test ones. Model accuracy, cost and processing latency are measured from the experiments. Experiment results show that while model GPT-4 passes the test with improved domain knowledge and Ernie has an accuracy of 85\% (just below the 86\% passing threshold), other LLM models including GPT-3.5 fail the test. For the test questions with images, the multimodal model GPT4-o has an excellent accuracy result of 96\%, and the MiniCPM-Llama3-V2.5 achieves an accuracy of 76\%. While GPT-4 holds stronger potential for CAV driving assistance applications, the cost of using model GPT4 is much higher, almost 50 times of that of using GPT3.5. The results can help make decision on the use of the existing LLMs for CAV applications and balancing on the model performance and cost.},
   author = {Zuoyin Tang and Jianhua He and Dashuai Pe and Kezhong Liu and Tao Gao and Jiawei Zheng},
   city = {New York, NY, USA},
   doi = {10.1145/3691555.3696825},
   isbn = {9798400712470},
   booktitle = {Proceedings of the 19th Workshop on Mobility in the Evolving Internet Architecture},
   keywords = {Connected autonomous vehicles,driving theory test,large language model,mobile cloud computing,mobile edge computing,remote driving},
   pages = {1-6},
   publisher = {Association for Computing Machinery},
   title = {Test Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles},
   url = {https://doi.org/10.1145/3691555.3696825},
   year = {2024}
}
@inproceedings{Guo2024,
   abstract = {Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications. However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved. Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving. Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds. However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction. Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges.In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports. SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model (LLM) in extracting accident information from textual data. Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories. Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs. We experiment with SoVAR, using the accident reports from the National Highway Traffic Safety Administration's (NHTSA) database to generate test scenarios for the industrial-grade ADS Apollo. The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures. Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo.},
   author = {An Guo and Yuan Zhou and Haoxiang Tian and Chunrong Fang and Yunjian Sun and Weisong Sun and Xinyu Gao and Anh Tuan Luu and Yang Liu and Zhenyu Chen},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695037},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {automatic test generation,autonomous driving system,constraint solving,software testing},
   pages = {268-280},
   publisher = {Association for Computing Machinery},
   title = {SoVAR: Build Generalizable Scenarios from Accident Reports for Autonomous Driving Testing},
   url = {https://doi.org/10.1145/3691620.3695037},
   year = {2024}
}
@inproceedings{Yang2024-2,
   abstract = {Rust is a relatively new programming language known for its memory safety and numerous advanced features. It has been widely used in system software in recent years. Thus, ensuring the reliability and robustness of the only implementation of the Rust compiler, rustc, is critical. However, compiler testing, as one of the most effective techniques to detect bugs, faces difficulties in generating valid Rust programs with sufficient diversity due to its stringent memory safety mechanisms. Furthermore, existing research primarily focuses on testing rustc to trigger crash errors, neglecting incorrect compilation results - miscompilation. Detecting miscompilation remains a challenge in the absence of multiple implementations of the Rust compiler to serve as a test oracle.This paper tackles these challenges by introducing rust-twins, a novel and effective approach to performing automated differential testing for rustc to detect both crashes and miscompilations. We devise four Rust-specific mutators and adapt fourteen general mutators for Rust, each intends to produce a syntax and semantic valid Rust program to trigger rustc crashes. Additionally, we develop a macroize approach to rewrite a regular Rust program into dual macros with equivalent behaviors but in different implementations. Furthermore, we design an assessment component to check the equivalence by comparing the expansion results with a simple macro input. Finally, rust-twins attempts to expand the two macros with numerous complex inputs to detect differences. Due to the macro expansion mechanism, the root causes of differences can arise not only from the macro expansion part but also from any other mis-implemented compiler code.We have evaluated rust-twins on the latest version of rustc. Our experimental results indicate that rust-twins achieves twice the total line coverage and identifies more crashes and differences than the best baseline technique, rustsmith, after 24 hours of testing. In total, rust-twins triggered 10 rustc crashes, and 229 of the generated macros exposed rustc differences. We analyzed and reported 12 previously unknown bugs, of which 8 have already been confirmed and fixed.},
   author = {Wenzhang Yang and Cuifeng Gao and Xiaoyuan Liu and Yuekang Li and Yinxing Xue},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695059},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {compiler testing,differential testing,rust},
   pages = {631-642},
   publisher = {Association for Computing Machinery},
   title = {Rust-twins: Automatic Rust Compiler Testing through Program Mutation and Dual Macros Generation},
   url = {https://doi.org/10.1145/3691620.3695059},
   year = {2024}
}
@inproceedings{Xu2024-1,
   abstract = {Machine translation has become an integral part of daily life, with terminology translation playing a crucial role in ensuring the accuracy of translation results. However, existing translation systems, such as Google Translate, have been shown to occasionally produce errors in terminology translation. Current metrics for assessing terminology translation rely on reference translations and bilingual dictionaries, limiting their effectiveness in large-scale automated MT system testing.To address this challenge, we propose a novel method: Metamorphic Testing for Terminology Translation (TermMT), which achieves effective and efficient testing for terminology translation in MT systems without relying on reference translations or bilingual terminology dictionaries. Our approach involves constructing metamorphic relations based on the characteristics of terms: (a) adding an appropriate reference of the term in the given context would not change the translation of the term; (b) if we modify part of a multi-word term, the translation of the revised word combination would change. To evaluate the effectiveness of TermMT, we tested the terminology translation capabilities of three machine translation systems, Google Translate, Bing Microsoft Translator, and mBART, using the English portion of the bilingual UM-corpus dataset. The results show that TermMT detected a total of 3,765 translation errors on Google Translate, 2,351 on Bing Microsoft Translator, and 6,011 on mBART, with precisions of 82.33\%, 83.00\%, and 86.33\%, respectively.},
   author = {Yihui Xu and Yanhui Li and Jun Wang and Xiaofang Zhang},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695069},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {machine translation,metamorphic testing,software testing,terminology translation},
   pages = {758-769},
   publisher = {Association for Computing Machinery},
   title = {Evaluating Terminology Translation in Machine Translation Systems via Metamorphic Testing},
   url = {https://doi.org/10.1145/3691620.3695069},
   year = {2024}
}
@inproceedings{Zhang2024,
   abstract = {Automated web GUI testing has been widely adopted since manual testing is time-consuming and tedious. Waiting strategy plays a vital role in automated web GUI testing since it significantly impacts the testing performance. Though important, little focus has been set on the waiting strategies in web GUI testing. Existing waiting strategies either wait for a predetermined time, which is not reliable in a dynamic environment, or only wait for a specific condition to be verified, which is often not robust enough to handle the complicated testing scenarios. In this work, we introduce a robust waiting strategy. Instead of waiting for a predetermined time or waiting for the availability of a particular element, our approach waits for a desired state to reach. This is achieved by capturing the Document Object Models (DOM) at the desired point, followed by an offline analysis to identify the differences between the DOMs associated with every two consecutive test actions. Such differences are used to determine the appropriate waiting time when automatically generating tests. Evaluation results with an industrial web application indicate that our approach produces more robust tests than the conventional waiting strategies used in web GUI testing. Furthermore, our generated tests are more representative of the recorded usage scenarios and are efficient with low overhead in test execution time.},
   author = {Haonan Zhang and Lizhi Liao and Zishuo Ding and Weiyi Shang and Nidhi Narula and Catalin Sporea and Andrei Toma and Sarah Sajedi},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695269},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {GUI rendering,automated web GUI testing,industrial experience report,waiting strategy},
   pages = {2065-2076},
   publisher = {Association for Computing Machinery},
   title = {Towards a Robust Waiting Strategy for Web GUI Testing for an Industrial Software System},
   url = {https://doi.org/10.1145/3691620.3695269},
   year = {2024}
}
@inproceedings{Ouedraogo2024,
   abstract = {Unit testing, essential for identifying bugs, is often neglected due to time constraints. Automated test generation tools exist but typically lack readability and require developer intervention. Large Language Models (LLMs) like GPT and Mistral show potential in test generation, but their effectiveness remains unclear.This study evaluates four LLMs and five prompt engineering techniques, analyzing 216 300 tests for 690 Java classes from diverse datasets. We assess correctness, readability, coverage, and bug detection, comparing LLM-generated tests to EvoSuite. While LLMs show promise, improvements in correctness are needed. The study highlights both the strengths and limitations of LLMs, offering insights for future research.},
   author = {Wendkuuni C Ouedraogo and Kader Kabore and Haoye Tian and Yewei Song and Anil Koyuncu and Jacques Klein and David Lo and Tegawende F Bissyande},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695330},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {automatic test generation,empirical evaluation,large language models,prompt engineering,unit tests},
   pages = {2464-2465},
   publisher = {Association for Computing Machinery},
   title = {LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation},
   url = {https://doi.org/10.1145/3691620.3695330},
   year = {2024}
}
@inproceedings{Huynh2024,
   abstract = {Testing Representational State Transfer (REST) APIs is crucial for ensuring the reliability and performance of APIs, which are essential to modern web services. This testing process helps identify and resolve issues related to data exchange and integration with other systems. Among the various API testing techniques, black-box testing relies on the OpenAPI Specification (OAS) to generate test cases and data. However, current API test automation methods are primarily focused on status code [10] and schema validation [1]. Status code validation involves ensuring that each HTTP request returns a response with a status code, a three-digit integer that indicates the outcome of the request. Schema validation verifies the correctness of the response data by comparing it to the schema. This includes checking that all required properties are present and that data types of these properties align with the schema specified.},
   author = {Hieu Huynh and Quoc-Tri Le and Tien N Nguyen and Vu Nguyen},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695341},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {API testing,large language models},
   pages = {2486-2487},
   publisher = {Association for Computing Machinery},
   title = {Using LLM for Mining and Testing Constraints in API Testing},
   url = {https://doi.org/10.1145/3691620.3695341},
   year = {2024}
}
@inproceedings{Kumar2024,
   abstract = {Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs. Demo Video: https://youtu.be/aqF_agNP-lU},
   author = {Atul Kumar and Diptikalyan Saha and Toshiaki Yasue and Kohichi Ono and Saravanan Krishnan and Sandeep Hans and Fumiko Satoh and Gerald Mitchell and Sachin Kumar},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695365},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {COBOL to Java,automatic validation,external resource testing},
   pages = {2415-2418},
   publisher = {Association for Computing Machinery},
   title = {Automated Validation of COBOL to Java Transformation},
   url = {https://doi.org/10.1145/3691620.3695365},
   year = {2024}
}
@inproceedings{Zhao2024,
   abstract = {Mutation testing is widely used to measure the test adequacy of a project. Despite its popularity, mutation testing is time-consuming and extremely expensive. To mitigate this problem, researchers propose Predictive Mutation Testing (PMT). Existing PMT approaches build classification models based on statistical program features or source code of programs to predict mutation testing results. Previous statistical feature-based PMT models need expensive overhead to collect dynamic features and neglect the rich information inherent in code text. Previous text-based PMT models extract essential code elements as input and outperform the feature-based models. However, they encode code text in a plain way. Therefore, they cannot sensitively capture subtle differences in mutants and they have difficulty in capturing the correlation between mutants and tests. To address these challenges, we propose a new model, SODA. SODA uses a new learning strategy, Mutational Semantic Learning, to make our model spot code mutation and its impact on test behavior. In particular, we employ a new sampling strategy to reinforce the corresponding relationship between mutants and tests by sampling same-mutant contrastive groups. Then we employ contrastive learning to make our model capture subtle differences in mutants. We conduct experiments to investigate the performance of SODA. The results demonstrate that both in the cross-project and cross-version scenarios, SODA achieves state-of-the-art classification performance (improves upon baselines by 5.32\%-114.92\% in kill-F1 score, 0.04\%-25.54\% in survive-F1 score, 4.25\%-60.43\% in accuracy) and has the lowest mutation score error.},
   author = {Yifan Zhao and Yizhou Chen and Zeyu Sun and Qingyuan Liang and Guoqing Wang and Dan Hao},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695491},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {contrastive learning,mutation testing},
   pages = {1133-1145},
   publisher = {Association for Computing Machinery},
   title = {Spotting Code Mutation for Predictive Mutation Testing},
   url = {https://doi.org/10.1145/3691620.3695491},
   year = {2024}
}
@inproceedings{Corradini2024,
   abstract = {Automatically crafting test scenarios for REST APIs helps deliver more reliable and trustworthy web-oriented systems. However, current black-box testing approaches rely heavily on the information available in the API's formal documentation, i.e., the Open API Specification (OAS for short). While useful, the OAS mostly covers syntactic aspects of the API (e.g., producer-consumer relations between operations, input value properties, and additional constraints in natural language), and it lacks a deeper understanding of the API business logic. Missing semantics include implicit ordering (logic dependency) between operations and implicit input-value constraints. These limitations hinder the ability of black-box testing tools to generate truly effective test cases automatically.This paper introduces DeepREST, a novel black-box approach for automatically testing REST APIs. It leverages deep reinforcement learning to uncover implicit API constraints, that is, constraints hidden from API documentation. Curiosity-driven learning guides an agent in the exploration of the API and learns an effective order to test its operations. This helps identify which operations to test first to take the API in a testable state and avoid failing API interactions later. At the same time, experience gained on successful API interactions is leveraged to drive accurate input data generation (i.e., what parameters to use and how to pick their values). Additionally, DeepREST alternates exploration with exploitation by mutating successful API interactions to improve test coverage and collect further experience.Our empirical validation suggests that the proposed approach is very effective in achieving high test coverage and fault detection and superior to a state-of-the-art baseline.},
   author = {Davide Corradini and Zeno Montolli and Michele Pasqua and Mariano Ceccato},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695511},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {REST API testing,automated blackbox testing,deep reinforcement learning},
   pages = {1383-1394},
   publisher = {Association for Computing Machinery},
   title = {DeepREST: Automated Test Case Generation for REST APIs Exploiting Deep Reinforcement Learning},
   url = {https://doi.org/10.1145/3691620.3695511},
   year = {2024}
}
@inproceedings{Jiang2024-1,
   abstract = {Automatic testing has garnered significant attention and success over the past few decades. Techniques such as unit testing and coverage-guided fuzzing have revealed numerous critical software bugs and vulnerabilities. However, a long-standing, formidable challenge for existing techniques is how to achieve higher testing coverage. Constraint-based techniques, such as symbolic execution and concolic testing, have been well-explored and integrated into the existing approaches. With the popularity of Large Language Models (LLMs), recent research efforts to design tailored prompts to generate inputs that can reach more uncovered target branches. However, the effectiveness of using LLMs for generating such directed inputs and the comparison with the proven constraint-based solutions has not been systematically explored.To bridge this gap, we conduct the first systematic study on the mainstream LLMs and constraint-based tools for directed input generation with a comparative perspective. We find that LLMs such as ChatGPT are comparable to or even better than the constraint-based tools, succeeding in 43.40\%-58.57\% samples in our dataset. Meanwhile, there are also limitations for LLMs in specific scenarios such as sequential calculation, where constraint-based tools are in a position of strength. Based on these findings, we propose a simple yet effective method to combine these two types of tools and implement a prototype based on ChatGPT and constraint-based tools. Our evaluation shows that our approach can outperform the baselines by 1.4x to 2.3x relatively. We believe our study can provide novel insights into directed input generation using LLMs, and our findings are essential for future testing research.},
   author = {Zongze Jiang and Ming Wen and Jialun Cao and Xuanhua Shi and Hai Jin},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695513},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {LLM,directed input generation,symbolic execution},
   pages = {1408-1420},
   publisher = {Association for Computing Machinery},
   title = {Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation},
   url = {https://doi.org/10.1145/3691620.3695513},
   year = {2024}
}
@inproceedings{Mathews2024,
   abstract = {Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.},
   author = {Noble Saji Mathews and Meiyappan Nagappan},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695527},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {LLM,TDD,code generation,software engineering,testing},
   pages = {1583-1594},
   publisher = {Association for Computing Machinery},
   title = {Test-Driven Development and LLM-based Code Generation},
   url = {https://doi.org/10.1145/3691620.3695527},
   year = {2024}
}
@inproceedings{Yang2024-3,
   abstract = {Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.},
   author = {Lin Yang and Chen Yang and Shutao Gao and Weijing Wang and Bo Wang and Qihao Zhu and Xiao Chu and Jianyi Zhou and Guangtai Liang and Qianxiang Wang and Junjie Chen},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3695529},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {empirical study,large language model,unit test generation},
   pages = {1607-1619},
   publisher = {Association for Computing Machinery},
   title = {On the Evaluation of Large Language Models in Unit Test Generation},
   url = {https://doi.org/10.1145/3691620.3695529},
   year = {2024}
}
@inproceedings{Xu2024-2,
   abstract = {While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70\% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR-irrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00\% of encoded MRs, which is 33.33\% more than using vanilla GPT-3.5. By incorporating MR-Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62\% and 18.91\%, respectively.},
   author = {Congying Xu and Songqiang Chen and Jiarong Wu and Shing-Chi Cheung and Valerio Terragni and Hengcheng Zhu and Jialun Cao},
   city = {New York, NY, USA},
   doi = {10.1145/3691620.3696020},
   isbn = {9798400712487},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {code generation,input transformation,large language models,metamorphic relation,metamorphic testing,software testing},
   pages = {557-569},
   publisher = {Association for Computing Machinery},
   title = {MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing},
   url = {https://doi.org/10.1145/3691620.3696020},
   year = {2024}
}
@inproceedings{Wadhams2024,
   abstract = {Developers face a challenging problem with no clear solution. Modern software breaches can wreak havoc on businesses and individuals alike. With code vulnerabilities being a leading cause, securing applications must be a priority for developers. Static Application Security Testing (SAST) has the potential to harden applications by assisting in the identification and resolution of security vulnerabilities. Despite this, many development teams have not adopted SAST tools into their environment. In this paper, we survey the recent literature to uncover why some developers are apprehensive towards SAST and identify what specific problems they encounter when using it. We found a variety of usability problems developers face when using SAST. Some are inherent of the tool and ultimately require some level of developer investment while others are tool shortcomings that SAST tool creators must address. Ultimately, we argue that in order to drive widespread adoption and consistent SAST usage, developers will need to embrace that some investment is required. Simultaneously, developers will be more likely to integrate SAST tools into their workflows if the creators of SAST tools simplify many aspects related to tool usage. Surmounting the primary obstacles preventing the adoption of SAST requires full consideration of both the technical and human factors.},
   author = {Zachary Douglas Wadhams and Clemente Izurieta and Ann Marie Reinhold},
   city = {New York, NY, USA},
   doi = {10.1145/3691621.3694947},
   isbn = {9798400712494},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering Workshops},
   keywords = {barriers,developers,literature review,sast,static application security testing,usability},
   pages = {161-166},
   publisher = {Association for Computing Machinery},
   title = {Barriers to Using Static Application Security Testing (SAST) Tools: A Literature Review},
   url = {https://doi.org/10.1145/3691621.3694947},
   year = {2024}
}
@inproceedings{Qin2024,
   abstract = {As the virtual reality (VR) industry expands, the need for automated GUI testing for applications is growing rapidly. With its long-term memory and ability to process mixed data, including images and text, Generative AI (GenAI) shows the potential to understand complex user interfaces. In this paper, we conduct a case study to investigate the potential of using GenAI for field of view (FOV) analysis in VR exploration testing. Specifically, we examine how the model can assist in test entity selection and test action suggestions. Our experiments demonstrate that while GPT-4o achieves a 63\% accuracy rate in object identification within an arbitrary FOV, it struggles with object organization and localization. We also identify critical contexts that can improve the accuracy of suggested actions across multiple FOVs. Finally, we discuss the limitations found during the experiment and offer insights into future research directions.},
   author = {Xue Qin and Garrett Weaver},
   city = {New York, NY, USA},
   doi = {10.1145/3691621.3694955},
   isbn = {9798400712494},
   booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering Workshops},
   keywords = {GUI exploration testing,case study,virtual reality},
   pages = {228-232},
   publisher = {Association for Computing Machinery},
   title = {Utilizing Generative AI for VR Exploration Testing: A Case Study},
   url = {https://doi.org/10.1145/3691621.3694955},
   year = {2024}
}
@article{Ji2024,
   abstract = {With the development of Deep Learning, Natural Language Processing (NLP) applications have reached or even exceeded human-level capabilities in certain tasks. Although NLP applications have shown good performance, they can still have bugs like traditional software and even lead to serious consequences. Inspired by Lego blocks and syntax structure analysis, we propose an assembling test generation method for NLP applications or models and implement it in NLPLego. The key idea of NLPLego is to assemble the sentence skeleton and adjuncts in order by simulating the building of Lego blocks to generate multiple grammatically and semantically correct sentences based on one seed sentence. The sentences generated by NLPLego have derivation relations and different degrees of variation. These characteristics make it well-suited for integration with metamorphic testing theory, addressing the challenge of test oracle absence in NLP application testing. To validate NLPLego, we conduct experiments on three commonly used NLP tasks (i.e., machine reading comprehension, sentiment analysis, and semantic similarity measures), focusing on the efficiency of test generation and the quality and effectiveness of generated tests. We select five advanced NLP models and one popular industrial NLP software as the tested subjects. Given seed tests from SQuAD 2.0, SST, and QQP, NLPLego successfully detects 1,732, 3,140, and 261,879 incorrect behaviors with around 93.1\% precision in three tasks, respectively. The experiment results show that NLPLego can efficiently generate high-quality tests for multiple NLP tasks to detect erroneous behaviors effectively. In the case study, we analyze the testing results provided by NLPLego to obtain intuitive representations of the different NLP capabilities of the tested subjects. The case study confirms that NLPLego can provide developers with clarity on the direction to improve NLP models or applications, laying the foundation for enhancing performance.},
   author = {Pin Ji and Yang Feng and Ruohao Zhang and Ruichen Xue and Yichi Zhang and Weitao Huang and Jia Liu and Zhihong Zhao},
   city = {New York, NY, USA},
   doi = {10.1145/3691631},
   issn = {1049-331X},
   journal = {ACM Trans. Softw. Eng. Methodol.},
   keywords = {Automated Testing,Metamorphic Testing,Natural Language Processing,Test Generation},
   month = {10},
   note = {Just Accepted},
   publisher = {Association for Computing Machinery},
   title = {NLPLego: Assembling Test Generation for Natural Language Processing Applications},
   url = {https://doi.org/10.1145/3691631},
   year = {2024}
}
@inproceedings{Lattuada2024,
   abstract = {Formal verification is a promising approach to eliminate bugs at compile time, before they ship. Indeed, our community has verified a wide variety of system software. However, much of this success has required heroic developer effort, relied on bespoke logics for individual domains, or sacrificed expressiveness for powerful proof automation.Building on prior work on Verus, we aim to enable faster, cheaper verification of rich properties for realistic systems. We do so by integrating and optimizing the best choices from prior systems, tuning our design to overcome barriers encountered in those systems, and introducing novel techniques.We evaluate Verus's effectiveness with a wide variety of case-study systems, including distributed systems, an OS page table, a library for NUMA-aware concurrent data structure replication, a crash-safe storage system, and a concurrent memory allocator, together comprising 6.1K lines of implementation and 31K lines of proof. Verus verifies code 3–61 faster and with less effort than the state of the art.Our results suggest that Verus offers a platform for exploring the next frontiers in system-verification research. Because Verus builds on Rust, Verus is also positioned for wider use in production by developers who have already adopted Rust in the pursuit of more robust systems.},
   author = {Andrea Lattuada and Travis Hance and Jay Bosamiya and Matthias Brun and Chanhee Cho and Hayley LeBlanc and Pranav Srinivasan and Reto Achermann and Tej Chajed and Chris Hawblitzel and Jon Howell and Jacob R Lorch and Oded Padon and Bryan Parno},
   city = {New York, NY, USA},
   doi = {10.1145/3694715.3695952},
   isbn = {9798400712517},
   booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
   pages = {438-454},
   publisher = {Association for Computing Machinery},
   title = {Verus: A Practical Foundation for Systems Verification},
   url = {https://doi.org/10.1145/3694715.3695952},
   year = {2024}
}
@inproceedings{Chen2022,
   abstract = {Question Answering (QA) is an attractive and challenging area in NLP community. There are diverse algorithms being proposed and various benchmark datasets with different topics and task formats being constructed. QA software has also been widely used in daily human life now. However, current QA software is mainly tested in a reference-based paradigm, in which the expected outputs (labels) of test cases need to be annotated with much human effort before testing. As a result, neither the just-in-time test during usage nor the extensible test on massive unlabeled real-life data is feasible, which keeps the current testing of QA software from being flexible and sufficient. In this paper, we propose a method, QAAskeR, with three novel Metamorphic Relations for testing QA software. qaAskeR does not require the annotated labels but tests QA software by checking its behaviors on multiple recursively asked questions that are related to the same knowledge. Experimental results show that qaAskeR can reveal violations at over 80\% of valid cases without using any pre-annotated labels. Diverse answering issues, especially the limited generalization on question types across datasets, are revealed on a state-of-the-art QA algorithm.},
   author = {Songqiang Chen and Shuo Jin and Xiaoyuan Xie},
   doi = {10.1109/ASE51524.2021.9678670},
   isbn = {9781665403375},
   booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {natural language processing,question answering,recursive metamorphic testing,testing and validation},
   pages = {104-116},
   publisher = {IEEE Press},
   title = {Testing your question answering software via asking recursively},
   url = {https://doi.org/10.1109/ASE51524.2021.9678670},
   year = {2022}
}
@inproceedings{Morales2024,
   abstract = {Large Language Models (LLMs) are being quickly integrated in a myriad of software applications. This may introduce a number of biases, such as gender, age or ethnicity, in the behavior of such applications. To face this challenge, we explore the automatic generation of tests suites to assess the potential biases of an LLM. Each test is defined as a prompt used as input to the LLM and a test oracle that analyses the LLM output to detect the presence of biases.},
   author = {Sergio Morales and Robert Clarisó and Jordi Cabot},
   doi = {10.1109/ASE56229.2023.00018},
   isbn = {9798350329964},
   booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {bias,ethics,fairness,large language models,testing},
   pages = {1705-1707},
   publisher = {IEEE Press},
   title = {Automating Bias Testing of LLMs},
   url = {https://doi.org/10.1109/ASE56229.2023.00018},
   year = {2024}
}

@inproceedings{Liu2024-4,
   abstract = {The popularity of automatic speech recognition (ASR) systems nowadays leads to an increasing need for improving their accessibility. Handling stuttering speech is an important feature for accessible ASR systems. To improve the accessibility of ASR systems for stutterers, we need to expose and analyze the failures of ASR systems on stuttering speech. The speech datasets recorded from stutterers are not diverse enough to expose most of the failures. Furthermore, these datasets lack ground truth information about the non-stuttered text, rendering them unsuitable as comprehensive test suites. Therefore, a methodology for generating stuttering speech as test inputs to test and analyze the performance of ASR systems is needed. However, generating valid test inputs in this scenario is challenging. The reason is that although the generated test inputs should mimic how stutterers speak, they should also be diverse enough to trigger more failures. To address the challenge, we propose Aster, a technique for automatically testing the accessibility of ASR systems. Aster can generate valid test cases by injecting five different types of stuttering. The generated test cases can both simulate realistic stuttering speech and expose failures in ASR systems. Moreover, Aster can further enhance the quality of the test cases with a multi-objective optimization-based seed updating algorithm. We implemented Aster as a framework and evaluated it on four open-source ASR models and three commercial ASR systems. We conduct a comprehensive evaluation of Aster and find that it significantly increases the word error rate, match error rate, and word information loss in the evaluated ASR systems. Additionally, our user study demonstrates that the generated stuttering audio is indistinguishable from real-world stuttering audio clips.},
   author = {Yi Liu and Yuekang Li and Gelei Deng and Felix Juefei-Xu and Yao Du and Cen Zhang and Chengwei Liu and Yeting Li and Lei Ma and Yang Liu},
   doi = {10.1109/ASE56229.2023.00107},
   isbn = {9798350329964},
   booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {accessibility testing,automatic speech recognition},
   pages = {510-521},
   publisher = {IEEE Press},
   title = {Aster: Automatic Speech Recognition System Accessibility Testing for Stutterers},
   url = {https://doi.org/10.1109/ASE56229.2023.00107},
   year = {2024}
}
@inproceedings{Feldt2024,
   abstract = {Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized "hallucination" of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.},
   author = {Robert Feldt and Sungmin Kang and Juyeon Yoon and Shin Yoo},
   doi = {10.1109/ASE56229.2023.00148},
   isbn = {9798350329964},
   booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {artificial intelligence,large language model,machine learning,software testing,test automation},
   pages = {1688-1693},
   publisher = {IEEE Press},
   title = {Towards Autonomous Testing Agents via Conversational Large Language Models},
   url = {https://doi.org/10.1109/ASE56229.2023.00148},
   year = {2024}
}
@inproceedings{Sun2024,
   abstract = {SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LaST, and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, LaST has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.},
   author = {Maolin Sun and Yibiao Yang and Yang Wang and Ming Wen and Haoxiang Jia and Yuming Zhou},
   doi = {10.1109/ASE56229.2023.00180},
   isbn = {9798350329964},
   booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {SMT solver,data augmentation,fuzzing,large language model,retrain-finetune},
   pages = {1288-1300},
   publisher = {IEEE Press},
   title = {SMT Solver Validation Empowered by Large Pre-Trained Language Models},
   url = {https://doi.org/10.1109/ASE56229.2023.00180},
   year = {2024}
}
@inproceedings{Kim2024-2,
   abstract = {Modern web services increasingly rely on REST APIs. Effectively testing these APIs is challenging due to the vast search space to be explored, which involves selecting API operations for sequence creation, choosing parameters for each operation from a potentially large set of parameters, and sampling values from the virtually infinite parameter input space. Current testing tools lack efficient exploration mechanisms, treating all operations and parameters equally (i.e., not considering their importance or complexity) and lacking prioritization strategies. Furthermore, these tools struggle when response schemas are absent in the specification or exhibit variants. To address these limitations, we present an adaptive REST API testing technique that incorporates reinforcement learning to prioritize operations and parameters during exploration. Our approach dynamically analyzes request and response data to inform dependent parameters and adopts a sampling-based strategy for efficient processing of dynamic API feedback. We evaluated our technique on ten RESTful services, comparing it against state-of-the-art REST testing tools with respect to code coverage achieved, requests generated, operations covered, and service failures triggered. Additionally, we performed an ablation study on prioritization, dynamic feedback analysis, and sampling to assess their individual effects. Our findings demonstrate that our approach outperforms existing REST API testing tools in terms of effectiveness, efficiency, and fault-finding ability.},
   author = {Myeongsoo Kim and Saurabh Sinha and Alessandro Orso},
   doi = {10.1109/ASE56229.2023.00218},
   isbn = {9798350329964},
   booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
   keywords = {automated rest API testing,reinforcement learning for testing},
   pages = {446-458},
   publisher = {IEEE Press},
   title = {Adaptive REST API Testing with Reinforcement Learning},
   url = {https://doi.org/10.1109/ASE56229.2023.00218},
   year = {2024}
}
@inproceedings{Wan2024,
   abstract = {The widespread adoption of Large Language Models (LLMs) is impeded by their demanding compute and memory resources. The first task of this paper is to explore optimization strategies to expedite LLMs, including quantization, pruning, and operation-level optimizations. One unique direction is to optimize LLM inference through novel software/hardware co-design methods. Given the accelerated LLMs, the second task of this paper is to study LLMs' performance in the usage scenario of circuit design and verification. Specifically, we place a particular emphasis on functional verification. Through automated prompt engineering, we harness the capabilities of the established LLM, GPT-4, to generate High-Level Synthesis (HLS) designs with predefined errors based on 11 open-source synthesizable HLS benchmark suites. This dataset is a comprehensive collection of over 1000 function-level designs, and each of which is afflicted with up to 45 distinct combinations of defects injected into the source code. This dataset, named Chrysalis, expands upon what's available in current HLS error models, offering a rich resource for training to improve how LLMs debug code. The dataset can be accessed at: https://github.com/UIUC-ChenLab/Chrysalis-HLS.},
   author = {Lily Jiaxin Wan and Yingbing Huang and Yuhong Li and Hanchen Ye and Jinghua Wang and Xiaofan Zhang and Deming Chen},
   doi = {10.1109/ASP-DAC58780.2024.10473893},
   isbn = {9798350393545},
   booktitle = {Proceedings of the 29th Asia and South Pacific Design Automation Conference},
   keywords = {functional verification,large language models,software/hardware co-design},
   pages = {435-441},
   publisher = {IEEE Press},
   title = {Software/Hardware Co-Design for LLM and Its Application for Design Verification},
   url = {https://doi.org/10.1109/ASP-DAC58780.2024.10473893},
   year = {2024}
}
@inproceedings{Aggarwal2021,
   abstract = {With widespread adoption of AI models for important decision making, ensuring reliability of such models remains an important challenge. In this paper, we present an end-to-end generic framework for testing AI Models which performs automated test generation for different modalities such as text, tabular, and time-series data and across various properties such as accuracy, fairness, and robustness. Our tool has been used for testing industrial AI models and was very effective to uncover issues present in those models.Demo video link- https://youtu.be/984UCU17YZI},
   author = {Aniya Aggarwal and Samiulla Shaikh and Sandeep Hans and Swastik Haldar and Rema Ananthanarayanan and Diptikalyan Saha},
   doi = {10.1109/ICSE-Companion52605.2021.00041},
   booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
   pages = {81-84},
   publisher = {IEEE Press},
   title = {Testing framework for black-box AI models},
   url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00041},
   year = {2021}
}
@inproceedings{Talebipour2023,
   abstract = {Creating UI tests for mobile applications is a difficult and time-consuming task. As such, there has been a considerable amount of work carried out to automate the generation of mobile tests—largely focused upon the goals of maximizing code coverage or finding crashes. However, comparatively fewer automated techniques have been proposed to generate a highly sought after type of test: usage-based tests. These tests exercise targeted app functionalities for activities such as regression testing. In this paper, we present the Avgust tool for automating the construction of usage-based tests for mobile apps. Avgust learns usage patterns from videos of app executions collected by beta testers or crowd-workers, translates these into an app-agnostic state-machine encoding, and then uses this encoding to generate new test cases for an unseen target app. We evaluated Avgust on 374 videos of use cases from 18 popular apps and found that it can successfully exercise the desired usage in 69\% of the tests. Avgust is an open-source tool available at https://github.com/felicitia/UsageTesting-Repo/tree/demo. A video illustrating the capabilities of Avgust can be found at: https://youtu.be/LPICxVd0YAg.},
   author = {Saghar Talebipour and Hyojae Park and Kesina Baral and Leon Yee and Safwat Ali Khan and Kevin Moran and Yuriy Brun and Nenad Medvidovic and Yixue Zhao},
   doi = {10.1109/ICSE-Companion58688.2023.00030},
   isbn = {9798350322637},
   booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
   keywords = {AI/ML,UI understanding,mobile application,mobile testing,test generation},
   pages = {83-87},
   publisher = {IEEE Press},
   title = {Avgust: A Tool for Generating Usage-Based Tests from Videos of App Executions},
   url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00030},
   year = {2023}
}
@inproceedings{Lemieux2023,
   abstract = {Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.},
   author = {Caroline Lemieux and Jeevana Priya Inala and Shuvendu K Lahiri and Siddhartha Sen},
   doi = {10.1109/ICSE48619.2023.00085},
   isbn = {9781665457019},
   booktitle = {Proceedings of the 45th International Conference on Software Engineering},
   pages = {919-931},
   publisher = {IEEE Press},
   title = {CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models},
   url = {https://doi.org/10.1109/ICSE48619.2023.00085},
   year = {2023}
}
@inproceedings{Li2023,
   abstract = {Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems.In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTest features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTest repairs the code completion outputs by selecting the output that mostly reflects the "average" appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86\%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40\% and 67\% with respect to BLEU score and Levenshtein edit similarity.},
   author = {Zongjie Li and Chaozheng Wang and Zhibo Liu and Haoxuan Wang and Dong Chen and Shuai Wang and Cuiyun Gao},
   doi = {10.1109/ICSE48619.2023.00110},
   isbn = {9781665457019},
   booktitle = {Proceedings of the 45th International Conference on Software Engineering},
   pages = {1238-1250},
   publisher = {IEEE Press},
   title = {CCTest: Testing and Repairing Code Completion Systems},
   url = {https://doi.org/10.1109/ICSE48619.2023.00110},
   year = {2023}
}

@inproceedings{Kang2023,
   abstract = {Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28\% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33\% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32\% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.},
   author = {Sungmin Kang and Juyeon Yoon and Shin Yoo},
   doi = {10.1109/ICSE48619.2023.00194},
   isbn = {9781665457019},
   booktitle = {Proceedings of the 45th International Conference on Software Engineering},
   keywords = {natural language processing,software engineering,test generation},
   pages = {2312-2323},
   publisher = {IEEE Press},
   title = {Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction},
   url = {https://doi.org/10.1109/ICSE48619.2023.00194},
   year = {2023}
}
@inproceedings{Upadhyay2024,
   author = {Mohit Upadhyay and Rohan Juneja and Weng-Fai Wong and Li-Shiuan Peh},
   doi = {10.23919/DATE58400.2024.10546727},
   booktitle = {2024 Design, Automation and Test in Europe Conference \& Exhibition (DATE)},
   keywords = {Deep learning;Computational modeling;Neural networks;Wires;Transforms;Computer architecture;Vectors;Deep learning on the edge;attention layer;network-on-chip;non-linear operators},
   pages = {1-6},
   title = {NOVA: NoC-based Vector Unit for Mapping Attention Layers on a CNN Accelerator},
   year = {2024}
}
@inproceedings{Xiao2024-2,
   author = {Chao Xiao and Yifei Deng and Zhijie Yang and Renzhi Chen and Hong Wang and Jingyue Zhao and Huadong Dai and Lei Wang and Yuhua Tang and Weixia Xu},
   doi = {10.23919/DATE58400.2024.10546707},
   booktitle = {2024 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Program processors;Neuromorphics;Scalability;Large language models;Instruction sets;Computer architecture;Manuals;Life estimation;Test pattern generators;Task analysis;processor function verification;large language model (LLM);test generation;neuromorphic processor},
   pages = {1-6},
   title = {LLM-based Processor Verification: A Case Study for Neuromorphic Processor},
   year = {2024}
}
@inproceedings{Delestrac2024,
   author = {Paul Delestrac and Debjyoti Battacharjee and Simei Yang and Diksha Moolchandani and Francky Catthoor and Lionel Torres and David Novo},
   doi = {10.23919/DATE58400.2024.10546769},
   booktitle = {2024 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Training;Measurement;Tensors;Computational modeling;Graphics processing units;Throughput;Software},
   pages = {1-6},
   title = {Multi-Level Analysis of GPU Utilization in ML Training Workloads},
   year = {2024}
}
@inproceedings{Liang2024,
   author = {Shengwen Liang and Ziming Yuan and Ying Wang and Dawen Xu and Huawei Li and Xiaowei Li},
   doi = {10.23919/DATE58400.2024.10546723},
   booktitle = {2024 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Training;Performance evaluation;Privacy;Costs;Graphics processing units;Information security;Transformers;Near data processing;Embedding;Question answering;In-memory computing;In-storage computing},
   pages = {1-6},
   title = {HyQA: Hybrid Near-Data Processing Platform for Embedding Based Question Answering System},
   year = {2024}
}

@article{Salcedo2024,
   abstract = {This article presents a novel approach to using generative artificial intelligence (AI), specifically GPT-4, to accelerate the design and verification of a vector processor SoC, demonstrating the potential of AI to streamline chip development processes and reduce time to market. —Matthew Guthaus, University of California at Santa Cruz, USA},
   author = {William Salcedo and Sara Achour and Courtney McBeth},
   doi = {10.1109/MDAT.2024.3404117},
   issue = {6},
   journal = {IEEE Design \& Test},
   keywords = {Registers;Hardware design languages;Vector processors;Computer architecture;Codes;Generative AI;Artificial intelligence;Vector processors},
   pages = {8-18},
   title = {Leveraging Generative AI for Rapid Design and Verification of a Vector Processor SoC},
   volume = {41},
   year = {2024}
}

@inproceedings{Afifi2024,
   author = {Salma Afifi and Febin Sunny and Mahdi Nikdast and Sudeep Pasricha},
   doi = {10.23919/DATE58400.2024.10546653},
   booktitle = {2024 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Optical device fabrication;Optical computing;Silicon photonics;Throughput;Transformers;Energy efficiency;Natural language processing;artificial intelligence;silicon photonics;hardware accelerators;large language models;graph neural networks},
   pages = {1-6},
   title = {Accelerating Neural Networks for Large Language Models and Graph Processing with Silicon Photonics},
   year = {2024}
}
@inproceedings{Kande2024,
   author = {Rahul Kande and Vasudev Gohil and Matthew DeLorenzo and Chen Chen and Jeyavijayan Rajendran},
   doi = {10.1109/VTS60656.2024.10538871},
   booktitle = {2024 IEEE 42nd VLSI Test Symposium (VTS)},
   keywords = {Industries;Codes;Hardware security;Source coding;Design methodology;Electronics industry;Very large scale integration;LLM;hardware;verification;security},
   pages = {1-4},
   title = {LLMs for Hardware Security: Boon or Bane?},
   year = {2024}
}
@inproceedings{Liu2024-5,
   abstract = {Large language models (LLMs) present unprecedented opportunities in task automation for industrial chip design and verification that can yield significant improvements in engineering productivity. Instead of deploying off-the-shelf LLMs, we present our methodology for adapting a language model to the domain of VLSI design, and we show that our domain-adapted model, ChipNeMo, achieves improved performance against models of similar size on benchmarks concerning chip design and electronic design automation (EDA). We finally present a case study on the prospective of applying LLMs to hardware formal verification. Our results indicate that the largest and most capable models, such as GPT-4, are able to generate syntactically correct SVA implementations, yet there exists room for improvement in ensuring precise reflection of user intent given as high-level natural language descriptions of formal properties.},
   author = {Mingjie Liu and Minwoo Kang and Ghaith Bany Hamad and Syed Suhaib and Haoxing Ren},
   doi = {10.1109/VTS60656.2024.10538589},
   booktitle = {2024 IEEE 42nd VLSI Test Symposium (VTS)},
   keywords = {Productivity;Adaptation models;Automation;Very large scale integration;Hardware;Reflection;Problem-solving},
   pages = {1-4},
   title = {Domain-Adapted LLMs for VLSI Design and Verification: A Case Study on Formal Verification},
   year = {2024}
}

@inproceedings{Zimmermann2023,
   abstract = {This paper introduces a new method for GUI-based software testing that utilizes GPT-3, a state-of-the-art language model. The approach uses GPT-3’s transformer architecture to interpret natural language test cases and programmatically navigate through the application under test. To overcome the memory limitations of the transformer architecture, we propose incorporating the current state of all GUI elements into the input prompt at each time step. Additionally, we suggest using a test automation framework to interact with the GUI elements and provide GPT-3 with information about the application’s current state. To simplify the process of acquiring training data, we also present a tool for this purpose. The proposed approach has the potential to improve the efficiency of software testing by eliminating the need for manual input and allowing non-technical users to easily input test cases for both desktop and mobile applications.},
   author = {Daniel Zimmermann and Anne Koziolek},
   doi = {10.1109/ICSTW58534.2023.00022},
   booktitle = {2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Software testing;Automation;Navigation;Natural languages;Training data;Computer architecture;Manuals;UI Testing;Test Automation;Deep Learning;Language Models},
   pages = {62-65},
   title = {Automating GUI-based Software Testing with GPT-3},
   year = {2023}
}

@inproceedings{Hoffmann2024-1,
   abstract = {Motivation. Software tests are a necessity in the development of software to secure functionality, reliability, and usability [10]; however, these tests are costly and time-consuming [6]. Although tool support for software testing has advanced, there remains considerable potential for enhancement. Many software tests are still devised manually, with the creation of unit tests being particularly laborious. Automating the generation of test cases is promising for streamlining this aspect of software testing [6].Large Language Models (LLMs) have exhibited capabilities in code generation [11, 13–15], test case generation [17], and various other domains [11]. The advancement of model performance of transformer-based LLMs is mainly achieved by expanding the model size in line with an increase in training data size [7, 8]. However, this approach leads to high computational costs which can only be afforded by corporations with significant financial resources. This highlights the need for transformer-based LLMs that perform well on a specific downstream task and are also cost-efficient. Addressing this, we focused on supervised fine-tuning (SFT) of more resource-efficient transformer-based LLMs LLaMA 2 13B, Code Llama 13B, and Mistral 7B for the specific downstream task of generating test cases for mobile applications.},
   author = {Jacob Hoffmann and Demian Frister},
   booktitle = {2024 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Software testing;Codes;Computational modeling;Training data;Transformers;Software;Data models;Software Testing;Mobile Testing;Machine Learning;Large Language Models},
   pages = {76-77},
   title = {Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models},
   year = {2024}
}

@inproceedings{Caizares2024,
   author = {Pablo C Cañizares and Daniel Ávila and Sara Pérez-Soler and Esther Guerra and Juan de Lar},
   booktitle = {2024 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Software testing;Automation;Natural languages;Oral communication;Debugging;Chatbots;Software systems;Testing;Test suite generation;Task-oriented conversational agents},
   pages = {23-33},
   title = {Coverage-based Strategies for the Automated Synthesis of Test Scenarios for Conversational Agents},
   year = {2024}
}
@inproceedings{Gtharsson2024,
   author = {Malte Götharsson and Karl Stahre and Gregory Gay and Francisco Gomes de Oliveira Neto},
   booktitle = {2024 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Software maintenance;Accuracy;Automation;Computer bugs;Semantics;Focusing;Writing;Bug Reports;Duplicate Bug Reports;Automated Duplicate Bug Report Detection;Natural Language Processing;Software Testing},
   pages = {193-203},
   title = {Exploring the Role of Automation in Duplicate Bug Report Detection: An Industrial Case Study},
   year = {2024}
}

@inproceedings{Haji2024,
   abstract = {Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot's test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28\% of the tests generated by Copilot are passing tests; 54.72\% of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45\% of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.},
   author = {Khalid El Haji and Carolin Brandt and Andy Zaidman},
   booktitle = {2024 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Codes;Runtime;Source coding;Writing;Syntactics;Software;Test pattern generators},
   pages = {45-55},
   title = {Using GitHub Copilot for Test Generation in Python: An Empirical Study},
   year = {2024}
}

@inproceedings{Ivanov2024,
   author = {Rosen Ivanov and Victoria Velkova},
   doi = {10.1109/AQTR61889.2024.10554146},
   booktitle = {2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)},
   keywords = {Protocols;Microservice architectures;Chatbots;Museums;Reliability;Robots;Programming profession;Chatbots;Microservices;MSA;AMQP;GPT API},
   pages = {1-5},
   title = {Microservice-Based Interface to ChatGPT},
   year = {2024}
}
@inproceedings{Chi2024,
   author = {Andrei Chiş and Oliviu Ionuţ Stoica and Ana-Maria Ghiran and Robert Andrei Buchmann},
   doi = {10.1109/AQTR61889.2024.10554074},
   booktitle = {2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)},
   keywords = {Current measurement;Semantics;Knowledge graphs;Transforms;Data models;Security;Proposals;knowledge graphs;security;privacy;data flow diagrams;threat modeling;LLMs},
   pages = {1-6},
   title = {A Knowledge Graph Approach to Cyber Threat Mitigation Derived from Data Flow Diagrams},
   year = {2024}
}
@inproceedings{Stanica2024,
   author = {Iulia-Cristina Stanica and Simona Magdalena Hainagiu and Alberta Milicu and Maria-Iuliana Dascalu},
   doi = {10.1109/AQTR61889.2024.10554213},
   booktitle = {2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)},
   keywords = {Automation;Anxiety disorders;Focusing;Virtual reality;Mental health;Games;User experience;virtual reality;eHealth;immersive exercises;anxiety reduction;emergent technology},
   pages = {1-6},
   title = {Design of Virtual Reality Games for eHealth: Immersive Exercises to Reduce Anxiety},
   year = {2024}
}
@inproceedings{Rostami2024,
   author = {Mohamadreza Rostami and Marco Chilese and Shaza Zeitouni and Rahul Kande and Jeyavijayan Rajendran and Ahmad-Reza Sadeghi},
   doi = {10.23919/DATE58400.2024.10546625},
   booktitle = {2024 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Program processors;Codes;Scalability;Computer bugs;Reinforcement learning;Manuals;Fuzzing},
   pages = {1-6},
   title = {Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing},
   year = {2024}
}
@inproceedings{Qiu2024,
   author = {Ruidi Qiu and Amro Eldebiky and Grace Li Zhang and Xunzhao Yin and Cheng Zhuo and Ulf Schlichtmann and Bing Li},
   doi = {10.23919/DATE58400.2024.10546606},
   booktitle = {2024 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Knowledge engineering;Optical design;Neural networks;Optical computing;Optical fiber networks;Optical imaging;Throughput},
   pages = {1-6},
   title = {OplixNet: Towards Area-Efficient Optical Split-Complex Networks with Real-to-Complex Data Assignment and Knowledge Distillation},
   year = {2024}
}
@inproceedings{Han2024,
   author = {F Han and Sigrid Eldh and Kristian Wiklund and Andreas Ermedah and Philipp Haller and Cyrille Artho},
   doi = {10.1109/ICST60714.2024.00042},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Location awareness;Software testing;Fault diagnosis;Embedded systems;Statistical analysis;Source coding;Maintenance engineering;continuous integration;software build;compilation error;fault localization},
   pages = {383-394},
   title = {In Industrial Embedded Software, are Some Compilation Errors Easier to Localize and Fix than Others?},
   year = {2024}
}
@inproceedings{Rahman2024-2,
   author = {Shanto Rahman and Abdelrahman Baz and Sasa Misailovic and August Shi},
   doi = {10.1109/ICST60714.2024.00018},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Training;Software testing;Quantization (signal);Codes;Computational modeling;Source coding;Artificial neural networks;Flaky Test Categorization;Large-Language Models;Quantization},
   pages = {93-104},
   title = {Quantizing Large-Language Models for Predicting Flaky Tests},
   year = {2024}
}

@inproceedings{Yoon2024,
   abstract = {GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61\% activity coverage, compared to 51 \% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 547 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.},
   author = {Juyeon Yoon and Robert Feldt and Shin Yoo},
   doi = {10.1109/ICST60714.2024.00020},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Measurement;Automation;Large language models;Semantics;Manuals;Software systems;software testing;GUI testing;test automation;artificial intelligence;large language model},
   pages = {129-139},
   title = {Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents},
   year = {2024}
}

@inproceedings{Molina2024,
   abstract = {Patch correctness assessment represents a crucial step in the patch validation process, with the potential to enhance the practical adoption of automated program repair (APR) techniques and substantially reduce validation costs. While some automated techniques have been proposed for assessing patch correctness, they primarily focus on either ranking patches based on their likelihood of being correct or classifying them as correct or incorrect without offering any further explanatory information. In this paper, we introduce FIXCHECK, a novel approach that combines random testing and large language models to automatically generate fault-revealing tests for potentially incorrect patches. To achieve this, FIXCHECK employs a two-fold process: Firstly, a random testing procedure generates a comprehensive set of test cases. Secondly, a large language model is utilized to derive meaningful assertions for each test case. Additionally, FIXCHECK incorporates a selection and prioritization mechanism, which evaluates the generated tests executed on the patched program and discards or ranks them based on their likelihood of revealing faults in the patch. To assess the effectiveness of our approach, we conducted evaluations on a benchmark comprising 160 patches, encompassing both patches created by developers and patches generated by APR tools. The results demonstrate that FIXCHECK effectively generates fault-revealing tests for 62 \% of incorrect patches written by developers, with a high level of confidence. Furthermore, it complements existing patch correctness assessment techniques by providing fault-revealing tests for up to 50\% of the incorrect patches identified by state-of-the-art techniques.},
   author = {Facundo Molina and Juan Manuel Copia and Alessandra Gorla},
   doi = {10.1109/ICST60714.2024.00036},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Fault diagnosis;Analytical models;Costs;Large language models;Maintenance engineering;Benchmark testing;Patch Correctness Assessment;Random Testing;Large Language Models},
   pages = {317-328},
   title = {Improving Patch Correctness Analysis via Random Testing and Large Language Models},
   year = {2024}
}

@inproceedings{Khan2024,
   author = {Safwat Ali Khan and Wenyu Wang and Yiran Ren and Bin Zhu and Jiangfan Shi and Alyssa McGowan and Wing Lam and Kevin Moran},
   doi = {10.1109/ICST60714.2024.00028},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Ion radiation effects;Visualization;Navigation;Semantics;Software;Mobile applications;mobile testing;automated input generation},
   pages = {221-232},
   title = {Aurora: Navigating UI Tarpits via Automated Neural Screen Understanding},
   year = {2024}
}
@inproceedings{Le2024,
   abstract = {API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI-driven approach that leverages the large language model GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation dependency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve test coverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the large language model for generating test scripts and data for API testing.},
   author = {Tri Le and Thien Tran and Duy Cao and Vy Le and Tien N Nguyen and Vu Nguyen},
   doi = {10.1109/ICST60714.2024.00017},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Codes;Large language models;Heuristic algorithms;Software algorithms;Restful API;Companies;REST API;Black-box testing;API testing;Large language models for testing},
   pages = {82-92},
   title = {KAT: Dependency-Aware Automated API Testing with Large Language Models},
   year = {2024}
}

@article{Mercati2024,
   author = {Pietro Mercati and Ganapati Bhat},
   doi = {10.1109/MDAT.2024.3432862},
   journal = {IEEE Design \& Test},
   keywords = {Biomedical monitoring;Internet of Things;Sensors;Monitoring;Circuits;Wearable devices;Wearable sensors;Wearable electronics;health monitoring;energy management;design optimization;privacy and security},
   pages = {1},
   title = {Self-Sustainable Wearable and Internet of Things (IoT) Devices for Health Monitoring: Opportunities and Challenges},
   year = {2024}
}

@inproceedings{Schwachhofer2024,
   abstract = {System-Level Test (SLT) has been an integral part of integrated circuit test flows for over a decade and continues to be significant. Nevertheless, there is a lack of systematic approaches for generating test programs, specifically focusing on the non-functional aspects of the Device under Test (DUT). Currently, test engineers manually create test suites using commercially available software to simulate the end-user environment of the DUT. This process is challenging and laborious and does not assure adequate control over non-functional properties. This paper proposes to use Large Language Models (LLMs) for SLT program generation. We use a pre-trained LLM and fine-tune it to generate test programs that optimize non-functional properties of the DUT, e.g., instructions per cycle. Therefore, we use Gem5, a microarchitectural simulator, in conjunction with Reinforcement Learning-based training. Finally, we write a prompt to generate C code snippets that maximize the instructions per cycle of the given architecture. In addition, we apply hyperparameter optimization to achieve the best possible results in inference.},
   author = {Denis Schwachhofer and Peter Domanski and Steffen Becker and Stefan Wagner and Matthias Sauer and Dirk Pflüger and Ilia Polian},
   doi = {10.1109/ETS61313.2024.10567741},
   booktitle = {2024 IEEE European Test Symposium (ETS)},
   keywords = {Training;Knowledge engineering;Codes;Systematics;Microarchitecture;Process control;Hyperparameter optimization;System-Level Test;Large Language Models;Test Generation;Functional Test;Optimization},
   pages = {1-4},
   title = {Training Large Language Models for System-Level Test Program Generation Targeting Non-functional Properties},
   year = {2024}
}

@inproceedings{Miah2024,
   author = {Tanha Miah and Hong Zhu},
   doi = {10.1109/AITest62860.2024.00022},
   booktitle = {2024 IEEE International Conference on Artificial Intelligence Testing (AITest)},
   keywords = {Computer languages;Codes;Natural languages;MIMICs;Benchmark testing;Metadata;Chatbots;Machine learning;Large language models;ChatGPT;Code generation;Performance evaluation;Usability;R programming language},
   pages = {109-119},
   title = {User Centric Evaluation of Code Generation Tools (Invited Paper)},
   year = {2024}
}

@inproceedings{Porter2024,
   abstract = {We propose extension of the concept of software correctness, and the associated task of software testing, to include interrogation and diagnosis, whereby actionable knowledge about the limitations and performance of the software is gleaned. The new paradigm is especially relevant for AI-based systems such as classifiers and large language models (LLMs) that involve interactions among complex software, underlying training/reference data and the analysis/input data. In this context, the need for both interrogation-to identify problematic outputs when there may be no measure of correctness-and diagnosisto determine where the problem lies-is evident. To make our rhetoric concrete, we present two case studies. Classifier boundaries enable characterization of robustness of classifier output and fragility of inputs. Cliques in metagenomic assembly provide novel insight into the software as well as the potential for performance improvement.},
   author = {Adam A Porter and Alan F Karr},
   doi = {10.1109/ICSTW60967.2024.00026},
   booktitle = {2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Software testing;Analytical models;Uncertainty;Measurement uncertainty;Training data;Software systems;Data models;classifier;uncertainty;DNA reads;metagenomic assembly},
   pages = {93-100},
   title = {Active Model Learning for Software Interrogation and Diagnosis},
   year = {2024}
}

@inproceedings{Hagar2024,
   abstract = {Interest in Artificial Intelligence is everywhere today, including mass media news articles, government research, academic writing, industry usage, and student learning assignments. AI will impact software, testing and related concepts such as software test environments and architectures. This paper presents a consideration of AI regarding test engineering concepts. The focus is on a concept supporting AI called prompt engineering, which helps people and testers using AI get better results. AI cannot be expected to help solve software test engineering problems without proper prompting. The paper introduces the AI concepts and relationships to historical testing. Actual example prompts are explored with implications and results. People learning prompt engineering to support testing will include students and active test engineer designers. While this paper is just a beginning on the test support concept of prompt engineering, future work is considered.},
   author = {Jon Hagar and Satoshi Masuda},
   doi = {10.1109/ICSTW60967.2024.00034},
   booktitle = {2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Software testing;Codes;Computer architecture;Writing;Media;Software;Regulation;Software Test Architecture (STA);Artificial Intelligence (AI);Prompt Engineering;Student-Tester/Designer Learning and Skills},
   pages = {116-121},
   title = {Prompt Engineering Impacts to Software Test Architectures for Beginner to Experts},
   year = {2024}
}

@inproceedings{Zilberman2024,
   abstract = {Large Language Models (LLMs) have shown great success in a wide range of text-generation tasks including the synthesis of code from natural language descriptions. As LLMbased techniques continue to grow in popularity, especially amongst entry-level developers, LLM-generated code has the potential to be deployed in a diverse set of application domains. While LLMs can generate syntactically correct code output, recent work has shown the presence of nonsensical and faulty reasoning in LLM-generated text. As such, overreliance on LLMs for software generation may potentially result in the deployment of faulty software leading to critical system failures. This study explores the capabilities of a single LLM to generate both software and corresponding test suites from the same initial program descriptions, which can be considered analogous to an individual developer coding and unit testing for a given piece of software. We present an empirical framework and evaluation methodology to assess the usefulness of LLM-generated test cases for verifying programs generated by the same LLM. Our findings indicate that LLMs frequently generate irrelevant tests that suffer from numerous quality concerns.},
   author = {Sol Zilberman and H C Betty Cheng},
   doi = {10.1109/ICSTW60967.2024.00018},
   booktitle = {2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Software testing;Codes;Automation;Reviews;Large language models;Conferences;Natural languages;automated software testing;software engineering;deep learning},
   pages = {29-36},
   title = {“No Free Lunch” when using Large Language Models to Verify Self-Generated Programs},
   year = {2024}
}

@inproceedings{Keller2024,
   author = {Tim Keller},
   doi = {10.1109/ICSTW60967.2024.00032},
   booktitle = {2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Software testing;Large language models;Conferences;Decision making;Cognition;Proposals;large language model;llm;task execution;python},
   pages = {108},
   title = {Thinktank: Leveraging LLM Reasoning for Advanced Task Execution in CI/CD},
   year = {2024}
}

@inproceedings{Maddala2024,
   abstract = {Writing SystemVerilog Assertions (SVA) is an important but complex step in verifying Register Transfer Level (RTL) designs. Conventionally, experts need to understand the design specifications and write the SVA assertions, which is time-consuming and error-prone. However, with the recent advancement of transformer models, the Large Language Models (LLMs) assisted assertion generation for design verification is gaining interest in recent times. Motivated by this, we proposed a novel LLM-based framework, LAAG-RV, to generate SVA from the natural language specifications of the design. Our framework provides a one-time Verilog loop for signal synchronization in the generated SVA to improve the generated assertion quality. For our experiments, we created a custom LLM based on OpenAI GPT-4. Furthermore, we developed test cases to validate the LLM-generated assertions. Initial observations show that some generated assertions contain issues and did not pass all the test cases. However, by iteratively prompting the LLMs using carefully crafted manual prompts derived from test case failures in a simulator, the framework can generate correct SVAs. Our results on OpenTitan designs demonstrate that LLMs significantly simplify the process of generating assertions, making it efficient and less error-prone.},
   author = {Karthik Maddala and Bhabesh Mali and Chandan Karfa},
   doi = {10.1109/ITCIndia62949.2024.10651860},
   booktitle = {2024 IEEE 8th International Test Conference India (ITC India)},
   keywords = {Large language models;Natural languages;Manuals;Writing;Transformers;Registers;Synchronization;SVA Generation;LLM;Assertion Based Verification},
   pages = {1-6},
   title = {LAAG-RV: LLM Assisted Assertion Generation for RTL Design Verification},
   year = {2024}
}

@inproceedings{Shi202-1,
   author = {Yihao Shi and Bo Wang and Shengbai Luo and Qingshan Xue and Xueyi Zhang and Sheng Ma},
   doi = {10.1109/ITC-Asia62534.2024.10661330},
   booktitle = {2024 IEEE International Test Conference in Asia (ITC-Asia)},
   keywords = {Fault tolerance;Analytical models;Prevention and mitigation;Fault tolerant systems;Text to image;Hardware;Software;ABFT;fault-tolerance;MM-LLMs;soft errors},
   pages = {1-6},
   title = {Understanding and Mitigating the Soft Error of Contrastive Language-Image Pre-training Models},
   year = {2024}
}

@inproceedings{Garg2024,
   abstract = {With the release of powerful language models trained on large code corpus (e.g., CodeBERT, trained on 6.4 million programs), a new family of mutation testing tools has arisen that promises to generate more “natural” mutants, where the mutated code aims at following the implicit rules and coding conventions produced by programmers. In this paper, we empirically study the observable behavior of CodeBERT-generated mutants and to what extent are these coupled with software vulnerabilities. To do so, we carefully analyze 45 reproducible vulnerabilities from the Vul4J dataset to determine whether the mutants and vulnerabilities fail the same tests and whether the failures are for the same reasons or not. Hence, we define different degrees of vulnerability-coupling classes. Strongly coupled mutants fail the same tests for the same reasons as the vulnerabilities, while test coupled mutants fail the same tests but for some different reason as the vulnerabilities. Partial coupling classes are also considered. Overall, CodeBERT-generated mutants strongly coupled with 32 out of these 45 vulnerabilities (i.e. The mutants fail on the same tests for the same reasons), while another 7 vulnerabilities are test-coupled by CodeBERT mutants (i.e. The mutants fail on the same tests but not for the same reasons). Interestingly, CodeBERT mutants are diverse enough to couple vulnerabilities from 14 out of the 15 types of vulnerabilities explored, i.e., CWEs (Common Weakness Enumeration). Finally, we observe that strongly coupled mutants are scarce (1.17 \% of the killable mutants), test coupled mutants represent 7.2 \%, and 64.9 \% of the killable mutants are not coupled with the vulnerabilities.},
   author = {Aayush Garg and Renzo Degiovanni and Mike Papadakis and Yves Le Traon},
   doi = {10.1109/ICST60714.2024.00035},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Couplings;Software testing;Codes;Large language models;Software;Encoding;vulnerabilities;mutants;large language models},
   pages = {305-316},
   title = {On the Coupling between Vulnerabilities and LLM-Generated Mutants: A Study on Vul4J Dataset},
   year = {2024}
}
@inproceedings{Lee2024,
   author = {Jae Yong Lee and Sungmin Kang and Juyeon Yoon and Shin Yoo},
   doi = {10.1109/ICST60714.2024.00049},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Java;Large language models;Computer bugs;Natural languages;Training data;Debugging;Benchmark;Debugging;Machine Learning},
   pages = {442-444},
   title = {The GitHub Recent Bugs Dataset for Evaluating LLM-Based Debugging Applications},
   year = {2024}
}
@inproceedings{Hyun2024,
   author = {Sangwon Hyun and Mingyu Guo and M Ali Babar},
   doi = {10.1109/ICST60714.2024.00019},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Measurement;Software testing;Systematics;Perturbation methods;Semantics;Metals;Probabilistic logic;Large-language models;Metamorphic testing;Quality attributes;Text perturbations},
   pages = {117-128},
   title = {METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities},
   year = {2024}
}
@inproceedings{Arcuschin2024,
   author = {Iván Arcuschin and Lisandro Di Meo and Michael Auer and Juan P Galeotti and Gordon Fraser},
   doi = {10.1109/ICST60714.2024.00025},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Reverse engineering;Benchmark testing;Reliability engineering;Generators;Computer crashes;Software reliability;ANDROID test generation;Espresso test cases;AndroZoo benchmark;Mutation analysis},
   pages = {185-196},
   title = {Brewing Up Reliability: Espresso Test Generation for Android Apps},
   year = {2024}
}

@inproceedings{Lohiya2024,
   abstract = {The important contribution that large language models (LLMs) have made to the development of a new software testing era is the main objective of this proposed approach. It emphasizes the role that LLMs play in producing complex and diverse input seeds, which opens the way for efficient bug discovery. In the study we also introduce a systematic approach for combining various input values, employing the principles of Combinatorial testing using the PICT (Pairwise independent Combinatorial testing). By promoting a more varied set of inputs for thorough testing, PICT enhances the seed production process. Then we show how these different seeds may be easily included in the American Fuzzy Lop (AFL) tool, demonstrating how AFL can effectively use them to find and detect software flaws. This integrated technique offers a powerful yet straightforward approach to software Quality.},
   author = {Darshan Lohiya and Monika Rani Golla and Sangharatna Godboley and P Radha Krishna},
   doi = {10.1109/ICST60714.2024.00048},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Codes;Systematics;Combinatorial testing;Large language models;Computer bugs;Software quality;Production;Large Language Model;AFL;Pairwise independent combinatorial testing},
   pages = {438-441},
   title = {Poster: gptCombFuzz: Combinatorial Oriented LLM Seed Generation for effective Fuzzing},
   year = {2024}
}

@inproceedings{Coutinho2024,
   author = {Joana Coutinho and Alexandre Lemos and Miguel Terra-Neves and André Ribeiro and Vasco Manquinho and Rui Quintino and Bartlomiej Matejczyk},
   doi = {10.1109/ICST60714.2024.00041},
   booktitle = {2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Computer languages;Visualization;Scalability;Mission critical systems;Manuals;Reliability theory;symbolic execution;satisfiablity module theory;test generation;bug detection;low-code},
   pages = {373-382},
   title = {BugOut: Automated Test Generation and Bug Detection for Low-Code},
   year = {2024}
}
@inproceedings{Kesri2021,
   author = {Vaibhav Kesri and Anmol Nayak and Karthikeyan Ponnalagu},
   doi = {10.1109/ICSTW52544.2021.00047},
   booktitle = {2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Software testing;Industries;Conferences;Pipelines;Software algorithms;Ontologies;Software systems;Automotive Domain Knowledge Graph;Software Testing;Natural Language Processing},
   pages = {234-238},
   title = {AutoKG - An Automotive Domain Knowledge Graph for Software Testing: A position paper},
   year = {2021}
}
@inproceedings{Jalil2023,
   author = {Sajed Jalil and Suzzana Rafi and Thomas D LaToza and Kevin Moran and Wing Lam},
   doi = {10.1109/ICSTW58534.2023.00078},
   booktitle = {2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Software testing;Codes;Limiting;Conferences;Natural languages;Predictive models;Chatbots;ChatGPT;testing;education;case study},
   pages = {4130-4137},
   title = {ChatGPT and Software Testing Education: Promises and Perils},
   year = {2023}
}
@inproceedings{Degiovanni2022,
   abstract = {We introduce µBert, a mutation testing tool that uses a pre-trained language model (CodeBERT) to generate mutants. This is done by masking a token from the expression given as input and using CodeBERT to predict it. Thus, the mutants are generated by replacing the masked tokens with the predicted ones. We evaluate µBert on 40 real faults from Defects4J and show that it can detect 27 out of the 40 faults, while the baseline (PiTest) detects 26 of them. We also show that µBert can be 2 times more cost-effective than PiTest, when the same number of mutants are analysed. Additionally, we evaluate the impact of µBert’s mutants when used by program assertion inference techniques, and show that they can help in producing better specifications. Finally, we discuss about the quality and naturalness of some interesting mutants produced by µBert during our experimental evaluation.},
   author = {Renzo Degiovanni and Mike Papadakis},
   doi = {10.1109/ICSTW55395.2022.00039},
   booktitle = {2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Software testing;Conferences;Mutation Testing;Natural Language Processing;Fault seeding;CodeBERT},
   pages = {160-169},
   title = {µBert: Mutation Testing using Pre-Trained Language Models},
   year = {2022}
}

@article{,
   journal = {Simulation, Testing, Verification, and Validation (STV2) of Autonomous Driving},
   keywords = {AI;artificial intelligence;autonomous driving architecture;autonomous systems;Industry Connections;simulation;STV2;validation;verification;white paper},
   pages = {1-42},
   title = {Simulation, Testing, Verification, and Validation (STV2) of Autonomous Driving},
   year = {2024}
}

@inproceedings{Paul2024,
   author = {Debalina Ghosh Paul and Hong Zhu and Ian Bayley},
   doi = {10.1109/AITest62860.2024.00015},
   booktitle = {2024 IEEE International Conference on Artificial Intelligence Testing (AITest)},
   keywords = {Measurement;Java;Codes;Tutorials;Manuals;Metadata;Benchmark testing;Machine learning;Large language models;ChatGPT;Code Generation;Benchmark;Performance evaluation;Scenario-based testing},
   pages = {55-63},
   title = {ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation},
   year = {2024}
}
@inproceedings{Paul2024-1,
   author = {Debalina Ghosh Paul and Hong Zhu and Ian Bayley},
   doi = {10.1109/AITest62860.2024.00019},
   booktitle = {2024 IEEE International Conference on Artificial Intelligence Testing (AITest)},
   keywords = {Measurement;Codes;Automation;Reviews;Large language models;Natural languages;Machine learning;Machine learning;Large language models;Code generation;Performance evaluation;Benchmarks;Metrics},
   pages = {87-94},
   title = {Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review},
   year = {2024}
}
@inproceedings{Farzandway2024,
   author = {Mahdi Farzandway and Fatemeh Ghassemi},
   doi = {10.1109/AITest62860.2024.00018},
   booktitle = {2024 IEEE International Conference on Artificial Intelligence Testing (AITest)},
   keywords = {Location awareness;Measurement;Technological innovation;Accuracy;Codes;Computer bugs;Semantics;bug localization;spectrum profile;NLP;Code-BERT},
   pages = {81-86},
   title = {SpecNLP: A Pre-trained Model Enhanced with Spectrum Profile for Bug Localization},
   year = {2024}
}

@inproceedings{Jiri2024,
   abstract = {Generative Artificial Intelligence is becoming an integral and enduring part of our lives, growing more powerful with each passing day. This paper explores Large Language Models and their application in text generation, specifically examining their potential to assist software quality assurance engineers in their daily tasks. Our focus is on the generation of unit tests as a critical component of software development. The research question is simple: Can Generative AI generate comprehensive unit tests? We started with Python and a very simple use case, and if Gen AI is successful, we will continue with complex tasks. Current literature focuses on success, but we are interested in failures as well. How many test cases are missing?},
   author = {Medlen Jiri and Bari Emese and Patrick Medlen},
   doi = {10.1109/AITest62860.2024.00020},
   booktitle = {2024 IEEE International Conference on Artificial Intelligence Testing (AITest)},
   keywords = {Analytical models;Generative AI;Large language models;Software quality;Chatbots;Standards;Python;Generative Artificial Intelligence;Software Quality Assurance;Unit Test;Python},
   pages = {95-100},
   title = {Leveraging Large Language Models for Python Unit Test},
   year = {2024}
}

@inproceedings{Iwama2019,
   author = {Futoshi Iwama and Takashi Fukuda},
   doi = {10.1109/ICST.2019.00012},
   booktitle = {2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)},
   keywords = {Automated Testing,Modeling of ASR System Testing,Speech recognition;Hidden Markov models;Robustness;Testing;Text recognition;Data models;Automation;Speech Recognition System},
   pages = {13-24},
   title = {Automated Testing of Basic Recognition Capability for Speech Recognition Systems},
   year = {2019}
}
@inproceedings{Bachmann2022,
   author = {Tobias Bachmann and Djurre van der Wal and Machiel van der Bijl and Daan van der Meij and Ana Oprescu},
   doi = {10.1109/ICST53961.2022.00044},
   booktitle = {2022 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Communication system signaling;Mathematical models;Rail transportation;Behavioral sciences;Safety;Complexity theory;Formal specifications;Axini;Axini Modeling Platform;AMP;Axini Modeling Language;AML;EULYNX;SysML;Model Based Testing;MBT;Signaling Systems},
   pages = {355-364},
   title = {Translating EULYNX SysML Models into Symbolic Transition Systems for Model-Based Testing of Railway Signaling Systems},
   year = {2022}
}
@inproceedings{Kherrazi2020,
   author = {Rachid Kherrazi},
   doi = {10.1109/ICSTW50294.2020.00021},
   booktitle = {2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Automata;Testing;Unified modeling language;C# languages;Tools;Data models;Computational modeling;State Machine Diagrams;Tabular Notation;State Transition Table (STT);Excel Sheet;Model Based Testing (MBT);Spec Explorer;STTSpec},
   pages = {18-23},
   title = {Using tabular notation to support model based testing: A practical experience using STTSpec and Spec Explorer},
   year = {2020}
}
@inproceedings{Wu2023-1,
   author = {Xiaoxue Wu and Wenjing Shan and Wei Zheng and Zhiguo Chen and Tao Ren and Xiaobing Sun},
   doi = {10.1109/AST58925.2023.00005},
   booktitle = {2023 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Software maintenance;Automation;Computer bugs;Semantics;Prediction algorithms;Distributed Bragg reflectors;Data mining;duplicate bug reports detection;automatic term extraction;DeBERTaV3},
   pages = {1-12},
   title = {An Intelligent Duplicate Bug Report Detection Method Based on Technical Term Extraction},
   year = {2023}
}
@inproceedings{Leveau2020,
   author = {Julien Leveau and Xavier Blanc and Laurent Réveillère and Jean-Rémy Falleri and Romain Rouvoy},
   doi = {10.1109/ICST46399.2020.00026},
   booktitle = {2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)},
   keywords = {Predictive models;Computational modeling;Monitoring;Software testing;Buildings;Context modeling;test;Exploratory test;n-gram;web applications},
   pages = {164-174},
   title = {Fostering the Diversity of Exploratory Testing in Web Applications},
   year = {2020}
}
@inproceedings{Trinca2022,
   author = {Miguel Trinca and João F Ferreira and Rui Abreu},
   doi = {10.1109/ICSTW55395.2022.00033},
   booktitle = {2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Program processors;Conferences;Fuzzing;Software;Task analysis;Software engineering;Software development management;Quantum Software Engineering;Fuzz Testing;Sequence-to-Sequence Models;Machine Learning},
   pages = {118-121},
   title = {A Preliminary Study on Generating Well-Formed Q# Quantum Programs for Fuzz Testing},
   year = {2022}
}
@inproceedings{Alshahwan2023,
   author = {Nadia Alshahwan and Mark Harman and Alexandru Marginean},
   doi = {10.1109/ICST57152.2023.00008},
   booktitle = {2023 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Maintenance engineering;Software;Artificial intelligence;Research and development;Automated Software Engineering;Software Testing;Automated Program Repair;Artificial Intelligence;Genetic Improvement;Automated Remediation},
   pages = {1-10},
   title = {Software Testing Research Challenges: An Industrial Perspective},
   year = {2023}
}

@inproceedings{LeTraon2023,
   abstract = {The recent release of ChatGPT conversational agent has been a surprise to me, and to many of my colleagues from the software engineering community. Progress goes extremely fast, while it appears to be a true "game-changing" technology that can even generate programs and fix bugs. Machine Learning (ML) provides engineers with the prospect of producing data-driven software, with little manual code writing. These ML-enabled software bring us to a new era where systems’ logic is automatically produced from data, with a small amount of human-written code. Would we trust such software mixing ML and regular code, would you rely on it and under which conditions? This is still too early to answer these questions, and a challenging direction to explore.This radical change questions the way software are engineered, validated, secured, deployed and maintained. The overall challenge is thus to automate these activities accounting for the statistical nature of ML-enabled software.Taking a software engineering perspective, and starting from a concrete case from the finance industry, the presentation will focus on testing and robustifying a ML model which is integrated in a larger software system that takes as input domain objects (e.g. financial transaction, malware, network traffic). One traditional way to robustify a ML model consists in generating adversarial inputs, e.g. leading to a misclassification, and retraining the model. Indeed, despite their impressive performance, ML models are sensitive to small perturbations in the input. The resulting adversarial inputs raise multiple questions about the robustness of such systems, especially in safety- and business-critical domains. However, the generation of feasible, exploitable adversarial test examples is challenging, as they must satisfy the business logic constraints over the feature space. We analyse the limitations of current adversarial approaches and explore new algorithms that combine multi-objective search with constraint-solving techniques. While the attack part is the offensive weapon, we also consider the challenge to efficiently shield (e.g. repair) the systems against such threats, and finally end the seminar by mentioning other research directions to deploy robust ML-enabled systems.},
   author = {Yves Le Traon},
   doi = {10.1109/ICST57152.2023.00010},
   booktitle = {2023 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Seminars;Codes;Weapons;Machine learning;Telecommunication traffic;Writing},
   pages = {12},
   title = {AI is a game-changing technology: how to test and robustify Machine-Learning software?},
   year = {2023}
}

@inproceedings{Richter2022,
   abstract = {Mutations are small, often token-level changes to program code, typically performed during mutation testing for evaluating the quality of test suites. Recently, code mutations have come in use for creating benchmarks of buggy code. Such bug benchmarks present valuable aids for the evaluation of testing, debugging or bug repair tools. Moreover, they can serve as training data for learning-based (neural) bug detectors. Key to all these applications is the creation of realistic bugs which closely resemble mistakes made by software developers. In this paper, we present a learning-based approach to mutation. We propose a novel contextual mutation operator which incorporates knowledge about the mutation context to inject natural and more realistic bugs into code. Our approach employs a masked language model to produce a context-dependent distribution over feasible token replacements. The strategy for producing realistic mutations is thus learned. Our experimental evaluation on Java, JavaScript and Python programs shows that sampling from a language model does not only produce mutants which more accurately represent real bugs (with a reproduction score nearly 70\% higher than for mutations employed in testing), but also lead to better performing bug detectors when trained on thus generated bug benchmarks.},
   author = {Cedric Richter and Heike Wehrheim},
   doi = {10.1109/ICST53961.2022.00027},
   booktitle = {2022 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Training;Software testing;Codes;Computer bugs;Training data;Detectors;Benchmark testing;Bug Detection;Mutation;Natural Code},
   pages = {162-173},
   title = {Learning Realistic Mutations: Bug Creation for Neural Bug Detectors},
   year = {2022}
}

@inproceedings{Bhme2020,
   author = {Marcel Böhme and Charaka Geethal and Van-Thuan Pham},
   doi = {10.1109/ICST46399.2020.00036},
   booktitle = {2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)},
   keywords = {Computer bugs;Maintenance engineering;Labeling;Tools;Fuzzing;Manuals;Semantics},
   pages = {274-285},
   title = {Human-In-The-Loop Automatic Program Repair},
   year = {2020}
}

@inproceedings{Ding2023-2,
   abstract = {The progress made in deep learning for natural language understanding has inspired researchers to explore similar techniques for programming language understanding. Various methods have been proposed for identifying vulnerabilities in code, including those that work on raw code or use abstract syntax tree (AST) and data-flow analysis. However, these methods only perform single-function analysis and cannot precisely pinpoint bugs. This study introduces a pipeline for detecting and locating null pointer vulnerabilities in C++ source code through cross-function analysis. The pipeline includes a data-flow analyzer capable of analyzing function call relationships and a deep learning model. We evaluate our approach on an industrial dataset and compare it with cppcheck using a user study. Our findings indicate that our method is an effective complement to cppcheck.},
   author = {Yue Ding and Qian Wu and Yinzhu Li and Dongdong Wang and Jiaxin Huang},
   doi = {10.1109/AITest58265.2023.00025},
   booktitle = {2023 IEEE International Conference On Artificial Intelligence Testing (AITest)},
   keywords = {Deep learning;Computer languages;Codes;Source coding;Pipelines;Computer bugs;C++ languages;Programming Language Understanding;Deep Learning;Vulnerability Detection;Cross Function},
   pages = {107-113},
   title = {Leveraging Deep Learning Models for Cross-function Null Pointer Risks Detection},
   year = {2023}
}

@inproceedings{Akli2023,
   abstract = {Flaky tests are tests that yield different outcomes when run on the same version of a program. This non-deterministic behaviour plagues continuous integration with false signals, wasting developers' time and reducing their trust in test suites. Studies highlighted the importance of keeping tests flakiness-free. Recently, the research community has been pushing forward the detection of flaky tests by suggesting many static and dynamic approaches. While promising, those approaches mainly focus on classifying tests as flaky or not and, even when high performances are reported, it remains challenging to understand the cause of flakiness. This part is crucial for researchers and developers that aim to fix it. To help with the comprehension of a given flaky test, we propose FlakyCat, the first approach for classifying flaky tests based on their root cause category. FlakyCat relies on CodeBERT for code representation and leverages a Siamese network-based Few-Shot learning method to train a multi-class classifier with few data. We train and evaluate FlakyCat on a set of 343 flaky tests collected from open-source Java projects. Our evaluation shows that FlakyCat categorises flaky tests accurately, with a weighted F1 score of 70\%. Furthermore, we investigate the performance of our approach for each category, revealing that Async waits, Unordered collections and Time-related flaky tests are accurately classified, while Concurrency-related flaky tests are more challenging to predict. Finally, to facilitate the comprehension of FlakyCat's predictions, we present a new technique for CodeBERT-based model interpretability that highlights code statements influencing the categorization.},
   author = {Amal Akli and Guillaume Haben and Sarra Habchi and Mike Papadakis and Yves Le Traon},
   doi = {10.1109/AST58925.2023.00018},
   booktitle = {2023 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Software testing;Concurrent computing;Java;Codes;Automation;Predictive models;Software;Software Testing;Flaky Tests;CodeBERT;Few-Shot learning;Siamese Networks},
   pages = {140-151},
   title = {FlakyCat: Predicting Flaky Tests Categories using Few-Shot Learning},
   year = {2023}
}
@inproceedings{Li2022,
   author = {Xiaomin Li and Chuanqi Tao and Jerry Gao and Hongjing Guo},
   doi = {10.1109/AITest55621.2022.00021},
   booktitle = {2022 IEEE International Conference On Artificial Intelligence Testing (AITest)},
   keywords = {Software testing;Measurement;Quality assurance;Machine learning;Banking;Big Data;Standards;dialogue systems;quality assurance;test methods;test tools;dialogue evaluation},
   pages = {87-94},
   title = {A Review of Quality Assurance Research of Dialogue Systems},
   year = {2022}
}
@inproceedings{Yue2023,
   author = {Lei Yue and Jingwen Li and Liwei Zheng and Li Li and Zhanqi Cui},
   doi = {10.1109/ATS59501.2023.10317957},
   booktitle = {2023 IEEE 32nd Asian Test Symposium (ATS)},
   keywords = {Location awareness;Measurement;Source coding;Computer bugs;Information retrieval;Software;software debugging;fault localization;information retrieval;mutation analysis;mutants},
   pages = {1-6},
   title = {Software Fault Localization Based on Combining Information Retrieval and Mutation Analysis*},
   year = {2023}
}
@inproceedings{Zhang2019,
   author = {Qishen Zhang and Tamas Kecskes and Ted Bapty and Janos Sztipanovits},
   doi = {10.23919/DATE.2019.8714993},
   booktitle = {2019 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Automation;Europe;cyber-physical systems;design automation;model integration;tool integration;semantic integration},
   pages = {1619-1624},
   title = {Semantic Integration Platform for Cyber-Physical System Design},
   year = {2019}
}
@inproceedings{Zeng2022,
   author = {Yueling Jenny Zeng and Min Jian Yang and Li-C. Wang},
   doi = {10.1109/ITCAsia55616.2022.00016},
   booktitle = {2022 IEEE International Test Conference in Asia (ITC-Asia)},
   keywords = {Semiconductor device modeling;Natural languages;Semantics;Asia;Production;Software;Wafer map;data feedforward;natural language processing;analytics},
   pages = {31-36},
   title = {Wafer Map Pattern Analytics Driven By Natural Language Queries},
   year = {2022}
}
@inproceedings{Kim2023,
   author = {Jinkook Kim and Minseok Jeon and Sejeong Jang and Hakjoo Oh},
   doi = {10.1109/ICST57152.2023.00037},
   booktitle = {2023 IEEE Conference on Software Testing, Verification and Validation (ICST)},
   keywords = {Software testing;Performance evaluation;Art;Solid state drives;Software algorithms;Genetic programming;Manuals;Flash based Storage;Non-functional property testing;Test input generation;Genetic algorithm},
   pages = {317-326},
   title = {Automating Endurance Test for Flash-based Storage Devices in Samsung Electronics},
   year = {2023}
}

@inproceedings{Fidalgo2020,
   abstract = {Reported vulnerabilities have grown significantly over the recent years, with SQL injection (SQLi) being one of the most prominent, especially in web applications. For these, such increase can be explained by the integration of multiple software parts (e.g., various plugins and modules), often developed by different organizations, composing thus web application variants. Machine Learning has the potential to be a great ally on finding vulnerabilities, aiding experts by reducing the search space or even by classifying programs on their own. However, previous work usually does not consider SQLi or utilizes techniques hard to scale. Moreover, there is a clear gap in vulnerability detection with machine learning for PHP, the most popular server-side language for web applications. This paper presents a Deep Learning model able to classify PHP slices as vulnerable (or not) to SQLi. As slices can belong to any variant, we propose the use of an intermediate language to represent the slices and interpret them as text, resorting to well-studied Natural Language Processing (NLP) techniques. Preliminary results of the use of the model show that it can discover SQLi, helping programmers and precluding attacks that would eventually cost a lot to repair.},
   author = {Ana Fidalgo and Ibéria Medeiros and Paulo Antunes and Nuno Neves},
   doi = {10.1109/ICSTW50294.2020.00083},
   booktitle = {2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
   keywords = {Natural language processing;Task analysis;Machine learning;Feature extraction;Software;Structured Query Language;Security;web application vulnerabilities;vulnerability detection;natural language processing;deep learning;software security},
   pages = {465-476},
   title = {Towards a Deep Learning Model for Vulnerability Detection on Web Application Variants},
   year = {2020}
}

@inproceedings{Ojdanic2023,
   abstract = {Recently many mutation testing tools have been proposed that rely on bug-fix patterns and natural language models trained on large code corpus. As these tools operate fundamentally differently from the grammar-based traditional approaches, a question arises of how these tools compare in terms of 1) fault detection and 2) cost-effectiveness. Simultaneously, mutation testing research proposes mutant selection approaches based on machine learning to mitigate its application cost. This raises another question: How do the existing mutation testing tools compare when guided by mutant selection approaches? To answer these questions, we compare four existing tools – μBERT (uses pre-trained language model for fault seeding), IBIR (relies on inverted fix-patterns), DeepMutation (generates mutants by employing Neural Machine Translation) and PIT (applies standard grammar-based rules) in terms of fault detection capability and cost-effectiveness, in conjunction with standard and deep learning based mutant selection strategies. Our results show that IBIR has the highest fault detection capability among the four tools; however, it is not the most cost-effective when considering different selection strategies. On the other hand, μBERT having a relatively lower fault detection capability, is the most cost-effective among the four tools. Our results also indicate that comparing mutation testing tools when using deep learning-based mutant selection strategies can lead to different conclusions than the standard mutant selection. For instance, our results demonstrate that combining μBERT with deep learning-based mutant selection yields 12\% higher fault detection than the considered tools.},
   author = {Milos Ojdanic and Ahmed Khanfir and Aayush Garg and Renzo Degiovanni and Mike Papadakis and Yves Le Traon},
   doi = {10.1109/AST58925.2023.00008},
   booktitle = {2023 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Deep learning;Costs;Fault detection;Bit error rate;Natural languages;Diversity reception;Software;Fault Seeding;Mutation Testing;Empirical Study;Empirical Comparison},
   pages = {35-46},
   title = {On Comparing Mutation Testing Tools through Learning-based Mutant Selection},
   year = {2023}
}

@inproceedings{Gao2023,
   author = {Wentao Gao and Jiayuan He and Van-Thuan Pham},
   doi = {10.1109/DeepTest59248.2023.00008},
   booktitle = {2023 IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)},
   keywords = {Deep learning;Law;Conferences;Back;Software;Internet;Machine translation;metamorphic testing;machine translation testing;back-translation},
   pages = {1-8},
   title = {Metamorphic Testing of Machine Translation Models using Back Translation},
   year = {2023}
}
@inproceedings{Kumar2023,
   author = {Y Kumar and P Morreale and P Sorial and J Delgado and J Jenny Li and P Martins},
   doi = {10.1109/AITest58265.2023.00017},
   booktitle = {2023 IEEE International Conference On Artificial Intelligence Testing (AITest)},
   keywords = {Adaptation models;System testing;Linguistics;Benchmark testing;Chatbots;Artificial intelligence;Standards;Chatbots;Validation of Chatbots;Bot Technologies;AI Linguistic Systems Testing Framework (testFAILS);AIDoctor},
   pages = {51-54},
   title = {A Testing Framework for AI Linguistic Systems (testFAILS)},
   year = {2023}
}
@inproceedings{Ahn2019,
   author = {Minwook Ahn and Seok Joong Hwang and Wonsub Kim and Seungrok Jung and Yeonbok Lee and Mookyoung Chung and Woohyung Lim and Youngjoon Kim},
   doi = {10.23919/DATE.2019.8714950},
   booktitle = {2019 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Field programmable gate arrays;Neural networks;Power demand;Hidden Markov models;Speech recognition;Computational modeling;Computer architecture;neural network;inference;accelerator;speech recognition;FPGA},
   pages = {1495-1500},
   title = {AIX: A high performance and energy efficient inference accelerator on FPGA for a DNN-based commercial speech recognition},
   year = {2019}
}
@inproceedings{Santa2020,
   author = {Maria M Santa and Octavian P Cuibus and Dahlia Al-Janabi and Tiberiu S Leţia},
   doi = {10.1109/AQTR49680.2020.9129999},
   booktitle = {2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)},
   keywords = {Analytical models;Automation;Conferences;Unified modeling language;Data models;Gettering;Robots;UML;OETPN;Activity Diagram;Object Communication Diagram;State Machine Diagram},
   pages = {1-6},
   title = {Relations of UML and OETPN Models},
   year = {2020}
}
@inproceedings{Khojah2023,
   author = {Ranim Khojah and Chi Hong Chao and Francisco Gomes de Oliveira Neto},
   doi = {10.1109/AST58925.2023.00021},
   booktitle = {2023 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Industries;Codes;Automation;Atmospheric measurements;Semantics;Particle measurements;Natural language processing;Diversity-based testing;Natural Language Processing (NLP);Test Case Prioritisation},
   pages = {168-178},
   title = {Evaluating the Trade-offs of Text-based Diversity in Test Prioritisation},
   year = {2023}
}
@inproceedings{Tahvili2019,
   author = {Sahar Tahvili and Leo Hatvani and Michael Felderer and Wasif Afzal and Markus Bohlin},
   doi = {10.1109/AITest.2019.00-13},
   booktitle = {2019 IEEE International Conference On Artificial Intelligence Testing (AITest)},
   keywords = {Manuals;Software;Natural languages;Clustering algorithms;Semantics;Software testing;Software Testing;Paragraph Vectors;Test Case Dependency;Clustering Doc2Vec;HDBSCAN;FCM},
   pages = {19-26},
   title = {Automated Functional Dependency Detection Between Test Cases Using Doc2Vec and Clustering},
   year = {2019}
}
@inproceedings{Fiorucci2021,
   author = {Tiziano Fiorucci and Jean-Marc Daveau and Giorgio di Natale and Philippe Roche},
   doi = {10.1109/IOLTS52814.2021.9486705},
   booktitle = {2021 IEEE 27th International Symposium on On-Line Testing and Robust System Design (IOLTS)},
   keywords = {Systematics;Quality assurance;Random access memory;Single event upsets;Failure analysis;Tools;Safety},
   pages = {1-6},
   title = {Automated Dysfunctional Model Extraction for Model Based Safety Assessment of Digital Systems},
   year = {2021}
}
@inproceedings{Nosrati2019,
   author = {Nooshin Nosrati and Katayoon Basharkhah and Rezgar Sadeghi and Carna Zivkovic and Christoph Grimm and Zainalabedin Navabi},
   doi = {10.1109/EWDTS.2019.8884423},
   booktitle = {2019 IEEE East-West Design and Test Symposium (EWDTS)},
   keywords = {Circuit faults;Software;Testing;Hardware;Software reliability;Analytical models;Abstract Model;Mixed Model;System-Level Test (SLT);Integrated Environment;SystemC},
   pages = {1-5},
   title = {Making System Level Test Possible by a Mixed-mode, Multi-level, Integrated Modeling Environment},
   year = {2019}
}

@inproceedings{Beller2023,
   abstract = {Catching and attributing code change-induced performance regressions in production is hard; predicting them beforehand, even harder. A primer on automatically learning to predict performance regressions in software, this article gives an account of the experiences we gained when researching and deploying an ML-based regression prediction pipeline at Meta.In this paper, we report on a comparative study with four ML models of increasing complexity, from (1) code-opaque, over (2) Bag of Words, (3) off-the-shelve Transformer-based, to (4) a bespoke Transformer-based model, coined SuperPerforator. Our investigation shows the inherent difficulty of the performance prediction problem, which is characterized by a large imbalance of benign onto regressing changes. Our results also call into question the general applicability of Transformer-based architectures for performance prediction: an off-the-shelve CodeBERT-based approach had surprisingly poor performance; even the highly customized SuperPerforator architecture achieved offline results that were on par with simpler Bag of Words models; it only started to significantly outperform it for down-stream use cases in an online setting. To gain further insight into SuperPerforator, we explored it via a series of experiments computing counterfactual explanations. These highlight which parts of a code change the model deems important, thereby validating it.The ability of SuperPerforator to transfer to an application with few learning examples afforded an opportunity to deploy it in practice at Meta: it can act as a pre-filter to sort out changes that are unlikely to introduce a regression, truncating the space of changes to search a regression in by up to 43\%, a 45x improvement over a random baseline.},
   author = {Moritz Beller and Hongyu Li and Vivek Nair and Vijayaraghavan Murali and Imad Ahmad and Jürgen Cito and Drew Carlson and Ari Aye and Wes Dyer},
   doi = {10.1109/AST58925.2023.00010},
   booktitle = {2023 IEEE/ACM International Conference on Automation of Software Test (AST)},
   keywords = {Codes;Automation;Computational modeling;Pipelines;Production;Computer architecture;Predictive models;Machine Learning;Performance Regressions;Prediction;Continuous Integration},
   pages = {56-67},
   title = {Learning to Learn to Predict Performance Regressions in Production at Meta},
   year = {2023}
}

@inproceedings{Guichard2019,
   author = {Jonathan Guichard and Elayne Ruane and Ross Smith and Dan Bean and Anthony Ventresque},
   doi = {10.1109/AITest.2019.000-7},
   booktitle = {2019 IEEE International Conference On Artificial Intelligence Testing (AITest)},
   keywords = {Testing;Robustness;Tools;Task analysis;Grammar;Engines;Software},
   pages = {55-62},
   title = {Assessing the Robustness of Conversational Agents using Paraphrases},
   year = {2019}
}
@inproceedings{Ruospo2021,
   author = {Annachiara Ruospo and Lucas Matana Luza and Alberto Bosio and Marcello Traiola and Luigi Dilillo and Ernesto Sanchez},
   doi = {10.1109/LATS53581.2021.9651807},
   booktitle = {2021 IEEE 22nd Latin American Test Symposium (LATS)},
   keywords = {Measurement;Deep learning;Computer network reliability;Perturbation methods;Artificial neural networks;Market research;Hardware;Reliability;Fault Injection;Neural Networks},
   pages = {1-5},
   title = {Pros and Cons of Fault Injection Approaches for the Reliability Assessment of Deep Neural Networks},
   year = {2021}
}
@inproceedings{Lu2019,
   author = {Renjie Lu and Haihua Shen and Yu Su and Huawei Li and Xiaowei Li},
   doi = {10.1109/ATS47505.2019.00021},
   booktitle = {2019 IEEE 28th Asian Test Symposium (ATS)},
   keywords = {Logic gates;Integrated circuit modeling;Trojan horses;Recurrent neural networks;Natural language processing;Directed graphs;Training;HT detection;Static analysis;Natural language processing (NLP);Recurrent neural network (RNN)},
   pages = {111-1115},
   title = {GramsDet: Hardware Trojan Detection Based on Recurrent Neural Network},
   year = {2019}
}
@inproceedings{Loh2023,
   author = {Gabriel H Loh and Raja Swaminathan},
   doi = {10.23919/DATE56975.2023.10137172},
   booktitle = {2023 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Industries;Semiconductor device modeling;Technological innovation;Moore's Law;Stacking;Computer architecture;Trademarks;chiplets;integration;stacking},
   pages = {1-6},
   title = {The Next Era for Chiplet Innovation},
   year = {2023}
}
@inproceedings{Dupree2023,
   author = {Matthew Dupree and Min Jian Yang and Yueling Jenny Zeng and Li-C. Wang},
   doi = {10.1109/ITC51656.2023.00028},
   booktitle = {2023 IEEE International Test Conference (ITC)},
   keywords = {Analytical models;Data analysis;Natural languages;Knowledge graphs;Reinforcement learning;Production;Software;Test Data Analytics;Langauge Models;Virtual Assistant;AI;Machine Learning;GPT},
   pages = {122-131},
   title = {IEA-Plot: Conducting Wafer-Based Data Analytics Through Chat},
   year = {2023}
}
@inproceedings{Itkin2019,
   author = {Iosif Itkin and Anna Gromova and Anton Sitnikov and Dmitry Legchikov and Evgenii Tsymbalov and Rostislav Yavorskiy and Andrey Novikov and Kirill Rudakov},
   doi = {10.1109/AITest.2019.000-9},
   booktitle = {2019 IEEE International Conference On Artificial Intelligence Testing (AITest)},
   keywords = {Data mining;Testing;Quality control;Computer science;Economics;Instruments;Software;Log Analysis;Clustering;Distributed Applications;Fintech},
   pages = {45-51},
   title = {User-Assisted Log Analysis for Quality Control of Distributed Fintech Applications},
   year = {2019}
}
@inproceedings{Yang2022,
   author = {Min Jian Yang and Yueling Zeng and Li-C. Wang},
   doi = {10.1109/ITC50671.2022.00037},
   booktitle = {2022 IEEE International Test Conference (ITC)},
   keywords = {Semiconductor device modeling;Analytical models;Correlation;Virtual assistants;Scalability;Natural languages;Formal languages;Wafer map;Data feedforward;Data feedback;Natural language processing},
   pages = {288-297},
   title = {Language Driven Analytics for Failure Pattern Feedforward and Feedback},
   year = {2022}
}
@inproceedings{Mhlburger2022,
   author = {Herbert Mühlburger and Franz Wotawa},
   doi = {10.1109/AITest55621.2022.00015},
   booktitle = {2022 IEEE International Conference On Artificial Intelligence Testing (AITest)},
   keywords = {Training;Substations;Semantics;Intrusion detection;Telecommunication traffic;Machine learning;TCPIP;Passive Testing;Security Testing;Intrusion Detection;Anomaly Detection;Power Grid Substation Networks;SCADA network traffic},
   pages = {42-47},
   title = {A Passive Testing Approach using a Semi-Supervised Intrusion Detection Model for SCADA Network Traffic},
   year = {2022}
}
@inproceedings{Stoica2020,
   author = {Anda Diana Stoica and Andrei-Cristian Rad and Ioan Horia Muntean and George Daian and Camelia Lemnaru and Rodica Potolea and Mihaela Dinsoreanu},
   doi = {10.1109/AQTR49680.2020.9129947},
   booktitle = {2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)},
   keywords = {Automation;Conferences;Pipelines;Focusing;Benchmark testing;Filling;Complexity theory;intent detection;slot filling;Romanian home assistant;diacritics;RASA NLU;Wit.ai},
   pages = {1-6},
   title = {The Impact of Romanian Diacritics on Intent Detection and Slot Filling},
   year = {2020}
}
@article{Mirsalari2022,
   author = {Seyed Ahmad Mirsalari and Najmeh Nazari and Sima Sinaei and Mostafa E Salehi and Masoud Daneshtalab},
   doi = {10.1109/MDAT.2021.3070245},
   issue = {3},
   journal = {IEEE Design \& Test},
   keywords = {Encoding;Computer architecture;Quantization (signal);Logic gates;Computational modeling;Multiplexing;Long short term memory;Neural networks;Wearable computers;Long Short -Term Memory (LSTM);ECG;EMG;Wearable Devices;Quantization},
   pages = {45-53},
   title = {FaCT-LSTM: Fast and Compact Ternary Architecture for LSTM Recurrent Neural Networks},
   volume = {39},
   year = {2022}
}
@inproceedings{Song2019,
   author = {Zhuoran Song and Ru Wang and Dongyu Ru and Zhenghao Peng and Hongru Huang and Hai Zhao and Xiaoyao Liang and Li Jiang},
   doi = {10.23919/DATE.2019.8715135},
   booktitle = {2019 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Training;Neurons;Synapses;Acceleration;Graphics processing units;Biological neural networks;Quantization (signal);training;neural network;dropout},
   pages = {108-113},
   title = {Approximate Random Dropout for DNN training acceleration in GPGPU},
   year = {2019}
}
@inproceedings{Yang2020,
   author = {Yipei Yang and Jing Ye and Yuan Cao and Jiliang Zhang and Xiaowei Li and Huawei Li and Yu Hu},
   doi = {10.1109/ATS49688.2020.9301614},
   booktitle = {2020 IEEE 29th Asian Test Symposium (ATS)},
   keywords = {Trojan horses;Hardware;Logic gates;Feature extraction;Security;Integrated circuit modeling;Flip-flops;hardware security;hardware Trojan detection;gate-level netlist},
   pages = {1-6},
   title = {Survey: Hardware Trojan Detection for Netlist},
   year = {2020}
}
@inproceedings{Gavarini2023,
   author = {G Gavarini and A Ruospo and E Sanchez},
   doi = {10.1109/IOLTS59296.2023.10224882},
   booktitle = {2023 IEEE 29th International Symposium on On-Line Testing and Robust System Design (IOLTS)},
   keywords = {Convolution;Neurons;Transformers;Distance measurement;Reliability;Convolutional neural networks;Task analysis;Deep Neural Network;Reliability;Fault Injection;Transformer},
   pages = {1-7},
   title = {Evaluation and Mitigation of Faults Affecting Swin Transformers},
   year = {2023}
}
@inproceedings{Utting2020,
   author = {Mark Utting and Bruno Legeard and Frédéric Dadeau and Frédéric Tamagnan and Fabrice Bouquet},
   doi = {10.1109/AITEST49225.2020.00020},
   booktitle = {2020 IEEE International Conference On Artificial Intelligence Testing (AITest)},
   keywords = {Machine learning;Test pattern generators;Web services;Clustering algorithms;automated regression testing;machine learning;customer traces;clustering;test generation},
   pages = {83-90},
   title = {Identifying and Generating Missing Tests using Machine Learning on Execution Traces},
   year = {2020}
}
@inproceedings{Markwirth2021,
   author = {Thomas Markwirth and Roland Jancke and Christoph Sohrmann},
   doi = {10.23919/DATE51398.2021.9474066},
   booktitle = {2021 Design, Automation and Test in Europe Conference and Exhibition (DATE)},
   keywords = {Digital twin;Prototypes;Real-time systems;Hardware;Formal specifications;Digital Twin;Safety Critical Systems;Fault injection;SystemC;Verification;Validation;HiL},
   pages = {446-450},
   title = {Dynamic fault injection into digital twins of safety-critical systems},
   year = {2021}
}
@article{Cohen2023,
   author = {Seffi Cohen and Dan Presil and Or Katz and Ofir Arbili and Shvat Messica and Lior Rokach},
   city = {NLD},
   doi = {10.1016/j.inffus.2023.101887},
   issn = {1566-2535},
   issue = {C},
   journal = {Inf. Fusion},
   keywords = {Back-translation,GPT,Hate-detection,TTA},
   month = {11},
   publisher = {Elsevier Science Publishers B. V.},
   title = {Enhancing social network hate detection using back translation and GPT-3 augmentations during training and test-time},
   volume = {99},
   url = {https://doi.org/10.1016/j.inffus.2023.101887},
   year = {2023}
}
@article{Ghorbani2024,
   abstract = {This paper presents a novel Artificial Intelligence (AI)-driven tool designed to convert deflection test results into crucial soil parameters essential for quality assurance in compaction projects. The accurate determination of these parameters, such as density and void ratio, is imperative for ensuring the structural integrity of infrastructures constructed on such soils. Moreover, it facilitates the utilization of modern non-destructive equipment in compaction endeavors. The determination of these parameters is notably challenging in unsaturated soils owing to the intricate interplay among factors such as suction, moisture content, void ratio, and resulting deflection. This paper presents a pioneering tool to address these challenges. By integrating unsaturated soil mechanics with advanced AI techniques, particularly reinforcement learning, the tool leverages a diverse array of inputs, including in-situ data, experimental observations, and physics-based modeling. This integration enables dynamic adaptation to changing field conditions and ensuring the tool’s real-time adaptability and predictive accuracy. Field trials validated the tool’s efficacy in predicting soil properties accurately without direct measurements of moisture content or suction, variables often unmeasured in practical soil compaction projects. This unique capability underscores significant advancements in real-time assessment of unsaturated soils, illustrating the transformative potential of AI in geotechnical engineering and unsaturated soil mechanics.},
   author = {Javad Ghorbani and Jayantha Kodikara},
   doi = {https://doi.org/10.1016/j.compgeo.2024.106543},
   issn = {0266-352X},
   journal = {Computers and Geotechnics},
   keywords = {Artificial Intelligence,Data assimilation,Reinforcement learning,Soil compaction,Uncertainty quantification,Unsaturated soils},
   pages = {106543},
   title = {Real-time inference of compacted soil properties using deflection tests: An AI-driven solution informed by unsaturated soil mechanics principles},
   volume = {173},
   url = {https://www.sciencedirect.com/science/article/pii/S0266352X24004798},
   year = {2024}
}

@article{Fetrati2024,
   abstract = {The Cone Penetrometer Test (CPT) is one of the preferred tools to carry out sub-soil investigations. Numerical simulations of CPTs are employed as complements to laboratory experiments and improve understanding of soil behavior. Various numerical methods and soil models have been developed to simulate CPTs in controlled environments such as calibration chambers. In this study, a comparative analysis of three advanced soil models—DeltaSand, hypoplasticity with intergranular strain, and Hardening Soil with small strain stiffness— is performed in CPT simulation. The Material Point Method is employed to allow for the large deformations occurring during CPT. In this study, after calibration of the material models using experimental lab tests, the results of CPT simulations are compared with experimental data in terms of cone resistance and soil deformations near the cone penetrometer tip. The effect of material parameters in soil model which represent the behaviour of soils at small strains is evaluated on the CPT results. The performance of all soil models in predicting the stress evolution under the cone and at different distances from it is also evaluated. The findings clarify the strengths and weaknesses of these soil models and their accuracy, which allow a better understanding of their capabilities and limitations.},
   author = {Majid Fetrati and Vahid Galavi and Majid Goodarzi and Tobias Mörz and Stefan Kreiter},
   doi = {https://doi.org/10.1016/j.compgeo.2024.106683},
   issn = {0266-352X},
   journal = {Computers and Geotechnics},
   keywords = {CPT,Constitutive model,DeltaSand,Hardening Soil,Hypoplasticity,MPM},
   pages = {106683},
   title = {Simulation of cone penetrometer tests in sand using three advanced constitutive models: A comparative study},
   volume = {176},
   url = {https://www.sciencedirect.com/science/article/pii/S0266352X24006220},
   year = {2024}
}
@article{Cafarchio2023,
   abstract = {Background and objectives
Percutaneous microwave thermal ablation is based on electromagnetic waves that generate dielectric heating, and it is widely recognized as one of the mostly used techniques for tumor treatment. The aim of this work is to validate a predictive model capable of providing physicians with guidelines to be used during thermal ablation procedures avoiding collateral damage.
Methods
A finite element commercial software, COMSOL Multiphysics, is employed to implement a tuning-parameter approach. Governing equations are written with reference to variable-porosity and Local Thermal Non-Equilibrium (LTNE) equations are employed. The simulations results are compared with available ex-vivo and in-vivo data with the help of regression analysis. For in-vivo data simulations, velocity vector modulus and direction are varied between 0.0007 and 0.0009 m/s and 90–270°, respectively, in order to use this parameter as a tuning one to simulate – and lately optimize with respect to the differences from experimental outcomes - all the possible directions of the blood flow with respect to the antenna, whose insertion angle is not registered in the dataset.
Results
The model is validated using reference data provided by the manufacturer (AMICA), which is obtained from ex-vivo bovine liver. The model accurately predicts the size and shape of the ablated area, resulting in an overestimation lesser than 10 \%. Additionally, predictive data are compared to an in-vivo dataset. The ablated volume is accurately predicted with a mean underestimation of 6 \%. The sphericity index is calculated as 0.75 and 0.62 for the predictions and in-vivo data, respectively.
Conclusion
This study developed a predictive model for microwave ablation of liver tumors that showed good performance in predicting ablation dimensions and sphericity index for ex-vivo bovine liver and for in-vivo human liver data with the tuning technique. The study emphasizes the necessity for additional development and validation to enhance the accuracy and reliability of in-vivo application.},
   author = {A Cafarchio and M Iasiello and G P Vanoli and A Andreozzi},
   doi = {https://doi.org/10.1016/j.compbiomed.2023.107669},
   issn = {0010-4825},
   journal = {Computers in Biology and Medicine},
   keywords = {Bioheat transfer,In-vivo validation,Microwave ablation,Numerical modeling,Thermal ablation},
   pages = {107669},
   title = {Microwave ablation modeling with AMICA antenna: Validation by means a numerical analysis},
   volume = {167},
   url = {https://www.sciencedirect.com/science/article/pii/S0010482523011344},
   year = {2023}
}
@article{Sanchez-Gendriz2024,
   abstract = {Background:
Effective and timely detection is vital for mitigating the severe impacts of Sexually Transmitted Infections (STI), including syphilis and HIV. Cyclic Voltammetry (CV) sensors have shown promise as diagnostic tools for these STI, offering a pathway towards cost-effective solutions in primary health care settings.
Objective:
This study aims to pioneer the use of Fourier Descriptors (FDs) in analyzing CV curves as 2D closed contours, targeting the simultaneous detection of syphilis and HIV.
Methods:
Raw CV signals are filtered, resampled, and transformed into 2D closed contours for FD extraction. Essential shape characteristics are captured through selected coefficients. A complementary geometrical analysis further extracts features like curve areas and principal axes lengths from CV curves. A Mahalanobis Distance Classifier is employed for differentiation between patient and control groups.
Results:
The evaluation of the proposed method revealed promising results with classification performance metrics such as Accuracy and F1-Score consistently achieving values rounded to 0.95 for syphilis and 0.90 for HIV. These results underscore the potential efficacy of the proposed approach in differentiating between patient and control samples for STI detection.
Conclusion:
By integrating principles from biosensors, signal processing, image processing, machine learning, and medical diagnostics, this study presents a comprehensive approach to enhance the detection of both syphilis and HIV. This setts the stage for advanced and accessible STI diagnostic solutions.},
   author = {Ignacio Sanchez-Gendriz and Dionísio D A Carvalho and Leonardo J Galvão-Lima and Ana Isabela Lopes Sales-Moioli and Talita Brito and Felipe Fernandes and Jorge Henriques and Thaisa Lima and Luiz Affonso Guedes and Agnaldo S Cruz and Antonio H F Morais and João Paulo Q Santos and Ernano Arrais and Karilany Dantas Coutinho and Guilherme Medeiros Machado and Aliete Cunha-Oliveira and Catarina Alexandra dos Reis Vale Gomes and Ricardo A M Valentim},
   doi = {https://doi.org/10.1016/j.compbiomed.2024.108454},
   issn = {0010-4825},
   journal = {Computers in Biology and Medicine},
   keywords = {Cyclic Voltammetry,Fourier Descriptors,Machine Learning,Syphilis and HIV detection},
   pages = {108454},
   title = {Digital dual test syphilis/HIV detection based on Fourier Descriptors of Cyclic Voltammetry curves},
   volume = {174},
   url = {https://www.sciencedirect.com/science/article/pii/S0010482524005389},
   year = {2024}
}
@article{Su2024-2,
   abstract = {Background
Clinical core medical knowledge (CCMK) learning is essential for medical trainees. Adaptive assessment systems can facilitate self-learning, but extracting experts' CCMK is challenging, especially using modern data-driven artificial intelligence (AI) approaches (e.g., deep learning).
Objectives
This study aims to develop a multi-expert knowledge–aggregated adaptive assessment scheme (MEKAS) using knowledge-based AI approaches to facilitate the learning of CCMK in otolaryngology (CCMK-OTO) and validate its effectiveness through a one-month training program for CCMK-OTO education at a tertiary referral hospital.
Methods
The MEKAS utilized the repertory grid technique and case-based reasoning to aggregate experts' knowledge to construct a representative CCMK base, thereby enabling adaptive assessment for CCMK-OTO training. The effects of longitudinal training were compared between the experimental group (EG) and the control group (CG). Both groups received a normal training program (routine meeting, outpatient/operation room teaching, and classroom teaching), while EG received MEKAS for self-learning. The EG comprised 22 UPGY trainees (6 postgraduate [PGY] and 16 undergraduate [UGY] trainees) and 8 otolaryngology residents (ENT-R); the CG comprised 24 UPGY trainees (8 PGY and 16 UGY trainees). The training effectiveness was compared through pre- and post-test CCMK-OTO scores, and user experiences were evaluated using a technology acceptance model-based questionnaire.
Results
Both UPGY (z = −3.976, P < 0.001) and ENT-R (z = −2.038, P = 0.042) groups in EG exhibited significant improvements in their CCMK-OTO scores, while UPGY in CG did not (z = −1.204, P = 0.228). The UPGY group in EG also demonstrated a substantial improvement compared to the UPGY group in CG (z = −4.943, P < 0.001). The EG participants were highly satisfied with the MEKAS system concerning self-learning assistance, adaptive testing, perceived satisfaction, intention to use, perceived usefulness, perceived ease of use, and perceived enjoyment, rating it between an overall average of 3.8 and 4.1 out of 5.0 on all scales.
Conclusions
The MEKAS system facilitates CCMK-OTO learning and provides an efficient knowledge aggregation scheme that can be applied to other medical subjects to efficiently build adaptive assessment systems for CCMK learning. Larger-scale validation across diverse institutions and settings is warranted further to assess MEKAS's scalability, generalizability, and long-term impact.},
   author = {Jun-Ming Su and Su-Yi Hsu and Te-Yung Fang and Pa-Chun Wang},
   doi = {https://doi.org/10.1016/j.compbiomed.2024.108765},
   issn = {0010-4825},
   journal = {Computers in Biology and Medicine},
   keywords = {Adaptive assessment,Clinical core medical knowledge,Knowledge aggregation,Knowledge-based AI approaches,Otolaryngology},
   pages = {108765},
   title = {Developing and validating a knowledge-based AI assessment system for learning clinical core medical knowledge in otolaryngology},
   volume = {178},
   url = {https://www.sciencedirect.com/science/article/pii/S0010482524008503},
   year = {2024}
}
@article{Golshanrad2024,
   abstract = {Recurrent neural networks (RNNs) have emerged as powerful tools for processing sequential data in various fields, including natural language processing and speech recognition. However, the lack of explainability in RNN models has limited their interpretability, posing challenges in understanding their internal workings. To address this issue, this paper proposes a methodology for extracting a state machine (SM) from an RNN-based model to provide insights into its internal function. The proposed SM extraction algorithm was assessed using four newly proposed metrics: Purity, Richness, Goodness, and Scale. The proposed methodology along with its assessment metrics contribute to increasing explainability in RNN models by providing a clear representation of their internal decision making process through the extracted SM. In addition to improving the explainability of RNNs, the extracted SM can be used to advance testing and monitoring of the primary RNN-based model. To enhance RNN testing, we introduce six model coverage criteria based on the extracted SM, serving as metrics for evaluating the effectiveness of test suites designed to analyze the primary model. We also propose a tree-based model to predict the error probability of the primary model for each input based on the extracted SM. We evaluated our proposed online error prediction approach using the MNIST dataset and Mini Speech Commands dataset, achieving an area under the curve (AUC) exceeding 80\% for the receiver operating characteristic (ROC) chart.},
   author = {Pouria Golshanrad and Fathiyeh Faghih},
   doi = {https://doi.org/10.1016/j.jss.2024.111987},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Coverage criteria,Error prediction,Explainability,Recurrent neural networks,State machine,Test},
   pages = {111987},
   title = {DeepCover: Advancing RNN test coverage and online error prediction using state machine extraction},
   volume = {211},
   url = {https://www.sciencedirect.com/science/article/pii/S016412122400030X},
   year = {2024}
}
@article{Wen2023,
   abstract = {In today's digital world, web applications are popular tools used by businesses. As more and more applications are deployed on the web, they are seen as increasingly attractive targets by malicious actors eager to exploit any security gaps present. Organizations are always at risk for potential vulnerabilities in their web-based software systems, which can lead to data loss, service interruption, and lack of trust. Therefore, organizations need to have an effective and efficient method for assessing and analyzing the security of acquired web-based software to ensure adequate confidence in its use. Quantitative security evaluation employs mathematical and computational techniques to express the security level that a system reaches. This research focuses on improving the quantitative analysis of web application security evaluation. We strive to unite the Open Web Application Security Project's (OWASP) Application Security Verification Standard (ASVS) into a structural and analyzable model, which aims to efficiently evaluate web application security levels while providing meaningful insights into their strengths and weaknesses.},
   author = {Shao-Fang Wen and Basel Katt},
   doi = {https://doi.org/10.1016/j.cose.2023.103532},
   issn = {0167-4048},
   journal = {Computers and Security},
   keywords = {Quantitative approach,Security analysis,Security evaluation,Web application security},
   pages = {103532},
   title = {A quantitative security evaluation and analysis model for web applications based on OWASP application security verification standard},
   volume = {135},
   url = {https://www.sciencedirect.com/science/article/pii/S016740482300442X},
   year = {2023}
}
@article{Dong2023,
   abstract = {For traditional single-passage machine reading comprehension, the text data of a single passage does not well reflect the complexity of practical application scenarios. Many researchers have shifted their research goals to study multi-passage machine reading comprehension. To solve the problem of multi-passage machine reading comprehension, this paper proposes a unified multi-module end-to-end reading comprehension model for passage retrieval, answer extraction, and multi-answer verification ranking using a pre-trained model. In this paper, the passage retrieval module selects the passage fragments with the highest probability of survival, and the answer extraction component extracts the possible candidate answers in each passage. The multi-answer verification ranking component uses an attention mechanism to fuse multiple candidate answer feature representations, obtains the score of each candidate answer and selects the candidate answer with the highest score as the final answer. Finally, through experimental validation, the proposed model achieves scores of 49.59 and 46.28 on the evaluation metrics BLUE-4 and ROUGH-L on the DuReader dataset, and scores of 44.78 and 46.45 on the metrics BLUE-1 and ROUGH-L on the MS-MARCO dataset, respectively.},
   author = {Runlu Dong and Xirong Wang and Lihong Dong and Zexuan Zhang},
   doi = {https://doi.org/10.1016/j.compeleceng.2023.108576},
   issn = {0045-7906},
   journal = {Computers and Electrical Engineering},
   keywords = {Application scenarios,Machine reading comprehension,Multi-passage,Passage retrieval,Verification sorting},
   pages = {108576},
   title = {Multi-passage extraction-based machine reading comprehension based on verification sorting},
   volume = {106},
   url = {https://www.sciencedirect.com/science/article/pii/S0045790623000010},
   year = {2023}
}
@article{Saif2024,
   abstract = {The current study aims to establish a connection between students' behavioral concerns, namely stress and anxiety, related to the completion of academic tasks, and their integration of technology using the Technology Acceptance Model (TAM) through the utilization of Chat-GPT via ubiquitous learning (UL) procedure. To achieve this objective, data was collected from 156 students studying management science who were engaged in their final year research projects or internship reports from selected universities in Pakistan. The gathered data underwent analysis through Structural Equation Modeling (SEM) using Smart PLS software. The findings reveal a significant relationship: students' stress contributes to the emergence of anxiety, which in turn motivates the adoption of technology-assisted solutions, specifically Chat-GPT, to efficiently complete assigned tasks within deadlines working through any device from anywhere. Consequently, the perceived ease of use and usefulness associated with Chat-GPT's AI-generated text contribute to shaping students' favorable attitudes toward utilizing Chat-GPT and also play a role in reducing their stress levels. Furthermore, the study confirms that the development of a positive attitude in students acts as a driving force, compelling them to engage with Chat-GPT through ubiquitous learning (UL) procedure, ultimately resulting in increased actual usage of Chat-GPT. This pattern, in turn, contributes to stress and anxiety reduction among management science students. The study's outcomes corroborate the TAM model, which aligns with the social exchange process, demonstrating its applicability within the context of the educational setup in management sciences and its potential to enhance the learning experiences of researchers.},
   author = {Naveed Saif and Sajid Ullah Khan and Imrab Shaheen and Faiz Abdullah ALotaibi and Mrim M Alnfiai and Mohammad Arif},
   doi = {https://doi.org/10.1016/j.chb.2023.108097},
   issn = {0747-5632},
   journal = {Computers in Human Behavior},
   keywords = {Chat-GPT,Higher education,Management sciences students,TAM,Ubiquitous learning (UL) procedure},
   pages = {108097},
   title = {Chat-GPT; validating Technology Acceptance Model (TAM) in education sector via ubiquitous learning mechanism},
   volume = {154},
   url = {https://www.sciencedirect.com/science/article/pii/S074756322300448X},
   year = {2024}
}
@article{Kamel2021,
   abstract = {We propose a formal approach based on Bigraphical Reactive Systems (BRSs) and model checking techniques for modeling and verifying the interaction behaviours of SLA-based cloud computing systems. In the first phase of this approach, we address the modeling of the static structure and the dynamic behavior of cloud systems using BRSs. We show how bigraphs enable the description of the different cloud entities, including cloud actors, cloud services, Service Level Agreements (SLAs), the diversity of their properties, and the complex interactions and dependencies among them. Furthermore, we propose a four-stages SLA lifecycle, and define a set of bigraphical reaction rules to abstract these stages and model the dynamic nature of the cloud. The second phase of this approach verifies that the behavior of services and cloud actors will cope with the agreed SLA terms during the lifecycle of the SLA. We map the proposed bigraphical models into SMV descriptions. Then, we express the interaction behaviors as a set of liveness and safety properties using Linear Temporal Logic (LTL) and Computation Tree Logic (CTL) formulas, and we use the NuSMV model checker to verify them. Finally, we define a case study on which we illustrate the application of our proposed approach.},
   author = {Oussama Kamel and Allaoua Chaoui and Gregorio Diaz and Mohamed Gharzouli},
   doi = {https://doi.org/10.1016/j.csi.2020.103483},
   issn = {0920-5489},
   journal = {Computer Standards and Interfaces},
   keywords = {Bigraph,Bigraphical reactive systems,Cloud computing,Formal verification,NuSMV,SLA},
   pages = {103483},
   title = {SLA-Driven modeling and verifying cloud systems: A Bigraphical reactive systems-based approach},
   volume = {74},
   url = {https://www.sciencedirect.com/science/article/pii/S092054892030369X},
   year = {2021}
}
@article{Grano2019,
   abstract = {Software testing represents a key software engineering practice to ensure source code quality and reliability. To support developers in this activity and reduce testing effort, several automated unit test generation tools have been proposed. Most of these approaches have the main goal of covering as more branches as possible. While these approaches have good performance, little is still known on the maintainability of the test code they produce, i.e.,whether the generated tests have a good code quality and if they do not possibly introduce issues threatening their effectiveness. To bridge this gap, in this paper we study to what extent existing automated test case generation tools produce potentially problematic test code. We consider seven test smells, i.e.,suboptimal design choices applied by programmers during the development of test cases, as measure of code quality of the generated tests, and evaluate their diffuseness in the unit test classes automatically generated by three state-of-the-art tools such as Randoop, JTExpert, and Evosuite. Moreover, we investigate whether there are characteristics of test and production code influencing the generation of smelly tests. Our study shows that all the considered tools tend to generate a high quantity of two specific test smell types, i.e.,Assertion Roulette and Eager Test, which are those that previous studies showed to negatively impact the reliability of production code. We also discover that test size is correlated with the generation of smelly tests. Based on our findings, we argue that more effective automated generation algorithms that explicitly take into account test code quality should be further investigated and devised.},
   author = {Giovanni Grano and Fabio Palomba and Dario Di Nucci and Andrea De Lucia and Harald C Gall},
   doi = {https://doi.org/10.1016/j.jss.2019.07.016},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Empirical studies,Software quality,Test case generation,Test smells},
   pages = {312-327},
   title = {Scented since the beginning: On the diffuseness of test smells in automatically generated test code},
   volume = {156},
   url = {https://www.sciencedirect.com/science/article/pii/S0164121219301487},
   year = {2019}
}
@article{Zhu2021,
   abstract = {Mutation testing is well-known for its efficacy in assessing test quality, and starting to be applied in the industry. However, what should a developer do when confronted with a low mutation score? Should the test suite be plainly reinforced to increase the mutation score, or should the production code be improved as well, to make the creation of better tests possible? In this paper, we aim to provide a new perspective to developers that enables them to understand and reason about the mutation score in the light of testability and observability. First, we investigate whether testability and observability metrics are correlated with the mutation score on six open-source Java projects. We observe a correlation between observability metrics and the mutation score, e.g., test directness, which measures the extent to which the production code is tested directly, seems to be an essential factor. Based on our insights from the correlation study, we propose a number of ”mutation score anti-patterns”, enabling software engineers to refactor their existing code or add tests to improve the mutation score. In doing so, we observe that relatively simple refactoring operations enable an improvement or increase in the mutation score.},
   author = {Qianqian Zhu and Andy Zaidman and Annibale Panichella},
   doi = {https://doi.org/10.1016/j.jss.2020.110864},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Code quality,Code refactoring,Mutation testing,Observability,Testability},
   pages = {110864},
   title = {How to kill them all: An exploratory study on the impact of code observability on mutation testing},
   volume = {173},
   url = {https://www.sciencedirect.com/science/article/pii/S0164121220302545},
   year = {2021}
}
@article{Xu2022,
   abstract = {In recent years, edge computing has become the ideal computing paradigm for various smart systems, such as smart logistics, smart health and smart transportation. This is due to its advantages including fast response times, energy efficiency and cost effectiveness over conventional cloud computing platforms. However, running complex computational scientific workflow tasks is still a very challenging issue at the edge, due to its typical three-layered computing environment consisting of an end device layer, an edge server layer, and a cloud server layer. A large number of recent studies have proposed different solutions for optimizing such computing resource management problems in an edge computing environment. However, since evaluation of most such studies is conducted through simulation, the effectiveness cannot be guaranteed in a real world environment. Therefore, to advance research on efficient execution and deployment problems for real world workflow applications using edge computing, an open-source edge workflow management system with comprehensive empirical evaluation capabilities is urgently required. This paper presents the first edge workflow system (named EdgeWorkflow) that is able to deploy user-created workflow applications to a real-world edge computing environment with “one-click” after optimizing the configuration with the simulation tool. With the aid of EdgeWorkflow, the user can automate the generation of specific edge computing environments, easily model and generate executable workflow applications with a visual modelling tool, effectively select various resource management methods included in the systems or apply their own resource management and task scheduling algorithms, efficiently monitor the statuses of computational tasks and obtain comprehensive reports on the execution results (such as those regarding time, cost and energy). We use an edge computing-based unmanned aerial vehicle (UAV) last-mile delivery system as a real-world case study, and a number of representative scientific workflows are employed for our experiments. Our experimental results show that EdgeWorkflow can effectively evaluate the performance of different resource management and workflow task scheduling algorithms and efficiently deploy and execute user-defined scientific workflow applications to user-specified edge computing environments.},
   author = {Jia Xu and Ran Ding and Xiao Liu and Xuejun Li and John Grundy and Yun Yang},
   doi = {https://doi.org/10.1016/j.jss.2022.111456},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Edge computing,One-click deployment,Resource management,Workflow system},
   pages = {111456},
   title = {EdgeWorkflow: One click to test and deploy your workflow applications to the edge},
   volume = {193},
   url = {https://www.sciencedirect.com/science/article/pii/S0164121222001479},
   year = {2022}
}
@article{Fischbach2023,
   abstract = {Acceptance testing is crucial to determine whether a system fulfills end-user requirements. However, the creation of acceptance tests is a laborious task entailing two major challenges: (1) practitioners need to determine the right set of test cases that fully covers a requirement, and (2) they need to create test cases manually due to insufficient tool support. Existing approaches for automatically deriving test cases require semi-formal or even formal notations of requirements, though unrestricted natural language is prevalent in practice. In this paper, we present our tool-supported approach CiRA (Conditionals in Requirements Artifacts) capable of creating the minimal set of required test cases from conditional statements in informal requirements. We demonstrate the feasibility of CiRA in a case study with three industry partners. In our study, out of 578 manually created test cases, 71.8 \% can be generated automatically. Additionally, CiRA discovered 80 relevant test cases that were missed in manual test case design. CiRA is publicly available at www.cira.bth.se/demo/.},
   author = {Jannik Fischbach and Julian Frattini and Andreas Vogelsang and Daniel Mendez and Michael Unterkalmsteiner and Andreas Wehrle and Pablo Restrepo Henao and Parisa Yousefi and Tedi Juricic and Jeannette Radduenz and Carsten Wiecher},
   doi = {https://doi.org/10.1016/j.jss.2022.111549},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Acceptance testing,Automatic test case creation,Causality extraction,Natural language processing,Requirements engineering},
   pages = {111549},
   title = {Automatic creation of acceptance tests by extracting conditionals from requirements: NLP approach and case study},
   volume = {197},
   url = {https://www.sciencedirect.com/science/article/pii/S0164121222002254},
   year = {2023}
}
@article{Wu2023,
   abstract = {The descriptive naming of unit tests has always been a focal task in test maintenance. Previous work tried to use different methods to generate descriptive names for unit tests or provide suggestions to improve existing names, but they often neglected developers’ needs. Therefore, they are unlikely to be useful to provide descriptive test names for developers in real world. Based on a recent study that can identify uniqueness of JUnit tests, we propose a uniqueness-based name generation approach to generate descriptive test names that meet developers’ needs. Comparing with several alternative approaches, the generated name from our approach are preferred by professional developers, or at least at the same level of preference as the original test names.},
   author = {Jianwei Wu and James Clause},
   doi = {https://doi.org/10.1016/j.jss.2023.111821},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Documentation,Software testing,Test maintenance},
   pages = {111821},
   title = {A uniqueness-based approach to provide descriptive JUnit test names},
   volume = {205},
   url = {https://www.sciencedirect.com/science/article/pii/S0164121223002169},
   year = {2023}
}
@article{Kessel2024,
   abstract = {A core principle of open science is the clear, concise and accessible publication of empirical data, including “raw” observational data as well as processed results. However, in empirical software engineering there are no established standards (de jure or de facto) for representing and “opening” observations collected in test-driven software experiments — that is, experiments involving the execution of software subjects in controlled scenarios. Execution data is therefore usually represented in ad hoc ways, often making it abstruse and difficult to access without significant manual effort. In this paper we present new data structures designed to address this problem by clearly defining, correlating and representing the stimuli and responses used to execute software subjects in test-driven experiments. To demonstrate their utility, we show how they can be used to promote the repetition, replication and reproduction of experimental evaluations of AI-based code completion tools. We also show how the proposed data structures facilitate the incremental expansion of execution data sets, and thus promote their repurposing for new experiments addressing new research questions.},
   author = {Marcus Kessel and Colin Atkinson},
   doi = {https://doi.org/10.1016/j.jss.2024.111971},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Automation,Behavior,Benchmark,Data structures,Empirical,Engineering,Experimentation,Generative artificial intelligence,HumanEval,Language-to-code,Large language models,Machine learning,Measurement,Observation,Open science,Replication,Reproducibility,Software},
   pages = {111971},
   title = {Promoting open science in test-driven software experiments},
   volume = {212},
   url = {https://www.sciencedirect.com/science/article/pii/S0164121224000141},
   year = {2024}
}
@article{Lathouwers2024,
   abstract = {Deductive verifiers require intensive user interaction in the form of writing precise specifications, thereby limiting their use in practice. While many solutions have been proposed to generate specifications, their evaluations and comparisons to other tools are limited. As a result, it is unclear what the best approaches for specification inference are and how these impact the overall specification writing process. In this paper we take steps to address this problem by providing an overview of specification inference tools that can be used for deductive verification of Java programs. For each tool, we discuss its approach to specification inference and identify its advantages and disadvantages. Moreover, we identify the types of specifications that it infers and use this to estimate the impact of the tool on the overall specification writing process. Finally, we identify the ideal features of a specification generator and discuss important challenges for future research.},
   author = {Sophie Lathouwers and Marieke Huisman},
   doi = {https://doi.org/10.1016/j.jss.2024.111972},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Annotations,Deductive verification,Specification generation,Specification inference,Specifications,Survey},
   pages = {111972},
   title = {Survey of annotation generators for deductive verifiers},
   volume = {211},
   url = {https://www.sciencedirect.com/science/article/pii/S0164121224000153},
   year = {2024}
}
@article{Quin2024,
   abstract = {A/B testing, also referred to as online controlled experimentation or continuous experimentation, is a form of hypothesis testing where two variants of a piece of software are compared in the field from an end user’s point of view. A/B testing is widely used in practice to enable data-driven decision making for software development. While a few studies have explored different facets of research on A/B testing, no comprehensive study has been conducted on the state-of-the-art in A/B testing. Such a study is crucial to provide a systematic overview of the field of A/B testing driving future research forward. To address this gap and provide an overview of the state-of-the-art in A/B testing, this paper reports the results of a systematic literature review that analyzed primary studies. The research questions focused on the subject of A/B testing, how A/B tests are designed and executed, what roles stakeholders have in this process, and the open challenges in the area. Analysis of the extracted data shows that the main targets of A/B testing are algorithms, visual elements, and workflow and processes. Single classic A/B tests are the dominating type of tests, primarily based in hypothesis tests. Stakeholders have three main roles in the design of A/B tests: concept designer, experiment architect, and setup technician. The primary types of data collected during the execution of A/B tests are product/system data, user-centric data, and spatio-temporal data. The dominating use of the test results are feature selection, feature rollout, continued feature development, and subsequent A/B test design. Stakeholders have two main roles during A/B test execution: experiment coordinator and experiment assessor. The main reported open problems are related to the enhancement of proposed approaches and their usability. From our study we derived three interesting lines for future research: strengthen the adoption of statistical methods in A/B testing, improving the process of A/B testing, and enhancing the automation of A/B testing.},
   author = {Federico Quin and Danny Weyns and Matthias Galster and Camila Costa Silva},
   doi = {https://doi.org/10.1016/j.jss.2024.112011},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {A/B test engineering,A/B testing,Systematic literature review},
   pages = {112011},
   title = {A/B testing: A systematic literature review},
   volume = {211},
   url = {https://www.sciencedirect.com/science/article/pii/S0164121224000542},
   year = {2024}
}
@article{Katsikeas2024,
   abstract = {ICT infrastructures are getting increasingly complex, and defending them against cyber attacks is cumbersome. As cyber threats continue to increase and expert resources are limited, organizations must find more efficient ways to evaluate their resilience and take proactive measures. Threat modeling is an excellent method of assessing the resilience of ICT systems, for example, by building Attack Graphs that illustrate an adversary’s attack vectors. Previously, the Meta Attack Language (MAL) was proposed, which serves as a framework to develop Domain Specific Languages (DSLs) and generate Attack Graphs for modeled infrastructures. coreLang is a MAL-based threat modeling language that utilizes Attack Graphs to enable attack simulations and security assessments. In this work, we present the first release version of coreLang in which MITRE ATT&CK tactics and techniques are mapped onto to serve as a validation and identify strengths and weaknesses to benefit the development cycle. Our validation showed that coreLang does cover 46\% of all the techniques included in the matrix, while if we additionally exclude the tactics that are intrinsically not covered by coreLang and MAL, the coverage percentage increases to 64\%.},
   author = {Sotirios Katsikeas and Andrei Buhaiu and Mathias Ekstedt and Zeeshan Afzal and Simon Hacks and Preetam Mukherjee},
   doi = {https://doi.org/10.1016/j.cose.2024.104057},
   issn = {0167-4048},
   journal = {Computers and Security},
   keywords = {Attack graphs,Cyber attack modeling,Domain specific language,ICT domain,Threat modeling},
   pages = {104057},
   title = {Development and validation of coreLang: A threat modeling language for the ICT domain},
   volume = {146},
   url = {https://www.sciencedirect.com/science/article/pii/S0167404824003626},
   year = {2024}
}
@article{GetirYaman2024,
   abstract = {A growing range of applications use AI and other autonomous agents to perform tasks that raise social, legal, ethical, empathetic, and cultural (SLEEC) concerns. To support a framework for the consideration of these concerns, we introduce SLEEC-TK, a toolkit for specification, validation, and verification of SLEEC requirements. SLEEC-TK is an Eclipse-based environment for defining SLEEC rules in a domain-specific language with a timed process algebraic semantics. SLEEC-TK uses model checking to identify redundant and conflicting rules, and to verify conformance of design models with SLEEC rules. We illustrate the use of SLEEC-TK for an assistive-care robot.},
   author = {Sinem Getir Yaman and Pedro Ribeiro and Charlie Burholt and Maddie Jones and Ana Cavalcanti and Radu Calinescu},
   doi = {https://doi.org/10.1016/j.scico.2024.103118},
   issn = {0167-6423},
   journal = {Science of Computer Programming},
   keywords = {Autonomous agents,Social,Verification and validation,empathethic and cultural requirements,ethical,legal},
   pages = {103118},
   title = {Toolkit for specification, validation and verification of social, legal, ethical, empathetic and cultural requirements for autonomous agents},
   volume = {236},
   url = {https://www.sciencedirect.com/science/article/pii/S0167642324000418},
   year = {2024}
}
@article{Holzinger2024,
   abstract = {Background and Objective:
Spinal cord injuries can have a severe impact on athletes’ or patients’ lives. High axial impact scenarios like tackling and scrummaging can cause hyperflexion and buckling of the cervical spine, which is often connected with bilateral facet dislocation. Typically, finite-element (FE) or musculoskeletal models are applied to investigate these scenarios, however, they have the drawbacks of high computational cost and lack of soft tissue information, respectively. Moreover, material properties of the involved tissues are commonly tested in quasi-static conditions, which do not accurately capture the mechanical behavior during impact scenarios. Thus, the aim of this study was to develop, calibrate and validate an approach for the creation of impact-specific hybrid, rigid body - finite-element spine models for high-dynamic axial impact scenarios.
Methods:
Five porcine cervical spine models were used to replicate in-vitro experiments to calibrate stiffness and damping parameters of the intervertebral joints by matching the kinematics of the in-vitro with the in-silico experiments. Afterwards, a five-fold cross-validation was conducted. Additionally, the von Mises stress of the lumped FE-discs was investigated during impact.
Results:
The results of the calibration and validation of our hybrid approach agree well with the in-vitro experiments. The stress maps of the lumped FE-discs showed that the highest stress of the most superior lumped disc was located anterior while the remaining lumped discs had their maximum in the posterior portion.
Conclusion:
Our hybrid method demonstrated the importance of impact-specific modeling. Overall, our hybrid modeling approach enhances the possibilities of identifying spine injury mechanisms by facilitating dynamic, impact-specific computational models.},
   author = {Thomas Holzinger and Dario Cazzola and Benedikt Sagl},
   doi = {https://doi.org/10.1016/j.cmpb.2024.108430},
   issn = {0169-2607},
   journal = {Computer Methods and Programs in Biomedicine},
   keywords = {Cervical spine,Finite-element method,Hybrid modeling,Impact-specific,Injury biomechanics,Musculoskeletal modeling},
   pages = {108430},
   title = {Development, calibration and validation of impact-specific cervical spine models: A novel approach using hybrid multibody and finite-element methods},
   volume = {257},
   url = {https://www.sciencedirect.com/science/article/pii/S0169260724004231},
   year = {2024}
}
@article{Velasco2024,
   abstract = {Developing economies often struggle with limited resources and personnel, making market validation for startups a challenge. Existing validation methods can be costly and complicated, requiring significant time and resources. This study aimed to address these challenges by developing a web-based market validation tool for startups. Results showed that the tool was well received by respondents, with the majority finding it easy to use and navigate. Additionally, respondents rated the tool as highly valuable for their startup process. This study highlights the potential benefits of web-based market validation tools for startups in developing economies, offering a cost-effective and accessible solution.},
   author = {Lemuel Clark P Velasco and Marlou Xerkxex D Rentucan and Jay Joshua M Largo and Niño Adam M Racaza},
   doi = {https://doi.org/10.1016/j.procs.2024.03.082},
   issn = {1877-0509},
   journal = {Procedia Computer Science},
   keywords = {Market Validation,Minimum Viable Product,Startup Business Company Validation Methodology,Web-based Market Validation},
   note = {Seventh Information Systems International Conference (ISICO 2023)},
   pages = {937-945},
   title = {A web-based market validation tool for the modified Startup Business Company Validation Methodology},
   volume = {234},
   url = {https://www.sciencedirect.com/science/article/pii/S187705092400440X},
   year = {2024}
}
@article{Zhang2020,
   abstract = {This paper proposes an approach to verifying context free properties of programs. In this approach, the system to be verified is modeled as a program m in Modeling, Simulation and Verification Language (MSVL), and the desired property is also specified by an MSVL program m′. Then program m and formula ¬m′ are interpreted by means of executing programs m and m′. If an acceptable execution path is generated, a counterexample is found, otherwise the property is valid. To show how the proposed approach works, an example is given.},
   author = {Nan Zhang and Zhenhua Duan and Cong Tian and Hongwei Du},
   doi = {https://doi.org/10.1016/j.tcs.2020.01.005},
   issn = {0304-3975},
   journal = {Theoretical Computer Science},
   keywords = {Algorithm,Automata,Model Checking,Runtime Verification,Temporal Logic},
   pages = {519-530},
   title = {A novel approach to verifying context free properties of programs},
   volume = {809},
   url = {https://www.sciencedirect.com/science/article/pii/S0304397520300190},
   year = {2020}
}
@article{Ng2022,
   abstract = {Stance detection identifies a person’s evaluation of a subject, and is a crucial component for many downstream applications. In application, stance detection requires training a machine learning model on an annotated dataset and applying the model on another to predict stances of text snippets. This cross-dataset model generalization poses three central questions, which we investigate using stance classification models on 7 publicly available English Twitter datasets ranging from 297 to 48,284 instances. (1) Are stance classification models generalizable across datasets? We construct a single dataset model to train/test dataset-against-dataset, finding models do not generalize well (avg F1=0.33). (2) Can we improve the generalizability by aggregating datasets? We find a multi dataset model built on the aggregation of datasets has an improved performance (avg F1=0.69). (3) Given a model built on multiple datasets, how much additional data is required to fine-tune it? We find it challenging to ascertain a minimum number of data points due to the lack of pattern in performance. Investigating possible reasons for the choppy model performance we find that texts are not easily differentiable by stances, nor are annotations consistent within and across datasets. Our observations emphasize the need for an aggregated dataset as well as consistent labels for the generalizability of models.},
   author = {Lynnette Hui Xian Ng and Kathleen M Carley},
   doi = {https://doi.org/10.1016/j.ipm.2022.103070},
   issn = {0306-4573},
   issue = {6},
   journal = {Information Processing and Management},
   keywords = {Cross validation,Machine learning,Natural language processing,Stance detection,Twitter},
   pages = {103070},
   title = {Is my stance the same as your stance? A cross validation study of stance detection datasets},
   volume = {59},
   url = {https://www.sciencedirect.com/science/article/pii/S0306457322001728},
   year = {2022}
}
@article{Kochkina2023,
   abstract = {Research on automated social media rumour verification, the task of identifying the veracity of questionable information circulating on social media, has yielded neural models achieving high performance, with accuracy scores that often exceed 90\%. However, none of these studies focus on the real-world generalisability of the proposed approaches, that is whether the models perform well on datasets other than those on which they were initially trained and tested. In this work we aim to fill this gap by assessing the generalisability of top performing neural rumour verification models covering a range of different architectures from the perspectives of both topic and temporal robustness. For a more complete evaluation of generalisability, we collect and release COVID-RV, a novel dataset of Twitter conversations revolving around COVID-19 rumours. Unlike other existing COVID-19 datasets, our COVID-RV contains conversations around rumours that follow the format of prominent rumour verification benchmarks, while being different from them in terms of topic and time scale, thus allowing better assessment of the temporal robustness of the models. We evaluate model performance on COVID-RV and three popular rumour verification datasets to understand limitations and advantages of different model architectures, training datasets and evaluation scenarios. We find a dramatic drop in performance when testing models on a different dataset from that used for training. Further, we evaluate the ability of models to generalise in a few-shot learning setup, as well as when word embeddings are updated with the vocabulary of a new, unseen rumour. Drawing upon our experiments we discuss challenges and make recommendations for future research directions in addressing this important problem.},
   author = {Elena Kochkina and Tamanna Hossain and Robert L Logan and Miguel Arana-Catania and Rob Procter and Arkaitz Zubiaga and Sameer Singh and Yulan He and Maria Liakata},
   doi = {https://doi.org/10.1016/j.ipm.2022.103116},
   issn = {0306-4573},
   issue = {1},
   journal = {Information Processing and Management},
   keywords = {Deep learning,Generalisability,Rumour dataset,Rumour verification},
   pages = {103116},
   title = {Evaluating the generalisability of neural rumour verification models},
   volume = {60},
   url = {https://www.sciencedirect.com/science/article/pii/S0306457322002175},
   year = {2023}
}
@article{Liu2024-6,
   abstract = {Table-based fact verification focuses on determining the truthfulness of statements by cross-referencing data in tables. This task is challenging due to the complex interactions inherent in table structures. To address this challenge, existing methods have devised a range of specialized models. Although these models demonstrate good performance, they still exhibit limited sensitivity to essential variations in information relevant to reasoning within both statements and tables, thus learning spurious patterns and leading to potentially unreliable outcomes. In this work, we propose a novel approach named Multi-Aspect Adversarial Contrastive Learning (Macol), aimed at enhancing the accuracy and robustness of table-based fact verification systems under the premise of resource efficiency. Specifically, we first extract pivotal logical reasoning clues to construct positive and adversarial negative instances for contrastive learning. We then propose a new training paradigm that introduces a contrastive learning objective, encouraging the model to recognize noise invariance between original and positive instances while also distinguishing logical differences between original and negative instances. Extensive experimental results on three widely studied datasets TABFACT, INFOTABS and SEM-TAB-FACTS demonstrate that Macol achieves state-of-the-art performance on benchmarks across various backbone architectures, with accuracy improvements reaching up to 5.4\%. Furthermore, Macol exhibits significant advantages in robustness and low-data resource scenarios, with improvements of up to 8.2\% and 9.1\%. It is worth noting that our method achieves comparable or even superior performance while being more resource-efficient compared to approaches that employ specific additional pre-training or utilize large language models (LLMs).},
   author = {Ruiheng Liu and Yu Zhang and Bailong Yang and Qi Shi and Luogeng Tian},
   doi = {https://doi.org/10.1016/j.ipm.2024.103853},
   issn = {0306-4573},
   issue = {6},
   journal = {Information Processing and Management},
   keywords = {Adversarial attack,Contrastive learning,Fact verification,Pre-trained language model,Semi-structured data},
   pages = {103853},
   title = {Robust and resource-efficient table-based fact verification through multi-aspect adversarial contrastive learning},
   volume = {61},
   url = {https://www.sciencedirect.com/science/article/pii/S0306457324002127},
   year = {2024}
}
@article{Zhang2024,
   abstract = {Accurate collaborative positioning by dual robots is crucial for achieving high precision in dual-robot mirror milling operations. However, the tool frames constructed in conventional dual-robot coordinate calibration approaches need to be substituted by a new set designated for the mirror milling, which may induce a notable reduction of collaborative positioning accuracy. This paper presents a novel collaborative coordinate calibration technique designed explicitly for the dual-robot mirror milling system with a measurement module, avoiding the need for subsequent tool frame replacement for mirror milling. The proposed method starts from designing a multi-probe ultrasonic measurement module on the mirror milling system's supporting head and developing an online workpiece pose identification technique by using the ultrasonic measurement. A specially designed mirror milling test is then implemented to correlate the dual-robot collaborative pose information with the pose daviation observed between the machined and ideal design features. By identifying the machined feature's pose, we realize a high-precision dual-robot coordinate calibration approach that ideally suites for the dual-robot mirror milling system equipped with a measurement module. The effectiveness and advantage of proposed method has been validated through a set of comparative experiments. Experiment results show that the proposed method obviously outperforms the benchmarking dual-robot coordinate calibration method reliant on laser tracker technology.},
   author = {Yang Zhang and Peng Guo and Nuodi Huang and Yaqi Zhang and Limin Zhu},
   doi = {https://doi.org/10.1016/j.rcim.2024.102749},
   issn = {0736-5845},
   journal = {Robotics and Computer-Integrated Manufacturing},
   keywords = {Dual-robot coordinate calibration,Milling test,Mirror milling,Multi-probe ultrasonic measurement},
   pages = {102749},
   title = {A milling test-based coordinate calibration approach for the dual-robot mirror milling system with a measurement module},
   volume = {89},
   url = {https://www.sciencedirect.com/science/article/pii/S0736584524000358},
   year = {2024}
}
@article{Corlatescu2024,
   abstract = {Modeling reading comprehension processes is a critical task for Learning Analytics, as accurate models of the reading process can be used to match students to texts, identify appropriate interventions, and predict learning outcomes. This paper introduces an improved version of the Automated Model of Comprehension, namely version 4.0. AMoC has its roots in two theoretical models of the comprehension process (i.e., the Construction-Integration model and the Landscape model), and the new version leverages state-of-the-art Large Language models, more specifically ChatGPT, to have a better contextualization of the text and a simplified construction of the underlying graph model. Besides showcasing the usage of the model, the study introduces three in-depth psychological validations that argue for the model's adequacy in modeling reading comprehension. In these studies, we demonstrated that AMoC is in line with the theoretical background proposed by the Construction-Integration and Landscape models, and it is better at replicating results from previous human psychological experiments than its predecessor. Thus, AMoC v4.0 can be further used as an educational tool to, for example, help teachers design better learning materials personalized for student profiles. Additionally, we release the code from AMoC v4.0 as open source in a Google Collab Notebook and a GitHub repository.},
   author = {Dragos-Georgian Corlatescu and Micah Watanabe and Stefan Ruseti and Mihai Dascalu and Danielle S McNamara},
   doi = {https://doi.org/10.1016/j.chb.2024.108154},
   issn = {0747-5632},
   journal = {Computers in Human Behavior},
   keywords = {Automated model of comprehension,ChatGPT,Large language models,Natural language processing,Reading comprehension},
   pages = {108154},
   title = {The automated model of comprehension version 4.0 – Validation studies and integration of ChatGPT},
   volume = {154},
   url = {https://www.sciencedirect.com/science/article/pii/S0747563224000219},
   year = {2024}
}
@article{Peng2021,
   abstract = {Machine reading comprehension (MRC) is a challenging task in natural language processing (NLP), which requires machine to determine the corresponding answer to a given passage and question. Whereas, there always exist unanswerable questions in the real world, which poses a new challenge to MRC tasks. Abundant research work has been carried out on the verification mechanism for the answerability of passage-question pairs. However, these researches only focus on its design and implementation, which has limitations in real-world scenarios. Thus, the method proposed in this paper not only verifies the answerability, but also validates and adjusts the predicted answer to obtain an elaborate answer span. Using powerful pre-trained model as encoder block, this paper explores a more comprehensive verification mechanism. Similar to how humans read passages and give answers, we propose a three-stage mechanism called ”Verification for an Elaborate Span” (V4ES): 1) sketchy reading that the model briefly browses the overall information of the passage and question, and then generates an initial answer; 2) intensive reading that it reads the passage and question again, judges the answerability of the question and gives an answer at this stage; 3) verification that it verifies these two answers produced at the previous two stages, and then gives the final prediction. Moreover, the proposed model is evaluated on two MRC challenge datasets: SQuAD2.0 and CMRC2018, and the experiment results show that our model has achieved great improvement compared with the ALBERT and BERT baselines. In conclusion, our proposed verification mechanism has demonstrated its effectiveness through a series of experiments and analysis.},
   author = {Yu Peng and Xiaoyu Li and Jingkuan Song and Yu Luo and Shijie Hu and Weizhong Qian},
   doi = {https://doi.org/10.1016/j.neucom.2021.08.084},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Answer verification,Machine reading comprehension,Question answering},
   pages = {80-91},
   title = {Verification mechanism to obtain an elaborate answer span in machine reading comprehension},
   volume = {466},
   url = {https://www.sciencedirect.com/science/article/pii/S0925231221012819},
   year = {2021}
}
@article{Chen2024-8,
   abstract = {As online rumors have the potential to greatly affect areas such as social order, stock prices, and presidential elections, there is an emerging necessity for the automation of rumor verification. Although the current methods have achieved satisfactory performance, they still suffer from the following problems. First, the current methods simply concatenate the representations of different subthreads in their models, which may result in omitting some important information. Second, although stance information has been considered for the rumor verification task, it has not been fully utilized. To solve the problems, we propose the Subthreads Stance–Rumor Interaction Network (SSRI-Net) model for rumor verification. The proposed SSRI-Net model first introduces the Subthreads Interaction Attention mechanism between different subthreads to capture the interaction information between subthreads for a better understanding on user posts. Moreover, we also design the Stance–Rumor Interaction Network to fully integrate users’ stance information with rumor verification. We have conducted experiments on two public datasets, namely SemEval-2017 and PHEME datasets, for performance evaluation. Our SSRI-Net model outperforms the previous best models by 5.8\% and 7.1\% in Macro-F1 and Accuracy respectively on the SemEval-2017 dataset. In addition, our SSRI-Net model also outperforms the previous best models by 4.7\% and 5.4\% in Macro-F1 and Accuracy respectively on the PHEME dataset. The experimental results have shown that our proposed SSRI-Net model has outperformed the baseline models and achieved the state-of-the-art performance for rumor verification.},
   author = {Zhendong Chen and Siu Cheung Hui and Lejian Liao and Heyan Huang},
   doi = {https://doi.org/10.1016/j.neucom.2024.127549},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Rumor verification,Stance–Rumor Interaction,Subthreads Interaction Attention},
   pages = {127549},
   title = {SSRI-Net: Subthreads Stance–Rumor Interaction Network for rumor verification},
   volume = {583},
   url = {https://www.sciencedirect.com/science/article/pii/S0925231224003205},
   year = {2024}
}
@article{PradoLima2020,
   abstract = {Context: Continuous Integration (CI) environments allow frequent integration of software changes, making software evolution more rapid and cost-effective. In such environments, the regression test plays an important role, as well as the use of Test Case Prioritization (TCP) techniques. Such techniques attempt to identify the test case order that maximizes certain goals, such as early fault detection. This research subject has been raising interest because some new challenges are faced in the CI context, as TCP techniques need to consider time constraints of the CI environments. Objective: This work presents the results of a systematic mapping study on Test Case Prioritization in Continuous Integration environments (TCPCI) that reports the main characteristics of TCPCI approaches and their evaluation aspects. Method: The mapping was conducted following a plan that includes the definition of research questions, selection criteria and search string, and the selection of search engines. The search returned 35 primary studies classified based on the goal and kind of used TCP technique, addressed CI particularities and testing problems, and adopted evaluation measures. Results: The results show a growing interest in this research subject. Most studies have been published in the last four years. 80\% of the approaches are history-based, that is, are based on the failure and test execution history. The great majority of studies report evaluation results by comparing prioritization techniques. The preferred measures are Time and number/percentage of Faults Detected. Few studies address CI testing problems and characteristics, such as parallel execution and test case volatility. Conclusions: We observed a growing number of studies in the field. Future work should explore other information sources such as models and requirements, as well as CI particularities and testing problems, such as test case volatility, time constraint, and flaky tests, to solve existing challenges and offer cost-effective approaches to the software industry.},
   author = {Jackson A Prado Lima and Silvia R Vergilio},
   doi = {https://doi.org/10.1016/j.infsof.2020.106268},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Continuous Integration,Software testing,Test Case Prioritization},
   pages = {106268},
   title = {Test Case Prioritization in Continuous Integration environments: A systematic mapping study},
   volume = {121},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584920300185},
   year = {2020}
}
@article{Garousi2020,
   abstract = {Context
To reduce manual effort of extracting test cases from natural-language requirements, many approaches based on Natural Language Processing (NLP) have been proposed in the literature. Given the large amount of approaches in this area, and since many practitioners are eager to utilize such techniques, it is important to synthesize and provide an overview of the state-of-the-art in this area.
Objective
Our objective is to summarize the state-of-the-art in NLP-assisted software testing which could benefit practitioners to potentially utilize those NLP-based techniques. Moreover, this can benefit researchers in providing an overview of the research landscape.
Method
To address the above need, we conducted a survey in the form of a systematic literature mapping (classification). After compiling an initial pool of 95 papers, we conducted a systematic voting, and our final pool included 67 technical papers.
Results
This review paper provides an overview of the contribution types presented in the papers, types of NLP approaches used to assist software testing, types of required input requirements, and a review of tool support in this area. Some key results we have detected are: (1) only four of the 38 tools (11\%) presented in the papers are available for download; (2) a larger ratio of the papers (30 of 67) provided a shallow exposure to the NLP aspects (almost no details).
Conclusion
This paper would benefit both practitioners and researchers by serving as an “index” to the body of knowledge in this area. The results could help practitioners utilizing the existing NLP-based techniques; this in turn reduces the cost of test-case design and decreases the amount of human resources spent on test activities. After sharing this review with some of our industrial collaborators, initial insights show that this review can indeed be useful and beneficial to practitioners.},
   author = {Vahid Garousi and Sara Bauer and Michael Felderer},
   doi = {https://doi.org/10.1016/j.infsof.2020.106321},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Natural Language Processing (NLP),Software testing,Systematic literature mapping,Systematic literature review},
   pages = {106321},
   title = {NLP-assisted software testing: A systematic mapping of the literature},
   volume = {126},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584920300744},
   year = {2020}
}
@article{Khaliq2022,
   abstract = {Context:
The use of automation tools in software testing helps keep pace with the timeline of the deliverables. Over time with the inclusion of continuous integration/continuous delivery (CI/CD) pipelines, automation tools are becoming less effective. The testing community is turning to AI to help keep the pace.
Objective:
We study the use of transformers to automate the process of test case generation directly from the User Interface (UI) element description instead of relying on the test specification document from which test cases are extracted manually. We also demonstrate the capability of the proposed approach in repairing flaky tests.
Method:
We employ object detection algorithms EfficientDet and DEtectionTRansformer for detecting the elements from an application UI automatically without requiring a tester to locate complex-scripted UI elements. We also use Tesseract to automatically identify the text present on the UI elements. We transform the generated UI element description to actual test designer-written test cases using text-generation transformers like GPT-2 and T5. The generated test cases are then translated into executable test scripts using a simple parser. We carry out our cases study on 30 e-commerce applications.
Results:
The percentage of correct executable test cases generated by the framework employing EfficientDet is 93.82\% and employing DEtectionTRansformer is 98.08\%. The framework eliminates an average of 96.05\% flakiness across the applications selected for the study.
Conclusion:
It is concluded that the proposed approach can be used with current automation tools in the industry to enhance their capability in generating test cases and repairing the flaky tests.},
   author = {Zubair Khaliq and Sheikh Umar Farooq and Dawood Ashraf Khan},
   doi = {https://doi.org/10.1016/j.infsof.2022.106969},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Automated testing,Deep learning,Software testing,Transformers,UI functional testing},
   pages = {106969},
   title = {A deep learning-based automated framework for functional User Interface testing},
   volume = {150},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584922001070},
   year = {2022}
}
@article{Gomez-Abajo2023,
   abstract = {Context:
Testing is essential to improve the correctness of software systems. Metamorphic testing (MT) is an approach especially suited when the system under test lacks oracles, or they are expensive to compute. However, building an MT environment for a particular domain (e.g., cloud simulation, model transformation, machine learning) requires substantial effort.
Objective:
Our goal is to facilitate the construction of MT environments for specific domains.
Method:
We propose a model-driven engineering approach to automate the construction of MT environments. Starting from a meta-model capturing the domain concepts, and a description of the domain execution environment, our approach produces an MT environment featuring comprehensive support for the MT process. This includes the definition of domain-specific metamorphic relations, their evaluation, detailed reporting of the testing results, and the automated search-based generation of follow-up test cases.
Results:
Our method is supported by an extensible platform for Eclipse, called Gotten. We demonstrate its effectiveness by creating an MT environment for simulation-based testing of data centres and comparing with existing tools; its suitability to conduct MT processes by replicating previous experiments; and its generality by building another MT environment for video streaming APIs.
Conclusion:
Gotten is the first platform targeted at reducing the development effort of domain-specific MT environments. The environments created with Gotten facilitate the specification of metamorphic relations, their evaluation, and the generation of new test cases.},
   author = {Pablo Gómez-Abajo and Pablo C Cañizares and Alberto Núñez and Esther Guerra and Juan de Lara},
   doi = {https://doi.org/10.1016/j.infsof.2023.107164},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Cloud computing,Domain-specific languages,Metamorphic testing,Model-driven engineering,Simulation},
   pages = {107164},
   title = {Automated engineering of domain-specific metamorphic testing environments},
   volume = {157},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584923000186},
   year = {2023}
}
@article{Zhao2023,
   abstract = {Context:
Crowdsourced testing can reduce testing costs and improve testing efficiency. However, crowdsourced testing generates massive test cases, requiring testers to select high-quality test cases for execution. Consequently, crowdsourced test cases require much effort to perform labeling due to the costly manual labor and domain knowledge.
Objective:
Existing methods usually fail to consider the crowdsourced testing scenario’s inadequate and imbalanced data issues. We aim to effectively and efficiently classify many crowdsourced test cases for developers to alleviate manual efforts.
Method:
In this paper, we propose a test case classification approach based on few-shot learning and test case augmentation to address the limitations mentioned above. The proposed approach generates new test cases by the large pre-trained masked language model and extracts embedding representation by training word embedding models. Then a Bidirectional Long Short-Term Memory (BiLSTM)-based classifier is designed to perform test case classification by extracting the in-depth features. Besides, we also apply the attention mechanism to assign high weights to words that represent the test case category by lexicon matching.
Results:
To verify the effectiveness of the classification framework, we select 1659 test cases from three crowdsourced testing projects to conduct in-usability evaluation experiments. The experimental results show that the proposed approach has a higher accuracy and precision rate than existing classification methods.
Conclusion:
It can be concluded that (1) the proposed approach is an effective test case classification technique for crowdsourced testing; (2) the proposed approach is practical to help developers select high-quality test cases quickly and effectively.},
   author = {Yuan Zhao and Sining Liu and Quanjun Zhang and Xiuting Ge and Jia Liu},
   doi = {https://doi.org/10.1016/j.infsof.2023.107228},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Attention mechanism,Few-shot learning,Test case classification},
   pages = {107228},
   title = {Test case classification via few-shot learning},
   volume = {160},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584923000824},
   year = {2023}
}
@article{Mndez2023,
   abstract = {Context:
Chess engines are computer programs that analyse chess positions. The goal of this analysis is to decide which player has an advantage and evaluate how big the advantage is. Using this analysis, chess engines are really powerful players who can consistently beat the best (human) players. Even though these programs are fantastic players, we cannot be sure that the code is fault free because it is very difficult to test them. In particular, we face the oracle problem: if the chess engine plays better than any potential tester, how can a tester claim that a certain evaluation is wrong or that a suggested move is not the best one?
Objective:
The main goal of our work is to provide a metamorphic testing tool to evaluate chess engines. In particular, we are interested in looking for inconsistent behaviours in the best publicly available chess engine, Stockfish, but we would also like to consider other chess engines.
Methods:
We developed a metamorphic testing solution to validate chess engines. First, we defined metamorphic relations that might reveal inconsistent behaviours. The underlying idea was that the evaluation of related positions should be the same. For example, if we consider a position and rotate all the pieces with respect to the central axis, then both positions should have the same evaluation. One of our main priorities was to have a fully automatised tool. Source inputs are obtained from available datasets while follow-up inputs are automatically computed by applying sound transformations to the source inputs with respect to the corresponding metamorphic rule. In order to assess the usefulness of our work, we applied it to analyse a dataset with more than 40,000 positions.
Results:
Empirical evidence validates the usefulness of our work to analyse the best available chess engine, Stockfish. Our tool revealed non-negligible deviations from the expected behaviour in Stockfish for all the MRs. Additional experiments showed that our tool can be easily used to analyse other chess engines such as Komodo, Houdini and Gull.
Conclusion:
The experiments demonstrate the usefulness of our approach to identify issues in the latest version of the widely recognised to be the best chess engine: Stockfish (version 15, released in April 2022). Our tool is flexible and can be easily extended with metamorphic relations that can be defined in the future by either us or other users. Since all our metamorphic relations are implemented and the code is freely available, users can use them as a pattern to implement new relations.},
   author = {Manuel Méndez and Miguel Benito-Parejo and Alfredo Ibias and Manuel Núñez},
   doi = {https://doi.org/10.1016/j.infsof.2023.107263},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Chess engines,Metamorphic testing,Software testing},
   pages = {107263},
   title = {Metamorphic testing of chess engines},
   volume = {162},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584923001179},
   year = {2023}
}
@article{Kuroishi2024,
   abstract = {Context:
Mobile application testing has gained considerable attention in recent years since mobile devices have become increasingly present in our lives. Unlike traditional software, mobile application testing has to deal with peculiarities, such as screen size and densities, different operating systems, and multiple sensors that increase the complexity of testing.
Objective:
This paper summarizes and analyzes the current secondary studies on mobile application testing through a tertiary study.
Method:
We selected and analyzed 21 secondary studies related to mobile application testing.
Results:
We categorized 21 secondary studies according to their main and specific research topics, test objectives, and testing platforms. Furthermore, we analyze 87 gaps and challenges identified by the secondary studies to understand which gaps have already been addressed and which gaps are still uncovered.
Conclusion:
Based on the results, we propose a research agenda with 15 open challenges related to mobile application testing to help future research.},
   author = {Pedro Henrique Kuroishi and José Carlos Maldonado and Auri Marcelo Rizzo Vincenzi},
   doi = {https://doi.org/10.1016/j.infsof.2023.107363},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Mobile application,Research agenda,Tertiary study,Testing},
   pages = {107363},
   title = {Towards the definition of a research agenda on mobile application testing based on a tertiary study},
   volume = {167},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584923002185},
   year = {2024}
}
@article{Dakhel2024,
   abstract = {Context:
One of the critical phases in the software development life cycle is software testing. Testing helps with identifying potential bugs and reducing maintenance costs. The goal of automated test generation tools is to ease the development of tests by suggesting efficient bug-revealing tests. Recently, researchers have leveraged Large Language Models (LLMs) of code to generate unit tests. While the code coverage of generated tests was usually assessed, the literature has acknowledged that the coverage is weakly correlated with the efficiency of tests in bug detection.
Objective:
To improve over this limitation, in this paper, we introduce MuTAP (Mutation Test case generation using Augmented Prompt) for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing.
Methods:
Our goal is achieved by augmenting prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs. MuTAP is capable of generating effective test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate their performance on different benchmarks.
Results:
Our results show that our proposed method is able to detect up to 28\% more faulty human-written code snippets. Among these, 17\% remained undetected by both the current state-of-the-art fully-automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning approaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57\% on synthetic buggy code, outperforming all other approaches in our evaluation.
Conclusion:
Our findings suggest that although LLMs can serve as a useful tool to generate test cases, they require specific post-processing steps to enhance the effectiveness of the generated test cases which may suffer from syntactic or functional errors and may be ineffective in detecting certain types of bugs and testing corner cases in PUTs.},
   author = {Arghavan Moradi Dakhel and Amin Nikanjam and Vahid Majdinasab and Foutse Khomh and Michel C Desmarais},
   doi = {https://doi.org/10.1016/j.infsof.2024.107468},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Large language model,Mutation testing,Test generation},
   pages = {107468},
   title = {Effective test generation using pre-trained Large Language Models and mutation testing},
   volume = {171},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584924000739},
   year = {2024}
}
@article{Alagarsamy2024,
   abstract = {Context:
Test case generation is a critical yet challenging task in software development. Recently, AthenaTest – a Deep Learning (DL) approach for generating unit test cases has been proposed. However, our revisiting study reveals that AthenaTest can generate less than one-fifth of the test cases correctly, due to a lack of assertion knowledge and test signature verification.
Objective:
This paper introduces A3Test, a novel DL-based approach to the generation of test cases, enhanced with assertion knowledge and a mechanism to verify consistency of the name and signatures of the tests. A3Test aims to adapt domain knowledge from assertion generation to test case generation.
Method:
A3Test employs domain adaptation principles and introduces a verification approach to name consistency and test signatures. We evaluate its effectiveness using 5,278 focal methods from the Defects4j dataset.
Results:
Our findings indicate that A3Test outperforms AthenaTest and ChatUniTest. A3Test generates 2.16\% to 395.43\% more correct test cases, achieves 2.17\% to 34.29\% higher method coverage, and 25.64\% higher line coverage. A3Test achieves 2.13\% to 12.20\% higher branch coverage, 2.22\% to 12.20\% higher mutation scores, and 2.44\% to 55.56\% more correct assertions compared to both ChatUniTest and AthenaTest respectively for one iteration. When generating multiple test cases per method A3Test still shows improvements and comparable efficacy to ChatUnitTest. A survey of developers reveals that the majority of the participants 70.51\% agree that test cases generated by A3Test are more readable than those generated by EvoSuite.
Conclusions:
A3Test significantly enhances test case generation through its incorporation of assertion knowledge and test signature verification, contributing to the generation of correct test cases.},
   author = {Saranya Alagarsamy and Chakkrit Tantithamthavorn and Aldeida Aleti},
   doi = {https://doi.org/10.1016/j.infsof.2024.107565},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Deep learning,Test case generation},
   pages = {107565},
   title = {A3Test: Assertion-Augmented Automated Test case generation},
   volume = {176},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584924001708},
   year = {2024}
}
@article{Khaliq2023,
   abstract = {The application of test cases for detecting the faults within the software is called software testing. Manual testing is laborious and time-consuming hence automation tools to test software were introduced. Despite the use of automation tools at the User Interface (UI) level of the test pyramid, the limitations of current automation tools like automated test case generation and automated repairing of fragile tests still force us to carry out a large amount of manual testing. In this paper, we propose a novel method using AI to address the given challenges. With our proposed method test cases are automatically generated from the structure of the UI using a pipelined architecture of object detection, text detection and NLP models. We show that the test cases generated by the proposed framework can be translated into executable test scripts using a simple parser. The proposed method generates an average of 98.8\% correct executable test cases for the applications under study. We also show the capability of the proposed method in generating new tests automatically when the application is modified. The proposed method generates an average of 98.605\% correct executable test cases when the UI is modified for the applications under study. We also empirically prove that a GPU implementation of the proposed framework results in just an additional average runtime of 0.92 seconds per test case which is significantly low given the benefits of automated generation of test scripts and automated repairing of fragile tests.},
   author = {Zubair Khaliq and Dawood Ashraf Khan and Sheikh Umar Farooq},
   doi = {https://doi.org/10.1016/j.engappai.2022.105446},
   issn = {0952-1976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Automated testing,Deep learning,Software testing,Transformers,UI functional testing},
   pages = {105446},
   title = {Using deep learning for selenium web UI functional tests: A case-study with e-commerce applications},
   volume = {117},
   url = {https://www.sciencedirect.com/science/article/pii/S0952197622004365},
   year = {2023}
}
@article{Wei2024,
   abstract = {The development of the industrial Internet of Things enables industrial control systems to become inter-networked and inter-connected, making them intelligent with high productivity. However, these systems are exposed to external environments and vulnerable to network attacks, which also suffer from internal vulnerabilities. Fuzz testing, in short fuzzing, is a technique to enhance the security of industrial control systems by finding errors when repeatedly executing software that injects illegal, malformed, or unexpected inputs into the systems. Unfortunately, traditional fuzzing of communication protocols faces low coverage and efficiency problems when being applied to industrial protocols, considering the characteristics of industrial protocols such as real-time and multi-interaction. Moreover, fuzzing is difficult to perform because many structures of industrial control protocols are not publicly available. Although researchers have started to focus on the fuzzing of industrial control protocols, existing literature still lacks a thorough survey of its recent advances. To fill this gap, we conduct a comprehensive survey on existing fuzzing methods for industrial control protocols. After a brief introduction to industrial control protocols and fuzzing, we propose a set of metrics for judging the pros and cons of existing fuzzing methods. Based on these metrics, we evaluate and compare the performance of fuzzing methods of industrial control protocols in the past eight years. Based on our review and analysis, we further summarize the open problems of these methods for achieving the proposed metrics and elaborate on future research directions toward secure industrial control systems.},
   author = {Xiaoyan Wei and Zheng Yan and Xueqin Liang},
   doi = {https://doi.org/10.1016/j.jnca.2024.104020},
   issn = {1084-8045},
   journal = {Journal of Network and Computer Applications},
   keywords = {Fuzzing,Industrial control protocols,Industrial security,Vulnerability detection},
   pages = {104020},
   title = {A survey on fuzz testing technologies for industrial control protocols},
   volume = {232},
   url = {https://www.sciencedirect.com/science/article/pii/S1084804524001978},
   year = {2024}
}
@article{Aldahmash2024,
   abstract = {The advent of online social networks (OSNs) has catalyzed the formation of novel learning communities. Identifying experts within OSNs has become a critical component for facilitating knowledge exchange and enhancing self-awareness, particularly in contexts such as rumor verification processes. Research efforts aimed at locating authorities in OSNs are scant, largely due to the scarcity of annotated datasets. This work represents a contribution to the domain of unsupervised learning to address the challenge of authorities’ identification in Twitter. We have employed advanced natural language processing technique to transfer knowledge concerning topics in the Arabic language and to discern the semantic connections among candidates within Twitter in zero-shot learning. We take advantage of the Single-labeled Arabic News Articles Dataset (SANAD) to perform the process of extracting domain features and applying these features in finding authorities using the Authority Finding in Arabic Twitter (AuFIN) dataset. Our evaluation assessed the extent of extracted topical features transferred and the efficacy of authorities’ retrieval in comparison to the latest unsupervised models in this domain. Our approach successfully extracted and integrated the limited available topical semantic features of the language into the representation of candidates. The findings indicate that our hybrid model surpasses those that rely solely on lexical features of language and network topology, as well as other contemporary approaches to topic-specific expert finding.},
   author = {Hend Aldahmash and Abdulrahman Alothaim and Abdulrahman Mirza},
   doi = {https://doi.org/10.1016/j.jksuci.2024.102111},
   issn = {1319-1578},
   issue = {6},
   journal = {Journal of King Saud University - Computer and Information Sciences},
   keywords = {Arabic tweets,Expert finding,Network semantics,Rumor,Social media,Transfer learning},
   pages = {102111},
   title = {Rumor gatekeepers: Unsupervised ranking of Arabic twitter authorities for information verification},
   volume = {36},
   url = {https://www.sciencedirect.com/science/article/pii/S1319157824002003},
   year = {2024}
}
@article{Yang2021,
   abstract = {The safety-critical system communities have been struggling to manage and maintain their legacy softwaresystems because upgrading such systems has been a complex challenge. To overcome or reduce this problem, reverse engineering has been increasingly used in safety-critical systems. This paper proposes C2AADL_Reverse, a model-driven reverse engineering approach for safety-critical software development and verification. C2AADL_Reverse takes multi-task C source code as input, and generates AADL (Architecture Analysis and Design Language) model of the legacy software systems. Compared with the existing works, this paper considers more reversed construction including AADL component structure, behavior, and multi-threaded run-time information. Moreover, two types of activities are proposed to ensure the correctness of C2AADL_Reverse. First, it is necessary to validate the reverse engineering process. Second, the generated AADL models should conform to desired critical properties. We propose the verification of the reverse-engineered AADL model by using UPPAAL to establish component-level properties and the Assume Guarantee REasoning Environment (AGREE) to perform compositional verification of the architecture. This combination of verification tools allows us to iteratively explore design and verification of detailed behavioral models, and to scale formal analysis to large models. In addition, the prototype tool and the evaluation of C2AADL_Reverse using a real-world aerospace case study are presented.},
   author = {Zhibin Yang and Zhikai Qiu and Yong Zhou and Zhiqiu Huang and Jean-Paul Bodeveix and Mamoun Filali},
   doi = {https://doi.org/10.1016/j.sysarc.2021.102202},
   issn = {1383-7621},
   journal = {Journal of Systems Architecture},
   keywords = {AADL,Compositional verification,Model-driven development,Model-driven reverse engineering,Safety-critical systems},
   pages = {102202},
   title = {C2AADL_Reverse: A model-driven reverse engineering approach to development and verification of safety-critical software},
   volume = {118},
   url = {https://www.sciencedirect.com/science/article/pii/S1383762121001454},
   year = {2021}
}
@article{Siddiqi2024,
   abstract = {Background
Gaps in information access impede immunization uptake, especially in low-resource settings where cutting-edge and innovative digital interventions are limited given the digital inequity. Our objective was to develop an Artificially Intelligent (AI) chatbot to respond to caregiver’s immunization-related queries in Pakistan and investigate its feasibility and acceptability in a low-resource, low-literacy setting.
Methods
We developed Bablibot (Babybot), a local language immunization chatbot, using Natural Language Processing (NLP) and Machine Learning (ML) technologies with Human in the Loop feature. We evaluated the bot through a sequential mixed-methods study. We enrolled caregivers visiting the 12 selected immunization centers for routine childhood vaccines. Additional caregivers were reached through targeted text message communication. We assessed Bablibot’s feasibility and acceptability by tracking user engagement and technological metrics, and through thematic analysis of in-depth interviews with 20 caregivers.
Findings
Between March 9, 2020, and April 15, 2021, 2,202 caregivers were enrolled in the study, of which, 677 (30.7\%) interacted with Bablibot (users). Bablibot responded to 1,877 messages through 874 conversations. Conversation topics included vaccination due dates (32.4\%; 283/874), side-effect management (15.7\%;137/874), or delaying vaccination due to child’s illness or COVID-lockdown (16.8\%;147/874). Over 90\% (277/307) of responses to text-based exit surveys indicated satisfaction with Bablibot. Qualitative analysis showed caregivers appreciated Bablibot’s usefulness and provided feedback for further improvement of the system.
Conclusion
Our results demonstrate the feasibility and acceptability of local-language NLP chatbots in providing real-time immunization information in low-resource settings. Text-based chatbots canminimize the workload on helpline operators, in addition to instantaneously resolving caregiver queries that otherwise lead to delay or default.},
   author = {Danya Arif Siddiqi and Fatima Miraj and Humdiya Raza and Owais Ahmed Hussain and Mehr Munir and Vijay Kumar Dharma and Mubarak Taighoon Shah and Ali Habib and Subhash Chandir},
   doi = {https://doi.org/10.1016/j.ijmedinf.2023.105288},
   issn = {1386-5056},
   journal = {International Journal of Medical Informatics},
   keywords = {Artificial intelligence,Chatbot,Childhood immunization,Natural language processing},
   pages = {105288},
   title = {Development and feasibility testing of an artificially intelligent chatbot to answer immunization-related queries of caregivers in Pakistan: A mixed-methods study},
   volume = {181},
   url = {https://www.sciencedirect.com/science/article/pii/S1386505623003064},
   year = {2024}
}
@article{Zhou2024,
   abstract = {Edge devices, due to their limited computational and storage resources, often require the use of compilers for program optimization. Therefore, ensuring the security and reliability of these compilers is of paramount importance in the emerging field of edge AI. One widely used testing method for this purpose is fuzz testing, which detects bugs by inputting random test cases into the target program. However, this process consumes significant time and resources. To improve the efficiency of compiler fuzz testing, it is common practice to utilize test case prioritization techniques. Some researchers use machine learning to predict the code coverage of test cases, aiming to maximize the test capability for the target compiler by increasing the overall predicted coverage of the test cases. Nevertheless, these methods can only forecast the code coverage of the compiler at a specific optimization level, potentially missing many optimization-related bugs. In this paper, we introduce C-CORE (short for Clustering by Code Representation), the first framework to prioritize test cases according to their code representations, which are derived directly from the source codes. This approach avoids being limited to specific compiler states and extends to a broader range of compiler bugs. Specifically, we first train a scaled pre-trained programming language model to capture as many common features as possible from the test cases generated by a fuzzer. Using this pre-trained model, we then train two downstream models: one for predicting the likelihood of triggering a bug and another for identifying code representations associated with bugs. Subsequently, we cluster the test cases according to their code representations and select the highest-scoring test case from each cluster as the high-quality test case. This reduction in redundant testing cases leads to time savings. Comprehensive evaluation results reveal that code representations are better at distinguishing test capabilities, and C-CORE significantly enhances testing efficiency. Across four datasets, C-CORE increases the average of the percentage of faults detected (APFD) value by 0.16 to 0.31 and reduces test time by over 50\% in 46\% of cases. When compared to the best results from approaches using predicted code coverage, C-CORE improves the APFD value by 1.1\% to 12.3\% and achieves an overall time-saving of 159.1\%.},
   author = {Wei Zhou and Xincong Jiang and Chuan Qin},
   doi = {https://doi.org/10.32604/cmes.2023.043248},
   issn = {1526-1492},
   issue = {2},
   journal = {CMES - Computer Modeling in Engineering and Sciences},
   keywords = {Compiler testing,code representation,test case prioritization},
   pages = {2069-2093},
   title = {C-CORE: Clustering by Code Representation to Prioritize Test Cases in Compiler Testing},
   volume = {139},
   url = {https://www.sciencedirect.com/science/article/pii/S1526149224001930},
   year = {2024}
}
@article{Malhotra2024,
   abstract = {Software change prediction plays key role for maintaining software quality. Identification of change prone parts of a software in early stages helps in optimization of resources and amount of effort required for the maintenance of software. The change prediction model can be built by using the same project for training and testing, which is termed as within-project change prediction. Sometimes, the available data is not sufficient for both training and testing, in such cases, it is beneficial to use other projects for testing. Usage of two different projects for training and testing for identification of changes is termed as Cross-Project Change Prediction (CPCP). In CPCP, the data distribution of features differentiates. The scalability of prediction models can be increased by using the two different projects for training and testing with different features, which is termed as Heterogeneous Cross-Project Change Prediction (HCPCP). Thus, the usability of such prediction models increases for future projects with unseen data. In this study, the feasibility of HCPCP and WPCP analyzed for open-source projects. HCPCP used feature-type Transfer Learning (TL) in this study. The analysis of HCPCP and CPCP is accomplished for open-source projects to enhance software quality such as maintainability, reliability, and robustness. Change prediction model developed using various Machine Learning (ML) techniques in this study. Performance of change prediction model analyzed using Area Under the Curve (AUC).},
   author = {Ruchika Malhotra and Shweta Meena},
   doi = {https://doi.org/10.1016/j.jocs.2024.102230},
   issn = {1877-7503},
   journal = {Journal of Computational Science},
   keywords = {Data Distribution,Feature Transfer,Heterogeneous Cross-Project Change Prediction,Machine Learning,Software Quality,Within-Project Change Prediction},
   pages = {102230},
   title = {Empirical validation of machine learning techniques for heterogeneous cross-project change prediction and within-project change prediction},
   volume = {76},
   url = {https://www.sciencedirect.com/science/article/pii/S1877750324000231},
   year = {2024}
}
@article{Elkin2022,
   abstract = {Serology and molecular tests are the two most commonly used methods for rapid COVID-19 infection testing. The two types of tests have different mechanisms to detect infection, by measuring the presence of viral SARS-CoV-2 RNA (molecular test) or detecting the presence of antibodies triggered by the SARS-CoV-2 virus (serology test). A handful of studies have shown that symptoms, combined with demographic and/or diagnosis features, can be helpful for the prediction of COVID-19 test outcomes. However, due to nature of the test, serology and molecular tests vary significantly. There is no existing study on the correlation between serology and molecular tests, and what type of symptoms are the key factors indicating the COVID-19 positive tests. In this study, we propose a machine learning based approach to study serology and molecular tests, and use features to predict test outcomes. A total of 2,467 donors, each tested using one or multiple types of COVID-19 tests, are collected as our testbed. By cross checking test types and results, we study correlation between serology and molecular tests. For test outcome prediction, we label 2,467 donors as positive or negative, by using their serology or molecular test results, and create symptom features to represent each donor for learning. Because COVID-19 produces a wide range of symptoms and the data collection process is essentially error prone, we group similar symptoms into bins. This decreases the feature space and sparsity. Using binned symptoms, combined with demographic features, we train five classification algorithms to predict COVID-19 test results. Experiments show that XGBoost achieves the best performance with 76.85\% accuracy and 81.4\% AUC scores, demonstrating that symptoms are indeed helpful for predicting COVID-19 test outcomes. Our study investigates the relationship between serology and molecular tests, identifies meaningful symptom features associated with COVID-19 infection, and also provides a way for rapid screening and cost effective detection of COVID-19 infection.},
   author = {Magdalyn E Elkin and Xingquan Zhu},
   doi = {https://doi.org/10.1016/j.smhl.2022.100331},
   issn = {2352-6483},
   journal = {Smart Health},
   keywords = {COVID-19,Classification,Machine Learning,Molecular test,Serology test,Symptoms},
   pages = {100331},
   title = {A machine learning study of COVID-19 serology and molecular tests and predictions},
   volume = {26},
   url = {https://www.sciencedirect.com/science/article/pii/S2352648322000654},
   year = {2022}
}
@article{Aker2019,
   abstract = {Verification of online rumours is becoming an increasingly important task with the prevalence of event discussions on social media platforms. This paper proposes an inner-attention-based neural network model that uses frequent, recurring terms from past rumours to classify a newly emerging rumour as true, false or unverified. Unlike other methods proposed in related work, our model uses the source rumour alone without any additional information, such as user replies to the rumour or additional feature engineering. Our method outperforms the current state-of-the-art methods on benchmark datasets (RumourEval2017) by 3\% accuracy and 6\% F-1 leading to 60.7\% accuracy and 61.6\% F-1. We also compare our attention-based method to two similar models which however do not make use of recurrent terms. The attention-based method guided by frequent recurring terms outperforms this baseline on the same dataset, indicating that the recurring terms injected by the attention mechanism have high positive impact on distinguishing between true and false rumours. Furthermore, we perform out-of-domain evaluations and show that our model is indeed highly competitive compared to the baselines on a newly released RumourEval2019 dataset and also achieves the best performance on classifying fake and legitimate news headlines.},
   author = {Ahmet Aker and Alfred Sliwa and Fahim Dalvi and Kalina Bontcheva},
   doi = {https://doi.org/10.1016/j.osnem.2019.07.001},
   issn = {2468-6964},
   journal = {Online Social Networks and Media},
   keywords = {Inner Attention Model,Recurring Terms in Rumours,Rumour Verification},
   pages = {100045},
   title = {Rumour verification through recurring information and an inner-attention mechanism},
   volume = {13},
   url = {https://www.sciencedirect.com/science/article/pii/S2468696419300588},
   year = {2019}
}
@article{Kolluri2021,
   abstract = {There is an abundance of misinformation, disinformation, and “fake news” related to COVID-19, leading the director-general of the World Health Organization to term this an ‘infodemic’. Given the high volume of COVID-19 content on the Internet, many find it difficult to evaluate veracity. Vulnerable and marginalized groups are being misinformed and subject to high levels of stress. Riots and panic buying have also taken place due to “fake news”. However, individual research-led websites can make a major difference in terms of providing accurate information. For example, the Johns Hopkins Coronavirus Resource Center website has over 81 million entries linked to it on Google. With the outbreak of COVID-19 and the knowledge that deceptive news has the potential to measurably affect the beliefs of the public, new strategies are needed to prevent the spread of misinformation. This study seeks to make a timely intervention to the information landscape through a COVID-19 “fake news”, misinformation, and disinformation website. In this article, we introduce CoVerifi, a web application which combines both the power of machine learning and the power of human feedback to assess the credibility of news. By allowing users the ability to “vote” on news content, the CoVerifi platform will allow us to release labelled data as open source, which will enable further research on preventing the spread of COVID-19-related misinformation. We discuss the development of CoVerifi and the potential utility of deploying the system at scale for combating the COVID-19 “infodemic”.},
   author = {Nikhil L Kolluri and Dhiraj Murthy},
   doi = {https://doi.org/10.1016/j.osnem.2021.100123},
   issn = {2468-6964},
   journal = {Online Social Networks and Media},
   keywords = {COVID-19,Infodemic,Machine learning,Media diet,Misinformation},
   pages = {100123},
   title = {CoVerifi: A COVID-19 news verification system},
   volume = {22},
   url = {https://www.sciencedirect.com/science/article/pii/S2468696421000070},
   year = {2021}
}
@article{Patel2024,
   abstract = {Ensuring software quality is critical aspect of the development process, with test beds playing a vital role in validating applications under several conditions. Traditional methods of test bed generation are time-consuming and often fail to cover wide range of testing scenarios. To address these challenges, we introduce a novel test bed generator software application BHRAMARI that automates the creation of test beds with high-quality code smells and samples. The integration of advanced technologies such as natural language processing and generative-AI paves the way for a new era in software testing, where automation and innovation ensure the highest standards of reliability.},
   author = {Soham Patel and Kailas Patil and Prawit Chumchu},
   doi = {https://doi.org/10.1016/j.simpa.2024.100687},
   issn = {2665-9638},
   journal = {Software Impacts},
   keywords = {Software TestBeds,Software engineering,Software preventative testing,Software testing,Software tool,TestBeds},
   pages = {100687},
   title = {BHRAMARI: Bug driven highly reusable automated model for automated test bed generation and integration},
   volume = {21},
   url = {https://www.sciencedirect.com/science/article/pii/S2665963824000757},
   year = {2024}
}
@article{Afzali2024,
   abstract = {The Gradient-Free Kernel Conditional Stein Discrepancy (GF-KCSD), presented in our prior work, represents a significant advancement in goodness-of-fit testing for conditional distributions. This method offers a robust alternative to previous gradient-based techniques, specially when the gradient calculation is intractable or computationally expensive. In this study, we explore previously unexamined aspects of GF-KCSD, with a particular focus on critical values and test power—essential components for effective hypothesis testing. We also present novel investigation on the impact of measurement errors on the performance of GF-KCSD in comparison to established benchmarks, enhancing our understanding of its resilience to these errors. Through controlled experiments using synthetic data, we demonstrate GF-KCSD’s superior ability to control type-I error rates and maintain high statistical power, even in the presence of measurement inaccuracies. Our empirical evaluation extends to real-world datasets, including brain MRI data. The findings confirm that GF-KCSD performs comparably to KCSD in hypothesis testing effectiveness while requiring significantly less computational time. This demonstrates GF-KCSD’s capability as an efficient tool for analyzing complex data, enhancing its value for scenarios that demand rapid and robust statistical analysis.},
   author = {Elham Afzali and Saman Muthukumarana and Liqun Wang},
   doi = {https://doi.org/10.1016/j.mlwa.2024.100581},
   issn = {2666-8270},
   journal = {Machine Learning with Applications},
   keywords = {Bootstrap resampling,Brain MRI data analysis,Conditional distributions,Goodness-of-fit testing,Gradient-free Kernel conditional stein discrepancy (GF-KCSD),Kernel methods,Measurement error},
   pages = {100581},
   title = {Navigating interpretability and alpha control in GF-KCSD testing with measurement error: A Kernel approach},
   volume = {17},
   url = {https://www.sciencedirect.com/science/article/pii/S2666827024000574},
   year = {2024}
}
@article{Chan2024,
   abstract = {Context:
Utilizing pretrained language models (PLMs) has become common practice in maintaining the content quality of question-answering (Q&A) websites. However, evaluating the effectiveness of PLMs poses a challenge as they tend to provide local optima rather than global optima.
Objective:
In this study, we propose using semantic-preserving Metamorphic Relations (MRs) derived from Metamorphic Testing (MT) to address this challenge and validate PLMs.
Methods:
To validate four selected PLMs, we conducted an empirical experiment using a publicly available dataset comprising 60000 data points. We defined three groups of Metamorphic Relations (MRGs), consisting of thirteen semantic-preserving MRs, which were then employed to generate “Follow-up” testing datasets based on the original “Source” testing datasets. The PLMs were trained using a separate training dataset. A comparison was made between the predictions of the four trained PLMs for “Source” and “Follow-up” testing datasets in order to identify instances of violations, which corresponded to inconsistent predictions between the two datasets. If no violation was found, it indicated that the PLM was insensitive to the associate MR; thereby, the MR can be used for validation. In cases where no violation occurred across the entire MRG, non-violation regions were identified and supported simulation metamorphic testing.
Results:
The results of this study demonstrated that the proposed MRs could effectively serve as a validation tool for content quality classification on Stack Overflow Q&A using PLMs. One PLM did not violate the “Uppercase conversion” MRG and the “Duplication” MRG. Furthermore, the absence of violations in the MRGs allowed for the identification of non-violation regions, confirming the ability of the proposed MRs to support simulation metamorphic testing.
Conclusion:
The experimental findings indicate that the proposed MRs can validate PLMs effectively and support simulation metamorphic testing for PLMs. However, further investigations are required to enhance the semantic comprehension and common sense knowledge of PLMs and explore highly informative statistical patterns of PLMs, in order to improve their overall performance.},
   author = {Pak Yuen Patrick Chan and Jacky Keung},
   doi = {https://doi.org/10.1016/j.nlp.2024.100114},
   issn = {2949-7191},
   journal = {Natural Language Processing Journal},
   keywords = {Content quality prediction,Metamorphic testing,Pretrained language model,Simulation testing,Validation},
   pages = {100114},
   title = {Validating pretrained language models for content quality classification with semantic-preserving metamorphic relations},
   volume = {9},
   url = {https://www.sciencedirect.com/science/article/pii/S2949719124000621},
   year = {2024}
}
@article{Treude2023,
   author = {Christoph Treude and Hideaki Hata},
   doi = {10.48550/ARXIV.2303.10131},
   journal = {CoRR},
   title = {She Elicits Requirements and He Tests: Software Engineering Gender
Bias in Large Language Models},
   volume = {abs/2303.10131},
   url = {https://doi.org/10.48550/arXiv.2303.10131},
   year = {2023}
}
@article{Charalambous2023,
   author = {Yiannis Charalambous and Norbert Tihanyi and Ridhi Jain and Youcheng Sun and Mohamed Amine Ferrag and Lucas C Cordeiro},
   doi = {10.48550/ARXIV.2305.14752},
   journal = {CoRR},
   title = {A New Era in Software Security: Towards Self-Healing Software via
Large Language Models and Formal Verification},
   volume = {abs/2305.14752},
   url = {https://doi.org/10.48550/arXiv.2305.14752},
   year = {2023}
}

@article{Hudson2024,
   author = {Sinclair Hudson and Sophia Jit and Boyue Caroline Hu and Marsha Chechik},
   doi = {10.48550/ARXIV.2406.08216},
   journal = {CoRR},
   title = {A Software Engineering Perspective on Testing Large Language Models:
Research, Practice, Tools and Benchmarks},
   volume = {abs/2406.08216},
   url = {https://doi.org/10.48550/arXiv.2406.08216},
   year = {2024}
}
@article{Ihalage2024,
   author = {Achintha Ihalage and Sayed M Taheri and Faris Muhammad and Hamed S Al-Raweshidy},
   doi = {10.48550/ARXIV.2407.03759},
   journal = {CoRR},
   title = {Convolutional vs Large Language Models for Software Log Classification
in Edge-Deployable Cellular Network Testing},
   volume = {abs/2407.03759},
   url = {https://doi.org/10.48550/arXiv.2407.03759},
   year = {2024}
}

@article{Widyasari2024,
   abstract = {With the advancement of Large Language Models (LLMs), their application in Software Quality Assurance (SQA) has increased. However, the current focus of these applications is predominantly on ChatGPT. There remains a gap in understanding the performance of various LLMs in this critical domain. This paper aims to address this gap by conducting a comprehensive investigation into the capabilities of several LLMs across two SQA tasks: fault localization and vulnerability detection. We conducted comparative studies using GPT-3.5, GPT-4o, and four other prominent publicly available LLMs—LLaMA-3-70B, LLaMA-3-8B, Gemma-7B, and Mixtral-8x7B, to evaluate their effectiveness in these tasks. Our findings reveal that several LLMs can outperform GPT-3.5 in both tasks. Additionally, even the lower-performing LLMs provided unique correct predictions, suggesting the potential of combining different LLMs’ results to enhance overall performance. By implementing a voting mechanism to combine the LLMs’ results, we achieved more than a 10\% improvement over the GPT-3.5 in both tasks. Furthermore, we introduced a cross-validation approach to refine the LLM answer by validating one LLM answer against another using a validation prompt. This approach led to performance improvements of 16\% in fault localization and 12\% in vulnerability detection compared to the GPT-3.5, with 4\% improvement compared to the best-performed LLMs. Our analysis also indicates that the inclusion of explanations in the LLMs’ results affects the effectiveness of the cross-validation technique.},
   author = {Ratnadira Widyasari and David Lo and Lizi Liao},
   doi = {10.48550/ARXIV.2409.01001},
   journal = {CoRR},
   title = {Beyond ChatGPT: Enhancing Software Quality Assurance Tasks with Diverse LLMs and Validation Techniques},
   volume = {abs/2409.01001},
   url = {https://doi.org/10.48550/arXiv.2409.01001},
   year = {2024}
}

@inproceedings{Dakhama2023,
   abstract = {We introduce a novel automated testing technique that combines LLM and search-based fuzzing. We use ChatGPT to parameterise C programs. We compile the resultant code snippets, and feed compilable ones to SearchGEM5, our extension to AFL++ fuzzer with customised new mutation operators. We run thus created 4005 binaries through our system under test, gem5, increasing its existing test coverage by more than 1000 lines. We discover 244 instances where gem5 simulation of the binary differs from the binary’s expected behaviour.},
   author = {Aidan Dakhama and Karine Even-Mendoza and William B Langdon and Héctor D Menéndez and Justyna Petke},
   doi = {10.1007/978-3-031-48796-5_14},
   editor = {Paolo Arcaini and Tao Yue and Erik M Fredericks},
   booktitle = {Search-Based Software Engineering - 15th International Symposium,
SSBSE 2023, San Francisco, CA, USA, December 8, 2023, Proceedings},
   pages = {160-166},
   publisher = {Springer},
   title = {SearchGEM5: Towards Reliable Gem5 with Search Based Software Testing
and Large Language Models},
   volume = {14415},
   url = {https://doi.org/10.1007/978-3-031-48796-5_14},
   year = {2023}
}

@article{Fatima2023,
   abstract = {Software testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing and failing across executions, even for the same version of the source code. Flaky test cases introduce overhead to software development as they can lead to unnecessary attempts to debug production or testing code. Besides rerunning test cases multiple times, which is time-consuming and computationally expensive, flaky test cases can be predicted using machine learning (ML) models, thus reducing the wasted cost of re-running and debugging these test cases. However, the state-of-the-art ML-based flaky test case predictors rely on pre-defined sets of features that are either project-specific, i.e., inapplicable to other projects, or require access to production code, which is not always available to software test engineers. Moreover, given the non-deterministic behavior of flaky test cases, it can be challenging to determine a complete set of features that could potentially be associated with test flakiness. Therefore, in this article, we propose Flakify, a black-box, language model-based predictor for flaky test cases. Flakify relies exclusively on the source code of test cases, thus not requiring to (a) access to production code (black-box), (b) rerun test cases, (c) pre-define features. To this end, we employed CodeBERT, a pre-trained language model, and fine-tuned it to predict flaky test cases using the source code of test cases. We evaluated Flakify on two publicly available datasets (FlakeFlagger and IDoFT) for flaky test cases and compared our technique with the FlakeFlagger approach, the best state-of-the-art ML-based, white-box predictor for flaky test cases, using two different evaluation procedures: (1) cross-validation and (2) per-project validation, i.e., prediction on new projects. Flakify achieved F1-scores of 79\% and 73\% on the FlakeFlagger dataset using cross-validation and per-project validation, respectively. Similarly, Flakify achieved F1-scores of 98\% and 89\% on the IDoFT dataset using the two validation procedures, respectively. Further, Flakify surpassed FlakeFlagger by 10 and 18 percentage points (pp) in terms of precision and recall, respectively, when evaluated on the FlakeFlagger dataset, thus reducing the cost bound to be wasted on unnecessarily debugging test cases and production code by the same percentages (corresponding to reduction rates of 25\% and 64\%). Flakify also achieved significantly higher prediction results when used to predict test cases on new projects, suggesting better generalizability over FlakeFlagger. Our results further show that a black-box version of FlakeFlagger is not a viable option for predicting flaky test cases.},
   author = {Sakina Fatima and Taher Ahmed Ghaleb and Lionel C Briand},
   doi = {10.1109/TSE.2022.3201209},
   issue = {4},
   journal = {IEEE Trans. Software Eng.},
   pages = {1912-1927},
   title = {Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests},
   volume = {49},
   url = {https://doi.org/10.1109/TSE.2022.3201209},
   year = {2023}
}

@inproceedings{Hoffmann2024-2,
   abstract = {Motivation. Software tests are a necessity in the development of software to secure functionality, reliability, and usability [10]; however, these tests are costly and time-consuming [6]. Although tool support for software testing has advanced, there remains considerable potential for enhancement. Many software tests are still devised manually, with the creation of unit tests being particularly laborious. Automating the generation of test cases is promising for streamlining this aspect of software testing [6].Large Language Models (LLMs) have exhibited capabilities in code generation [11, 13–15], test case generation [17], and various other domains [11]. The advancement of model performance of transformer-based LLMs is mainly achieved by expanding the model size in line with an increase in training data size [7, 8]. However, this approach leads to high computational costs which can only be afforded by corporations with significant financial resources. This highlights the need for transformer-based LLMs that perform well on a specific downstream task and are also cost-efficient. Addressing this, we focused on supervised fine-tuning (SFT) of more resource-efficient transformer-based LLMs LLaMA 2 13B, Code Llama 13B, and Mistral 7B for the specific downstream task of generating test cases for mobile applications.},
   author = {Jacob Hoffmann and Demian Frister},
   doi = {10.1145/3644032.3644454},
   editor = {Francesca Lonetti and Antonio Guerriero and Mehrdad Saadatmand and Christof J Budnik and Jenny Li},
   booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation
of Software Test (AST 2024), Lisbon, Portugal, April 15-16, 2024},
   pages = {76-77},
   publisher = {ACM},
   title = {Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models},
   url = {https://doi.org/10.1145/3644032.3644454},
   year = {2024}
}

@inproceedings{Paz2019,
   abstract = {Engineering avionics software is a complex task. Even more so due to their safety-critical nature. Aviation authorities require avionics software suppliers to provide appropriate evidence of achieving DO-178C objectives for the verification of outputs from the requirements and design processes, and requirements-based testing. This concern is leading suppliers to consider and incorporate more effective engineering methods that can support them in their verification and certification endeavours. This paper presents SpecML, a modelling language providing a requirements specification infrastructure for avionics software. The goal of SpecML is threefold: 1) enforce certification information mandated by DO-178C, 2) capture requirements in natural language to encourage adoption in industry. and 3) capture requirements in a structured, semantically-rich formalism to enable requirements-based analyses and testing. The modelling language has been developed as a UML profile extending SysML Requirements. A reference implementation has been developed and experiences on its application to an openly-available avionics software specification are reported.},
   author = {Andrés Paz and Ghizlane El-Boussaidi},
   doi = {10.1109/RET.2019.00008},
   editor = {Gregory Gay and Sahar Tahvili and Michael Unterkalmsteiner},
   booktitle = {Proceedings of the 6th International Workshop on Requirements Engineering and Testing, RET@ICSE 2019, Montreal, QC, Canada, May 28, 2019},
   pages = {1-8},
   publisher = {IEEE / ACM},
   title = {A requirements modelling language to facilitate avionics software verification and certification},
   url = {https://doi.org/10.1109/RET.2019.00008},
   year = {2019}
}

@inproceedings{Treude2023,
   author = {Christoph Treude and Hideaki Hata},
   doi = {10.1109/MSR59073.2023.00088},
   booktitle = {20th IEEE/ACM International Conference on Mining Software Repositories,
MSR 2023, Melbourne, Australia, May 15-16, 2023},
   pages = {624-629},
   publisher = {IEEE},
   title = {She Elicits Requirements and He Tests: Software Engineering Gender
Bias in Large Language Models},
   url = {https://doi.org/10.1109/MSR59073.2023.00088},
   year = {2023}
}
@inproceedings{Santos2024,
   abstract = {A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates coherent content, including grammatically precise sentences, human-like paragraphs, and syntactically accurate code snippets. LLMs can play a pivotal role in software development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts, specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.},
   author = {Robson Santos and Ítalo Santos and Cleyton V C de Magalhães and Ronnie de Souza Santos},
   doi = {10.1109/ICST60714.2024.00039},
   booktitle = {IEEE Conference on Software Testing, Verification and Validation,
ICST 2024, Toronto, ON, Canada, May 27-31, 2024},
   pages = {353-360},
   publisher = {IEEE},
   title = {Are We Testing or Being Tested? Exploring the Practical Applications
of Large Language Models in Software Testing},
   url = {https://doi.org/10.1109/ICST60714.2024.00039},
   year = {2024}
}

@article{Schfer2024,
   abstract = {Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2\% and branch coverage of 52.8\%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3\% statement coverage and 25.6\% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8\% of TestPilot's generated tests have ≤ 50\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TestPilot with two additional LLMs, OpenAI's older code-cushman-002 LLM and StarCoder, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2\% median statement coverage), and somewhat worse results with the latter (54.0\% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.},
   author = {Max Schäfer and Sarah Nadi and Aryaz Eghbali and Frank Tip},
   doi = {10.1109/TSE.2023.3334955},
   issue = {1},
   journal = {IEEE Trans. Software Eng.},
   pages = {85-105},
   title = {An Empirical Evaluation of Using Large Language Models for Automated
Unit Test Generation},
   volume = {50},
   url = {https://doi.org/10.1109/TSE.2023.3334955},
   year = {2024}
}

@inproceedings{Tsigkanos2023,
   abstract = {When testing scientific software, it is often challenging or even impossible to craft a test oracle for checking whether the program under test produces the expected output when being executed on a given input – also known as the oracle problem in software engineering. Metamorphic testing mitigates the oracle problem by reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over three case studies of scientific software documentation, and compare variables extracted to ground truth manually labelled by experts.},
   author = {Christos Tsigkanos and Pooja Rani and Sebastian Müller and Timo Kehrer},
   doi = {10.1007/978-3-031-35995-8_23},
   editor = {Jir\'\{\i\} Mikyska and Clélia de Mulatier and Maciej Paszynski and Valeria V Krzhizhanovskaya and Jack J Dongarra and Peter M A Sloot},
   booktitle = {Computational Science - ICCS 2023 - 23rd International Conference,
Prague, Czech Republic, July 3-5, 2023, Proceedings, Part I},
   pages = {321-335},
   publisher = {Springer},
   title = {Variable Discovery with Large Language Models for Metamorphic Testing of Scientific Software},
   volume = {14073},
   url = {https://doi.org/10.1007/978-3-031-35995-8_23},
   year = {2023}
}

@inproceedings{Vito2024,
   abstract = {In recent years, Large Language Models (LLMs) have emerged as powerful tools capable of understanding and generating natural language text and source code with remarkable proficiency. Leveraging this capability, we are currently investigating the potential of LLMs to streamline software development processes by automating two key tasks: issue report classification and test scenario generation. For issue report classification the challenge lies in accurately categorizing and prioritizing incoming bug reports or feature requests. By employing LLMs, we aim to develop models that can efficiently classify issue reports, facilitating prompt response and resolution by software development teams. Test scenario generation involves the automatic generation of test cases to validate software functionality. In this context, LLMs offer the potential to analyze requirements documents, user stories, or other forms of textual input to automatically generate comprehensive test scenarios, reducing the manual effort required in test case creation. In this paper, we outline our research objectives, methodologies, and anticipated contributions to these topics in the field of software engineering. Through empirical studies and experimentation, we seek to assess the effectiveness and feasibility of integrating LLMs into existing software development workflows. By shedding light on the opportunities and challenges associated with LLMs in software engineering, this paper aims to pave the way for future advancements in this rapidly evolving domain.},
   author = {Gabriele De Vito and Luigi Libero Lucio Starace and Sergio Di Martino and Filomena Ferrucci and Fabio Palomba},
   editor = {Sergio Di Martino and Carlo Sansone and Elio Masciari and Silvia Rossi and Michela Gravina},
   booktitle = {Proceedings of the Ital-IA Intelligenza Artificiale - Thematic Workshops
co-located with the 4th CINI National Lab AIIS Conference on Artificial
Intelligence (Ital-IA 2024), Naples, Italy, May 29-30, 2024},
   pages = {48-53},
   publisher = {CEUR-WS.org},
   title = {Large Language Models in Software Engineering: A Focus on Issue
Report Classification and User Acceptance Test Generation},
   volume = {3762},
   url = {https://ceur-ws.org/Vol-3762/534.pdf},
   year = {2024}
}

@article{Wang2024-6,
   abstract = {Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.},
   author = {Junjie Wang and Yuchao Huang and Chunyang Chen and Zhe Liu and Song Wang and Qing Wang},
   doi = {10.1109/TSE.2024.3368208},
   issue = {4},
   journal = {IEEE Trans. Software Eng.},
   pages = {911-936},
   title = {Software Testing With Large Language Models: Survey, Landscape, and Vision},
   volume = {50},
   url = {https://doi.org/10.1109/TSE.2024.3368208},
   year = {2024}
}

@inproceedings{BermeoContoJorgeandZiga-Prieto2019,
   abstract = {Specification languages offer abstractions and notations that facilitate the systematic and analytical reasoning about important aspects in a specific domain problematic. In a software engineering process domain, the usage of specification languages improve the quality and delivery time of the artefacts generated during the execution of the process activities. Cloud applications, or cloud services, are service-oriented applications whose consumption is constantly growing; however, their development require support for new roles and activities. In this work we are interested in knowing how specification languages are being used by researchers and practitioners to support the development of cloud services. This work presents a systematic mapping that provides guidance to determine the current state and to characterize the specification languages that support the service life cycle activities in a cloud services development domain.},
   author = {Miguel
and Solano-Quinde Lizandro Bermeo Conto Jorge
and Zúñiga-Prieto},
   city = {Cham},
   editor = {Guillermo
and Zúñiga-Prieto Miguel
and D'Armas Mayra
and Zúñiga Sánchez Miguel Botto-Tobar Miguel
and Pizarro},
   isbn = {978-3-030-05532-5},
   booktitle = {Technology Trends},
   pages = {72-88},
   publisher = {Springer International Publishing},
   title = {A Systematic Mapping Study of Specification Languages in Cloud Services Development},
   year = {2019}
}
@inproceedings{FernandesNatashaandDras2019,
   abstract = {We address the problem of how to ``obfuscate'' texts by removing stylistic clues which can identify authorship, whilst preserving (as much as possible) the content of the text. In this paper we combine ideas from ``generalised differential privacy'' and machine learning techniques for text processing to model privacy for text documents. We define a privacy mechanism that operates at the level of text documents represented as ``bags-of-words''—these representations are typical in machine learning and contain sufficient information to carry out many kinds of classification tasks including topic identification and authorship attribution (of the original documents). We show that our mechanism satisfies privacy with respect to a metric for semantic similarity, thereby providing a balance between utility, defined by the semantic content of texts, with the obfuscation of stylistic clues. We demonstrate our implementation on a ``fan fiction'' dataset, confirming that it is indeed possible to disguise writing style effectively whilst preserving enough information and variation for accurate content classification tasks. We refer the reader to our complete paper [15] which contains full proofs and further experimentation details.},
   author = {Mark
and McIver Annabelle Fernandes Natasha
and Dras},
   city = {Cham},
   editor = {David Nielson Flemming
and Sands},
   isbn = {978-3-030-17138-4},
   booktitle = {Principles of Security and Trust},
   pages = {123-148},
   publisher = {Springer International Publishing},
   title = {Generalised Differential Privacy for Text Document Processing},
   year = {2019}
}
@inproceedings{Lpez-HernndezJsicaandAlmela2019,
   abstract = {Automatic spelling correction is one of the most important problems in natural language processing. Its difficulty increases in medical corpora, due to the intrinsic particularities that have these texts. These features include the use of specific terminology, abbreviations, acronyms and the presence of writing errors. In this article we present a systematic review of the literature on automatic spelling detection and correction for the medical domain. There are many works on detection and automatic correction, but there is no review delving into the process of automatic correction in the medical domain. Therefore, we intend to synthesize all the existing information on this research topic and the types of studies that have been carried out to date. We present the main techniques and resources, and finally also the limitations and specific challenges. The results reflect the importance of compiling an exhaustive dictionary. In addition, the results show the ordinary use of distance algorithms of spelling and phonetic similarity, as well as with statistical techniques. The improvement of performance in recent years is especially relevant because of the use of context-based methods, such as linguistic models or neural embeddings.},
   author = {Ángela
and Valencia-García Rafael López-Hernández Jésica
and Almela},
   city = {Cham},
   editor = {Gema
and Del Cioppo-Morstadt Javier
and Vera-Lucio Néstor
and Bucaram-Leverone Martha Valencia-García Rafael
and Alcaraz-Mármol},
   isbn = {978-3-030-34989-9},
   booktitle = {Technologies and Innovation},
   pages = {95-108},
   publisher = {Springer International Publishing},
   title = {Automatic Spelling Detection and Correction in the Medical Domain: A Systematic Literature Review},
   year = {2019}
}
@inproceedings{HerruzoAnaandPashenkov2020,
   abstract = {In this paper we review the overall process for the design, development, and deployment of ``What I See Is What You Get'', an experiential installation that creates live interactive visuals, by analyzing human facial expressions and behaviors, accompanied by text generated using Machine Learning algorithms trained on the art collection of The J. Paul Getty Museum in Los Angeles. The project is developed by students and faculty in an academic environment and exhibited at the Getty Museum. We also study the pedagogical process implemented to address the curriculum's learning outcomes in an ``applied'' environment while designing a contemporary new media art piece. Special attention is paid to the level and quality of the interaction between users and the piece, demonstrating how advances in technology and computing such as Deep Learning and Natural Language Processing can contribute to deeper connections and new layers of interactivity.},
   author = {Nikita Herruzo Ana
and Pashenkov},
   city = {Cham},
   editor = {Eva Irene Brooks Anthony
and Brooks},
   isbn = {978-3-030-53294-9},
   booktitle = {Interactivity, Game Creation, Design, Learning, and Innovation},
   pages = {343-359},
   publisher = {Springer International Publishing},
   title = {``What I See Is What You Get'' Explorations of Live Artwork Generation, Artificial Intelligence, and Human Interaction in a Pedagogical Environment},
   year = {2020}
}
@inproceedings{AskarpourMehrnooshandBersani2020,
   abstract = {The general attitude of students towards formal specification and verification of systems is not exactly what one could call enthusiastic. Generally, software engineering courses at universities include an introduction to specification with formal notations such as Z, Alloy, UML, etc. However, it seems that the importance of formal specification to replicate expected system behavior does not sink in as it should with the students. Moreover, other products of computer science (e.g., machine learning algorithms, robot systems deployment), rather than software, benefit from formal specification as well. This paper is a general report of our observations on teaching formal methods on undergraduate and graduate levels at Politecnico di Milano.},
   author = {Marcello M Askarpour Mehrnoosh
and Bersani},
   city = {Cham},
   editor = {Alfredo
and Mazzara Manuel
and Meyer Bertrand
and Naumchev Alexandr
and Sadovykh Andrey Bruel Jean-Michel
and Capozucca},
   isbn = {978-3-030-57663-9},
   booktitle = {Frontiers in Software Engineering Education},
   pages = {3-18},
   publisher = {Springer International Publishing},
   title = {Teaching Formal Methods: An Experience Report},
   year = {2020}
}
@inproceedings{BumerFrederikSandKersting2020,
   abstract = {Peer-to-Peer news portals allow Internet users to write news articles and make them available online to interested readers. Despite the fact that authors are free in their choice of topics, there are a number of quality characteristics that an article must meet before it is published. In addition to meaningful titles, comprehensibly written texts and meaningful images, relevant tags are an important criteria for the quality of such news. In this case study, we discuss the challenges and common mistakes that Peer-to-Peer reporters face when tagging news and how incorrect information can be corrected through the orchestration of existing Natural Language Processing services. Lastly, we use this illustrative example to give insight into the challenges of dealing with bottom-up taxonomies.},
   author = {Joschka
and Buff Bianca
and Geierhos Michaela Bäumer Frederik S.
and Kersting},
   city = {Cham},
   editor = {Rita
and Gudonienė Daina
and Sukackė Vilma Lopata Audrius
and Butkienė},
   isbn = {978-3-030-59506-7},
   booktitle = {Information and Software Technologies},
   pages = {368-382},
   publisher = {Springer International Publishing},
   title = {Tag Me If You Can: Insights into the Challenges of Supporting Unrestricted P2P News Tagging},
   year = {2020}
}
@inproceedings{BahramiMehdiandChen2020,
   abstract = {Web Application Programming Interface (API) allows third-party and subscribed users to access data and functions of a software application through the network or the Internet. Web APIs expose data and functions to the public users, authorized users or enterprise users. Web API providers publish API documentations to help users to understand how to interact with web-based API services, and how to use the APIs in their integration systems. The exponential raise of the number of public web service APIs may cause a challenge for software engineers to choose an efficient API. The challenge may become more complicated when web APIs updated regularly by API providers. In this paper, we introduce a novel transformation-based approach which crawls the web to collect web API documentations (unstructured documents). It generates a web API Language model from API documentations, employs different machine learning algorithms to extract information and produces a structured web API specification that compliant to Open API Specification (OAS) format. The proposed approach improves information extraction patterns and learns the variety of structured and terminologies. In our experiment, we collect a sheer number of web API documentations. Our evaluation shows that the proposed approach find RESTful API documentations with 75\% accuracy, constructs API endpoints with 84\%, constructs endpoint attributes with 95\%, and assigns endpoints to attributes with an accuracy 98\%. The proposed approach were able to produces more than 2,311 OAS web API Specifications.},
   author = {Wei-Peng Bahrami Mehdi
and Chen},
   city = {Cham},
   editor = {Yunni
and Seshadri Sangeetha
and Zhang Liang-Jie Wang Qingyang
and Xia},
   isbn = {978-3-030-59592-0},
   booktitle = {Services Computing – SCC 2020},
   pages = {103-119},
   publisher = {Springer International Publishing},
   title = {Automated Web Service Specification Generation Through a Transformation-Based Learning},
   year = {2020}
}
@inproceedings{LiuLeiandBahrami2020,
   abstract = {In recent years, Web Application Programming Interfaces (APIs) are becoming more and more popular with the development of the Internet industry and software engineering. Many companies provide public Web APIs for their services, and developers can greatly accelerate the development of new applications by relying on such APIs to execute complex tasks without implementing the corresponding functionalities themselves. The proliferation of web APIs, however, also introduces a challenge for developers to search and discover the desired API and its endpoint. This is a practical and crucial problem because according to ProgrammableWeb, there are more than 22,000 public Web APIs each of which may have tens or hundreds of endpoints. Therefore, it is difficult and time-consuming for developers to find the desired API and its endpoint to satisfy their development needs. In this paper, we present an intelligent system for Web API searches based on natural language queries by using a two-step transfer learning. To train the model, we collect a significant amount of sentences from crowdsourcing and utilize an ensemble deep learning model to predict the correct description sentences for an API and its endpoint. A training dataset is built by synthesizing the correct description sentences and then is used to train the two-step transfer learning model for Web API search. Extensive evaluation results show that the proposed methods and system can achieve high accuracy to search a Web API and its endpoint.},
   author = {Mehdi
and Park Junhee
and Chen Wei-Peng Liu Lei
and Bahrami},
   city = {Cham},
   editor = {Yasuhiko
and Serhani Mohamed Adel
and Zhang Liang-Jie Ku Wei-Shinn
and Kanemasa},
   isbn = {978-3-030-59618-7},
   booktitle = {Web Services – ICWS 2020},
   pages = {96-113},
   publisher = {Springer International Publishing},
   title = {Web API Search: Discover Web API and Its Endpoint with Natural Language Queries},
   year = {2020}
}
@inproceedings{YanWeiandWang2020,
   abstract = {It has been found that very often long queries are more challenging than short queries for information search engines to obtain good results. In this paper, we present a word embedding-based approach. First short queries or concepts are extracted from the original query. Then with the help of a trained word embedding model, all of the query elements go through a series of reformulation operations including deletion, substitution, and addition of terms so as to obtain more profitable query representations. Finally all the reformulated elements are linearly combined with the original query. Experiments are conducted on three TREC collections, and the experimental results show that the proposed method is able to improve retrieval performance on average and especially effective for long queries. Compared with several state-of-the-art baseline methods, the proposed method is very good.},
   author = {Yarong
and Huang Chunlan
and Wu Shengli Yan Wei
and Wang},
   city = {Cham},
   editor = {Xuemin
and Hendler James
and Song Wei
and Xu Zhuoming
and Liu Genggeng Wang Guojun
and Lin},
   isbn = {978-3-030-60029-7},
   booktitle = {Web Information Systems and Applications},
   pages = {202-214},
   publisher = {Springer International Publishing},
   title = {Word Embedding-Based Reformulation for Long Queries in Information Search},
   year = {2020}
}
@inproceedings{RughbeerYastilandPillay2020,
   abstract = {Information Retrieval is the task of satisfying an information need by retrieving relevant information from large collections. Recently, deep neural networks have achieved several performance breakthroughs in the field, owing to the availability of large-scale training sets. When training data is limited, however, neural retrieval systems vastly underperform. To compensate for the lack of training data, researchers have turned to transfer learning by relying on labelled data from other search domains. Despite having access to several publicly available datasets, researchers are currently unguided in selecting the best training set for their particular applications. To address this knowledge gap, we propose a rigorous method to select an optimal training set for a specific search domain. We validate this method on the TREC-COVID challenge, which was organized by the Allen Institute for Artificial Intelligence and the National Institute of Standards and Technology. Our neural model ranked first from 143 competing systems. More importantly, it was able to achieve this result by training on a dataset that was selected using our proposed method. This work highlights the performance gains that may be achieved through careful dataset selection in transfer learning.},
   author = {Anban W.
and Jembere Edgar Rughbeer Yastil
and Pillay},
   city = {Cham},
   editor = {Aurona Gerber},
   isbn = {978-3-030-66151-9},
   booktitle = {Artificial Intelligence Research},
   pages = {53-65},
   publisher = {Springer International Publishing},
   title = {Dataset Selection for Transfer Learning in Information Retrieval},
   year = {2020}
}
@inproceedings{DaswaniMohinishandDesai2020,
   abstract = {In an organization as big as a university that has many distinct departments and administrative bodies, it becomes almost impossible to easily obtain information online or by other means. Assistance over the phone or in-person is often limited to office hours and the information online is scattered through numerous (often nested) web pages, often independently administered and maintained by each sub-division. In this work, we present CollegeBot, a conversational AI agent that uses natural language processing and machine learning to assist visitors of a university's web site in easily locating information related to their queries. We discuss how we create the knowledge base by collecting and appropriately preprocessing information that is used to train the conversational agent for answering domain-specific questions. We have evaluated two different algorithms for training the conversational model for the chatbot, namely a semantic similarity model and a deep learning one leveraging Sequence-to-Sequence learning model. The proposed system is able to capture the user's intent and switch context appropriately. It also leverages the open source AIML chatbot ALICE to answer any generic (non domain-specific) questions. We present a proof-of-concept prototype for San Jose State University, to demonstrate how such an approach can be easily adopted by other academic institutions as well.},
   author = {Kavina
and Patel Mili
and Vani Reeya
and Eirinaki Magdalini Daswani Mohinish
and Desai},
   city = {Cham},
   editor = {Masaaki
and Degen Helmut
and Reinerman-Jones Lauren Stephanidis Constantine
and Kurosu},
   isbn = {978-3-030-60117-1},
   booktitle = {HCI International 2020 - Late Breaking Papers: Multimodality and Intelligence},
   pages = {44-63},
   publisher = {Springer International Publishing},
   title = {CollegeBot: A Conversational AI Approach to Help Students Navigate College},
   year = {2020}
}
@inproceedings{Gonzlez-TablasAnaIandRashed2022,
   abstract = {Several frameworks that cover cyber security education and professional development have been introduced as a guidance for learners, educators and professionals to the different knowledge areas of the field. One of the most important frameworks is the Cyber Security Body of Knowledge (CyBOK). In this paper, we apply the BERTopic topic modeling technique to CyBOK. We aim, by using this technique, to identify the most relevant topics related to each CyBOK's knowledge area in an automated way. Our results indicate that it is possible to find a meaningful topic model describing CyBOK and, thus, suggests the possibility of applying related techniques to texts to identify their main themes.},
   author = {Mohammed González-Tablas Ana I.
and Rashed},
   city = {Cham},
   editor = {Steven Clarke Nathan
and Furnell},
   isbn = {978-3-031-12172-2},
   booktitle = {Human Aspects of Information Security and Assurance},
   pages = {49-65},
   publisher = {Springer International Publishing},
   title = {Exploring CyBOK with Topic Modeling Techniques},
   year = {2022}
}

@inproceedings{SasakiTaikiandIto2022,
   abstract = {In recent years, scholarly databases have made many scientific papers available on the Internet. While these databases facilitate access to excellent papers, they also increase the possibility of encountering inferior papers. However, it is difficult to predict the quality of a paper just from a glance at the paper. In this paper, we propose a machine learning approach to predicting the quality of scientific papers. Specifically, we predict the quality of an article by classifying for the abstract of the paper whether the article is included in a superior journal or not. The proposed model is trained using a BERT-based model widely used in natural language processing. After training, we achieved a test accuracy of 95.1\% and 89.6\% in medicine and computer science, respectively. In addition, the results of the classification are visualized by evaluating the sentence combinations in the abstract to clarify the details of the classification.},
   author = {Yasuaki
and Nakano Koji
and Kasagi Akihiko Sasaki Taiki
and Ito},
   city = {Cham},
   editor = {Plamen
and Jayne Chrisina
and Papaleonidas Antonios
and Aydin Mehmet Pimenidis Elias
and Angelov},
   isbn = {978-3-031-15937-4},
   booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2022},
   pages = {212-223},
   publisher = {Springer Nature Switzerland},
   title = {BERT-Based Scientific Paper Quality Prediction},
   year = {2022}
}
@inproceedings{Benito-SantiagoHermiloandCrdova-Esparza2022,
   abstract = {This research conducted a systematic review of related works on machine translation of languages with low digital resources. First, we carried out the information search in the databases: ScienceDirect, IEEE Xplore, ACM Digital Library. Eighteen articles were collected following inclusion and exclusion criteria, considering a search period from 2016 to 2022. Subsequently, we analyzed and classified these articles according to the libraries developed and/or used based on machine learning, statistics, or grammar. The results indicate that pre-training and morphological segmentation techniques with finite state machines and machine learning techniques improve the translation of languages with low digital resources. In addition, according to the articles compiled in the specialized databases, in Mexico, unlike other countries that we analyzed, there are few publications on the translation of languages with low digital resources, and we mostly found research papers published in international conferences.},
   author = {Diana Margarita
and Castro-Sánchez Noé Alejandro
and Herrera-Navarro Ana-Marcela Benito-Santiago Hermilo
and Córdova-Esparza},
   city = {Cham},
   editor = {Juan
and Martínez Seis Bella Pichardo Lagunas Obdulia
and Martínez-Miranda},
   isbn = {978-3-031-19496-2},
   booktitle = {Advances in Computational Intelligence},
   pages = {41-56},
   publisher = {Springer Nature Switzerland},
   title = {Machine Translation of Texts from Languages with Low Digital Resources: A Systematic Review},
   year = {2022}
}
@inproceedings{Nez-RobinsonDanielandTalavera-Montalto2022,
   abstract = {Transformer models have evolved natural language processing tasks in machine learning and set a new standard for the state of the art. Thanks to the self-attention component, these models have achieved significant improvements in text generation tasks (such as extractive and abstractive text summarization). However, research works involving text summarization and the legal domain are still in their infancy, and as such, benchmarks and a comparative analysis of these state of the art models is important for the future of text summarization of this highly specialized task. In order to contribute to these research works, the researchers propose a comparative analysis of different, fine-tuned Transformer models and datasets in order to provide a better understanding of the task at hand and the challenges ahead. The results show that Transformer models have improved upon the text summarization task, however, consistent and generalized learning is a challenge that still exists when training the models with large text dimensions. Finally, after analyzing the correlation between objective results and human opinion, the team concludes that the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [13] metrics used in the current state of the art are limited and do not reflect the precise quality of a generated summary.},
   author = {Jose
and Ugarte Willy Núñez-Robinson Daniel
and Talavera-Montalto},
   city = {Cham},
   editor = {Filipe
and Augusto Maria Fernanda Guarda Teresa
and Portela},
   isbn = {978-3-031-20319-0},
   booktitle = {Advanced Research in Technologies, Information, Innovation and Sustainability},
   pages = {372-386},
   publisher = {Springer Nature Switzerland},
   title = {A Comparative Analysis on the Summarization of Legal Texts Using Transformer Models},
   year = {2022}
}
@inproceedings{MarquezBogartYailandAlanis2022,
   abstract = {In the last decade, Artificial Intelligence (AI) has become lead on the field of information generation and processing tasks through the emergence of Machine Learning (ML), as well as the data specialist mentions Machine Learning is a master of pattern recognition, and is capable of transform a data sample into a computer program capable of drawing inferences from new data sets for which it has not been previously trained, based on artificial neural networks (ANN) processing in academic texts, which are used to identify patterns and classify different types of information, currently treated as Deep Learning (DL) which is a subset of Machine Learning, this algorithm tries to imitate the human brain by continuously analyzing data with a given logical structure, which has led to its applicability to different fields such as robotics, voice processing, artificial vision, natural language processing (NLP), with the intention to provide computer systems with the ability to learn. Natural language processing has traditionally been a complex and non-trivial task in algorithm design. Making use of AI, new thresholds are being reached in the state of the art of different problems and with constant advances in the models in use, they are being reached faster and faster.},
   author = {Arnulfo
and Magdaleno-Palencia Jose Sergio
and Quezada Angeles Marquez Bogart Yail
and Alanis},
   city = {Cham},
   editor = {Filipe
and Augusto Maria Fernanda Guarda Teresa
and Portela},
   isbn = {978-3-031-20319-0},
   booktitle = {Advanced Research in Technologies, Information, Innovation and Sustainability},
   pages = {535-545},
   publisher = {Springer Nature Switzerland},
   title = {Artificial Neural Networks Applied to Natural Language Processing in Academic Texts},
   year = {2022}
}
@inproceedings{Ardimento2022,
   abstract = {The problem of bug-fixing time can be treated as a supervised text categorization task in Natural Language Processing. In recent years, following the use of deep learning also in the field of Natural Language Processing, pre-trained contextualized representations of words have become widespread. One of the most used pre-trained language representations models is named Google BERT (hereinafter, for brevity, BERT). BERT uses a self-attention mechanism that allows learning the bidirectional context representation of a word in a sentence, which constitutes one of the main advantages over the previously proposed solutions. However, due to the large size of BERT, it is difficult for it to put it into production. To address this issue, a smaller, faster, cheaper and lighter version of BERT, named DistilBERT, has been introduced at the end of 2019. This paper compares the efficacy of BERT and DistilBERT, combined with the Logistic Regression, in predicting bug-fixing time from bug reports of a large-scale open-source software project, LiveCode. In the experimentation carried out, DistilBERT retains almost 100\% of its language understanding capabilities and, in the best case, it is 63.28\% faster than BERT. Moreover, with a not time-consuming tuning of the C parameter in Logistic Regression, the DistilBERT provides an accuracy value even better than BERT.},
   author = {Pasquale Ardimento},
   city = {Cham},
   editor = {Marco
and Mikkonen Tommi
and Klünder Jil
and Abrahamsson Pekka Taibi Davide
and Kuhrmann},
   isbn = {978-3-031-21388-5},
   booktitle = {Product-Focused Software Process Improvement},
   pages = {610-620},
   publisher = {Springer International Publishing},
   title = {Predicting Bug-Fixing Time: DistilBERT Versus Google BERT},
   year = {2022}
}
@inproceedings{LiMinandLi2022,
   abstract = {Provide auxiliary tools for Tibetan vocabulary learning, and solve the problems of long response time and poor application performance of existing online learning systems. Using the multi-terminal fusion technology, the optimized design of the Tibetan vocabulary online learning system is realized from the three aspects of hardware, database and software functions. Adjust the connection mode of the server, and modify hardware devices such as embedded processors and resource collectors. Use the optimized system circuit to connect hardware devices to complete the optimization of the hardware system. Collect Tibetan vocabulary online learning resource data, and connect each database table according to the logical relationship between the data. With the support of hardware devices and databases, set permissions for different users and choose the online learning mode of Tibetan vocabulary. Use multi-terminal fusion technology to achieve resource integration. Realize system functions such as Tibetan vocabulary learning information retrieval, online interactive practice, and online testing. Through system testing, it is found that the operating success rate of the designed system reaches 99.2\%, and the response time is less than 6000ms. And through the application of the design system, the students' Tibetan language scores have been significantly improved.},
   author = {Nan Li Min
and Li},
   city = {Cham},
   editor = {Guanglu Fu Weina
and Sun},
   isbn = {978-3-031-21161-4},
   booktitle = {e-Learning, e-Education, and Online Training},
   pages = {211-228},
   publisher = {Springer Nature Switzerland},
   title = {Design of Tibetan Vocabulary Online Learning System Based on Multi-terminal Integration},
   year = {2022}
}
@inproceedings{KwonSunjaeandJang2023-1,
   abstract = {Edge-cloud system is a crucial computing infrastructure for the innovations of modern society. In addition, the high interest in the edge-cloud system leads to various studies for testing to ensure the reliability of the system. However, like traditional software systems, the amount of resources for testing is always limited. Thus, we suggest CodeBERT Based Just-In-Time (JIT) Software Defect Prediction (SDP) model to address the limitation. This method helps practitioners prioritize the limited testing resources for the defect-prone functions in commits and improves the system's reliability. We generate GitHub Pull-Request (GHPR) datasets on two open-source framework projects for edge-cloud system in GitHub. After that, we evaluate the performance of the proposed model on the GHPR datasets in within-project environment and cross-project environment. To the best of our knowledge, it is the first attempt to apply SDP to edge-cloud systems, and as a result of the evaluation, we can confirm the applicability of JIT SDP in edge-cloud project. In addition, we expect the proposed method would be helpful for the effective allocation of limited resources when developing edge-cloud systems.},
   author = {Jong-In
and Lee Sungu
and Ryu Duksan
and Baik Jongmoon Kwon Sunjae
and Jang},
   city = {Cham},
   editor = {Anna
and Cappiello Cinzia
and Khattak Hasan Ali
and Ko InYoung
and Loseto Giuseppe
and Mrissa Michael
and Nanni Luca
and Pinoli Pietro
and Ragone Azzurra
and Ruta Michele
and Scioscia Floriano
and Srivastava Abhishek Agapito Giuseppe
and Bernasconi},
   isbn = {978-3-031-25380-5},
   booktitle = {Current Trends in Web Engineering},
   pages = {11-21},
   publisher = {Springer Nature Switzerland},
   title = {CodeBERT Based Software Defect Prediction for Edge-Cloud Systems},
   year = {2023}
}
@inproceedings{Horng-JyhPaulWuandCheng2023,
   abstract = {Lab-based teaching and learning is essential for STEM education as proficiency in the practice of programming is critical; be it for the development or operation of software applications or the provisioning of cloud services. This enables learners to practice conceptual understanding by applying it to solve real-world problems. In the face of rapid advancement and disruptions of technologies, computer lab curriculum needs to be agile to keep pace with ever changing education landscape. In this paper, we describe a case study on the journey taken by the STEM programme of Singapore University of Social Sciences to evolve its lab teaching via developing virtual lab infrastructure and continuously adapting it to teaching needs via the DevOps process. First, lab environments are migrated to cloud-based virtual machines, providing continuous access within or outside of the physical labs. Infrastructure-as-code and automation are applied to continuously develop and to deploy incremental adaptation through containerized apps in a unified workspace. These apps are customized to suit the specific requirements of the lab curriculum, be it for programming, analytics, database management or even cloud computing. Next, special purpose interactive lab guides are developed to provide automatic and interactive feedback to students who are engaged in the lab exercises. Finally, all online activities in the unified workspace are captured for analysis to assess the pedagogical and operational effectiveness of the unified lab workspace. As of date, the virtual lab infrastructure supports around 3500 students in 14 STEM courses annually.},
   author = {Casey How Kiam
and Tah Bryan Lim Yong
and Lie Toh Hong
and Beng Justin See Tiong
and Guan Roy Ong Ban
and Yi Jane Tan Jing
and Ziwen Liu
and Arcuino Conejos Sheila Maria
and Chang Luke Peh Lu
and Yongqing Zhu Horng-Jyh Paul Wu
and Cheng},
   city = {Cham},
   editor = {Dario Uden Lorna
and Liberona},
   isbn = {978-3-031-34754-2},
   booktitle = {Learning Technology for Education Challenges},
   pages = {55-68},
   publisher = {Springer Nature Switzerland},
   title = {Virtual Lab Workspace for Programming Computers – Towards Agile STEM Education},
   year = {2023}
}
@inproceedings{KozachekDianaandYaqin2023,
   abstract = {Collaborative brainstorming harbours various positive effects: Enhancement of creativity and social skills, broader discussions and contributions, and instant feedback [28, 46], while the technique of mind mapping simultaneously visualises the results of this process. Using a Virtual Reality (VR) application, a technology increasingly adopted for ideation [23, 44], this study creates a setting that allows collaborators to produce meaningful results in an immersive digital environment. By nature, group settings remain complex and dynamic, with emotions playing a significant role in the outcome [2]. So far, emotional responses have mainly been researched through biophysical responses on a single-user basis [13].},
   author = {Muhammad Ainul
and Prasad Kunal
and Wang Sheng-Ming Kozachek Diana
and Yaqin},
   city = {Cham},
   editor = {Ayako Kurosu Masaaki
and Hashizume},
   isbn = {978-3-031-35599-8},
   booktitle = {Human-Computer Interaction},
   pages = {245-263},
   publisher = {Springer Nature Switzerland},
   title = {Evaluating the Outcome of Collaborative VR Mind Mapping Sessions with Sentiment Analysis and Emotional Intelligence},
   year = {2023}
}
@inproceedings{BhattacharyaDebasisandIto2023,
   abstract = {Since the start of the COVID-19 pandemic in Spring 2020 many institutions of higher secondary education have resorted to distance education, allowing courses to be taught in an online modality and allowing students, faculty, and staff to work from home or remote locations. Various educational and distance learning tools and technologies, such as electronic mail, learning management systems, video-teleconferencing systems and content management systems have evolved to support to support distant learners. This distributed nature of higher education has led to a greater reliance for authentication tools and processes to identify students and faculty. In addition, university researchers who conduct their research from home or remote locations have an increased threat or vulnerability to hackers and criminal organizations. Finally, the educational organization itself is under greater stress to comply with local, state, and federal regulations to conform to student confidentiality and privacy laws. This paper studies the impact that Working from Home (WFH) since the start of the COVID-19 pandemic has had to the workings of the Information Technology organization at a large public university in the State of Hawaii, with a focus on privacy and confidentiality issues and the resilience of the university.},
   author = {Jodi Bhattacharya Debasis
and Ito},
   city = {Cham},
   editor = {Abbas Moallem},
   isbn = {978-3-031-35822-7},
   booktitle = {HCI for Cybersecurity, Privacy and Trust},
   pages = {435-446},
   publisher = {Springer Nature Switzerland},
   title = {Working for Home – Privacy and Confidentiality Issues in University Education},
   year = {2023}
}
@inproceedings{BrcknerAnjaandHein2023,
   abstract = {In the course of digital transformation and advancing automation through artificial intelligence (AI), the manufacturing industry, in particular small- and medium-sized enterprises (SMEs), is under pressure to move forward and face the associated challenge of redesigning both their business processes and their work organization. The importance of this transformation process increases with the introduction of new forms of human-machine collaboration, like Human-AI collaboration. To design digital transformation in a sustainable and value-adding way, well-established human-computer interaction (HCI) practices can be used as a framework. This paper explores the question of how human-centered HCI practices can be adapted to the context of the manufacturing industry, considering their unique characteristics. As a basis for this research, a literature review was conducted, examining English- and German-language articles on common HCI practices with a focus on practical use cases and empirical research in an industrial context. The review's findings show that while traditional human-centered design principles such as user-oriented design and usability engineering are broadly considered in HCI, there are several organizational conditions that need to be considered more strongly, such as the limited resources and expertise concerning digitalization available in SMEs. Therefore, this study examines the extent to which human-centered HCI practices in the manufacturing industry can be tailored to the specific characteristics of SMEs. The findings from this review can be used in future research as a basis for developing guidelines for implementing human-centered HCI practices in SMEs. Building on this, guidelines and frameworks can be developed that consider SME specifics such as flexibility and limited resources and are application-ready for practical use. This will support a human-centric digitalization process that is sustainable and value-adding in the long term.},
   author = {Philipp
and Hein-Pensel Franziska
and Mayan Jasmin
and Wölke Mandy Brückner Anja
and Hein},
   city = {Cham},
   editor = {Margherita
and Ntoa Stavroula
and Salvendy Gavriel Stephanidis Constantine
and Antona},
   isbn = {978-3-031-35989-7},
   booktitle = {HCI International 2023 Posters},
   pages = {3-15},
   publisher = {Springer Nature Switzerland},
   title = {Human-Centered HCI Practices Leading the Path to Industry 5.0: A Systematic Literature Review},
   year = {2023}
}
@inproceedings{TsigkanosChristosandRani2023,
   abstract = {When testing scientific software, it is often challenging or even impossible to craft a test oracle for checking whether the program under test produces the expected output when being executed on a given input – also known as the oracle problem in software engineering. Metamorphic testing mitigates the oracle problem by reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over three case studies of scientific software documentation, and compare variables extracted to ground truth manually labelled by experts.},
   author = {Pooja
and Müller Sebastian
and Kehrer Timo Tsigkanos Christos
and Rani},
   city = {Cham},
   editor = {Clélia
and Paszynski Maciej
and Krzhizhanovskaya Valeria V.
and Dongarra Jack J.
and Sloot Peter M A Mikyška Jiří
and de Mulatier},
   isbn = {978-3-031-35995-8},
   booktitle = {Computational Science – ICCS 2023},
   pages = {321-335},
   publisher = {Springer Nature Switzerland},
   title = {Variable Discovery with Large Language Models for Metamorphic Testing of Scientific Software},
   year = {2023}
}
@inbook{Patel2020,
   abstract = {In the preceding chapters, we have solely relied on the structure of the HTML documents themselves to scrape information from them, and that is a powerful method to extract information.},
   author = {Jay M Patel},
   city = {Berkeley, CA},
   doi = {10.1007/978-1-4842-6576-5_4},
   isbn = {978-1-4842-6576-5},
   booktitle = {Getting Structured Data from the Internet: Running Web Crawlers/Scrapers on a Big Data Production Scale},
   pages = {135-223},
   publisher = {Apress},
   title = {Natural Language Processing (NLP) and Text Analytics},
   url = {https://doi.org/10.1007/978-1-4842-6576-5_4},
   year = {2020}
}
@inbook{LyuShingandRzeznik2023,
   abstract = {Artificial intelligence and machine learning have always captured the imaginations of science fiction authors and the media. Begun in the 1950s, the field of artificial intelligence has been through multiple ups and downs. Recently, it has received more media attention again because of technology breakthroughs in deep learning and consumer-facing applications on the market, such as ChatGPT and other advanced online chatbots. In this chapter we work with some basic Artificial intellegince and Machine Learning techniques to show how Rust can be applied to this space.},
   author = {Andrew Lyu Shing
and Rzeznik},
   city = {Berkeley, CA},
   doi = {10.1007/978-1-4842-9331-7_9},
   isbn = {978-1-4842-9331-7},
   booktitle = {Practical Rust Projects: Build Serverless, AI, Machine Learning, Embedded, Game, and Web Applications},
   pages = {321-373},
   publisher = {Apress},
   title = {Artificial Intelligence and Machine Learning},
   url = {https://doi.org/10.1007/978-1-4842-9331-7_9},
   year = {2023}
}
@inbook{Jung2023,
   abstract = {Currently, blockchain technology is most commonly associated with digital currencies such as Bitcoin and Ethereum. However, the underlying technology of blockchain has the potential to be applied in a wide range of industries and systems. The decentralization of blockchain technology provides increased transparency and security in various processes, which makes it a suitable technology for real-world usages such as supply chain management, digital identity, and voting systems.},
   author = {Klaas Jung},
   city = {Berkeley, CA},
   doi = {10.1007/978-1-4842-9627-1_3},
   isbn = {978-1-4842-9627-1},
   booktitle = {The Quiet Crypto Revolution: How Blockchain and Cryptocurrency Are Changing Our Lives},
   pages = {31-64},
   publisher = {Apress},
   title = {The Future of Blockchain Technology},
   url = {https://doi.org/10.1007/978-1-4842-9627-1_3},
   year = {2023}
}
@inbook{Sorvisto2023,
   abstract = {This chapter is about infrastructure. You might think of buildings and roads when you hear the word infrastructure, but in MLOps, infrastructure refers to the most fundamental services we need to build more complex systems like training, inference, and model deployment pipelines. For example, we need a way to create data stores that can store features for model training and servers with compute and memory resources for hosting training pipelines. In the next section, we will look at a way we can simplify the process of creating infrastructure by using containers to package up software that can easily be maintained, deployed, and reproduced.},
   author = {Dayne Sorvisto},
   city = {Berkeley, CA},
   doi = {10.1007/978-1-4842-9642-4_4},
   isbn = {978-1-4842-9642-4},
   booktitle = {MLOps Lifecycle Toolkit: A Software Engineering Roadmap for Designing, Deploying, and Scaling Stochastic Systems},
   pages = {103-138},
   publisher = {Apress},
   title = {Infrastructure for MLOps},
   url = {https://doi.org/10.1007/978-1-4842-9642-4_4},
   year = {2023}
}
@inbook{Taulli2023,
   abstract = {Alignable operates a social network for small business owners. It has over 8 million members and has coverage of about 35,000 local communities.},
   author = {Tom Taulli},
   city = {Berkeley, CA},
   doi = {10.1007/978-1-4842-9852-7_5},
   isbn = {978-1-4842-9852-7},
   booktitle = {ChatGPT and Bard for Business Automation: Achieving AI-Driven Growth},
   pages = {65-89},
   publisher = {Apress},
   title = {Use Cases},
   url = {https://doi.org/10.1007/978-1-4842-9852-7_5},
   year = {2023}
}
@inbook{Taulli2023,
   abstract = {IT (Information Technology) projects are often challenging to implement, say with ERP (Enterprise Resource Planning) and CRM (Customer Relationship Management) systems. It's common for organization to release disappointing returns on investment.},
   author = {Tom Taulli},
   city = {Berkeley, CA},
   doi = {10.1007/978-1-4842-9852-7_6},
   isbn = {978-1-4842-9852-7},
   booktitle = {ChatGPT and Bard for Business Automation: Achieving AI-Driven Growth},
   pages = {91-106},
   publisher = {Apress},
   title = {Change Management},
   url = {https://doi.org/10.1007/978-1-4842-9852-7_6},
   year = {2023}
}
@inproceedings{BermeoContoJorgeandZiga-Prieto2019,
   abstract = {Specification languages offer abstractions and notations that facilitate the systematic and analytical reasoning about important aspects in a specific domain problematic. In a software engineering process domain, the usage of specification languages improve the quality and delivery time of the artefacts generated during the execution of the process activities. Cloud applications, or cloud services, are service-oriented applications whose consumption is constantly growing; however, their development require support for new roles and activities. In this work we are interested in knowing how specification languages are being used by researchers and practitioners to support the development of cloud services. This work presents a systematic mapping that provides guidance to determine the current state and to characterize the specification languages that support the service life cycle activities in a cloud services development domain.},
   author = {Miguel
and Solano-Quinde Lizandro Bermeo Conto Jorge
and Zúñiga-Prieto},
   city = {Cham},
   editor = {Guillermo
and Zúñiga-Prieto Miguel
and D'Armas Mayra
and Zúñiga Sánchez Miguel Botto-Tobar Miguel
and Pizarro},
   isbn = {978-3-030-05532-5},
   booktitle = {Technology Trends},
   pages = {72-88},
   publisher = {Springer International Publishing},
   title = {A Systematic Mapping Study of Specification Languages in Cloud Services Development},
   year = {2019}
}
@inproceedings{FernandesNatashaandDras2019,
   abstract = {We address the problem of how to ``obfuscate'' texts by removing stylistic clues which can identify authorship, whilst preserving (as much as possible) the content of the text. In this paper we combine ideas from ``generalised differential privacy'' and machine learning techniques for text processing to model privacy for text documents. We define a privacy mechanism that operates at the level of text documents represented as ``bags-of-words''—these representations are typical in machine learning and contain sufficient information to carry out many kinds of classification tasks including topic identification and authorship attribution (of the original documents). We show that our mechanism satisfies privacy with respect to a metric for semantic similarity, thereby providing a balance between utility, defined by the semantic content of texts, with the obfuscation of stylistic clues. We demonstrate our implementation on a ``fan fiction'' dataset, confirming that it is indeed possible to disguise writing style effectively whilst preserving enough information and variation for accurate content classification tasks. We refer the reader to our complete paper [15] which contains full proofs and further experimentation details.},
   author = {Mark
and McIver Annabelle Fernandes Natasha
and Dras},
   city = {Cham},
   editor = {David Nielson Flemming
and Sands},
   isbn = {978-3-030-17138-4},
   booktitle = {Principles of Security and Trust},
   pages = {123-148},
   publisher = {Springer International Publishing},
   title = {Generalised Differential Privacy for Text Document Processing},
   year = {2019}
}
@inproceedings{Lpez-HernndezJsicaandAlmela2019,
   abstract = {Automatic spelling correction is one of the most important problems in natural language processing. Its difficulty increases in medical corpora, due to the intrinsic particularities that have these texts. These features include the use of specific terminology, abbreviations, acronyms and the presence of writing errors. In this article we present a systematic review of the literature on automatic spelling detection and correction for the medical domain. There are many works on detection and automatic correction, but there is no review delving into the process of automatic correction in the medical domain. Therefore, we intend to synthesize all the existing information on this research topic and the types of studies that have been carried out to date. We present the main techniques and resources, and finally also the limitations and specific challenges. The results reflect the importance of compiling an exhaustive dictionary. In addition, the results show the ordinary use of distance algorithms of spelling and phonetic similarity, as well as with statistical techniques. The improvement of performance in recent years is especially relevant because of the use of context-based methods, such as linguistic models or neural embeddings.},
   author = {Ángela
and Valencia-García Rafael López-Hernández Jésica
and Almela},
   city = {Cham},
   editor = {Gema
and Del Cioppo-Morstadt Javier
and Vera-Lucio Néstor
and Bucaram-Leverone Martha Valencia-García Rafael
and Alcaraz-Mármol},
   isbn = {978-3-030-34989-9},
   booktitle = {Technologies and Innovation},
   pages = {95-108},
   publisher = {Springer International Publishing},
   title = {Automatic Spelling Detection and Correction in the Medical Domain: A Systematic Literature Review},
   year = {2019}
}
@inproceedings{HerruzoAnaandPashenkov2020,
   abstract = {In this paper we review the overall process for the design, development, and deployment of ``What I See Is What You Get'', an experiential installation that creates live interactive visuals, by analyzing human facial expressions and behaviors, accompanied by text generated using Machine Learning algorithms trained on the art collection of The J. Paul Getty Museum in Los Angeles. The project is developed by students and faculty in an academic environment and exhibited at the Getty Museum. We also study the pedagogical process implemented to address the curriculum's learning outcomes in an ``applied'' environment while designing a contemporary new media art piece. Special attention is paid to the level and quality of the interaction between users and the piece, demonstrating how advances in technology and computing such as Deep Learning and Natural Language Processing can contribute to deeper connections and new layers of interactivity.},
   author = {Nikita Herruzo Ana
and Pashenkov},
   city = {Cham},
   editor = {Eva Irene Brooks Anthony
and Brooks},
   isbn = {978-3-030-53294-9},
   booktitle = {Interactivity, Game Creation, Design, Learning, and Innovation},
   pages = {343-359},
   publisher = {Springer International Publishing},
   title = {``What I See Is What You Get'' Explorations of Live Artwork Generation, Artificial Intelligence, and Human Interaction in a Pedagogical Environment},
   year = {2020}
}
@inproceedings{AskarpourMehrnooshandBersani2020,
   abstract = {The general attitude of students towards formal specification and verification of systems is not exactly what one could call enthusiastic. Generally, software engineering courses at universities include an introduction to specification with formal notations such as Z, Alloy, UML, etc. However, it seems that the importance of formal specification to replicate expected system behavior does not sink in as it should with the students. Moreover, other products of computer science (e.g., machine learning algorithms, robot systems deployment), rather than software, benefit from formal specification as well. This paper is a general report of our observations on teaching formal methods on undergraduate and graduate levels at Politecnico di Milano.},
   author = {Marcello M Askarpour Mehrnoosh
and Bersani},
   city = {Cham},
   editor = {Alfredo
and Mazzara Manuel
and Meyer Bertrand
and Naumchev Alexandr
and Sadovykh Andrey Bruel Jean-Michel
and Capozucca},
   isbn = {978-3-030-57663-9},
   booktitle = {Frontiers in Software Engineering Education},
   pages = {3-18},
   publisher = {Springer International Publishing},
   title = {Teaching Formal Methods: An Experience Report},
   year = {2020}
}
@inproceedings{ZaitsevOleksandrandDucasse2020,
   abstract = {Programming is a form of communication between the person who is writing code and the one reading it. Nevertheless, very often developers neglect readability, and even well-written code becomes less understandable as software evolves. Together with the growing complexity of software systems, this creates an increasing need for automated tools for improving the readability of source code. In this work, we focus on method names and study how a descriptive name can be automatically generated from a method's body. We experiment with two approaches from the field of text summarization: One based on TF-IDF and the other on deep recurrent neural network. We collect a dataset of methods from 50 real world projects. We evaluate our approaches by comparing the generated names to the actual ones and report the result using Precision and Recall metrics. For TF-IDF, we get results as good as 28\% precision and 45\% recall; and for deep neural network, 46\% precision and 32\% recall.},
   author = {Stephane
and Bergel Alexandre
and Eveillard Mathieu Zaitsev Oleksandr
and Ducasse},
   city = {Cham},
   editor = {Fernando
and Rodrigues da Silva Alberto
and Pérez-Castillo Ricardo Shepperd Martin
and Brito e Abreu},
   isbn = {978-3-030-58793-2},
   booktitle = {Quality of Information and Communications Technology},
   pages = {93-106},
   publisher = {Springer International Publishing},
   title = {Suggesting Descriptive Method Names: An Exploratory Study of Two Machine Learning Approaches},
   year = {2020}
}
@inproceedings{BumerFrederikSandKersting2020,
   abstract = {Peer-to-Peer news portals allow Internet users to write news articles and make them available online to interested readers. Despite the fact that authors are free in their choice of topics, there are a number of quality characteristics that an article must meet before it is published. In addition to meaningful titles, comprehensibly written texts and meaningful images, relevant tags are an important criteria for the quality of such news. In this case study, we discuss the challenges and common mistakes that Peer-to-Peer reporters face when tagging news and how incorrect information can be corrected through the orchestration of existing Natural Language Processing services. Lastly, we use this illustrative example to give insight into the challenges of dealing with bottom-up taxonomies.},
   author = {Joschka
and Buff Bianca
and Geierhos Michaela Bäumer Frederik S.
and Kersting},
   city = {Cham},
   editor = {Rita
and Gudonienė Daina
and Sukackė Vilma Lopata Audrius
and Butkienė},
   isbn = {978-3-030-59506-7},
   booktitle = {Information and Software Technologies},
   pages = {368-382},
   publisher = {Springer International Publishing},
   title = {Tag Me If You Can: Insights into the Challenges of Supporting Unrestricted P2P News Tagging},
   year = {2020}
}
@inproceedings{BahramiMehdiandChen2020,
   abstract = {Web Application Programming Interface (API) allows third-party and subscribed users to access data and functions of a software application through the network or the Internet. Web APIs expose data and functions to the public users, authorized users or enterprise users. Web API providers publish API documentations to help users to understand how to interact with web-based API services, and how to use the APIs in their integration systems. The exponential raise of the number of public web service APIs may cause a challenge for software engineers to choose an efficient API. The challenge may become more complicated when web APIs updated regularly by API providers. In this paper, we introduce a novel transformation-based approach which crawls the web to collect web API documentations (unstructured documents). It generates a web API Language model from API documentations, employs different machine learning algorithms to extract information and produces a structured web API specification that compliant to Open API Specification (OAS) format. The proposed approach improves information extraction patterns and learns the variety of structured and terminologies. In our experiment, we collect a sheer number of web API documentations. Our evaluation shows that the proposed approach find RESTful API documentations with 75\% accuracy, constructs API endpoints with 84\%, constructs endpoint attributes with 95\%, and assigns endpoints to attributes with an accuracy 98\%. The proposed approach were able to produces more than 2,311 OAS web API Specifications.},
   author = {Wei-Peng Bahrami Mehdi
and Chen},
   city = {Cham},
   editor = {Yunni
and Seshadri Sangeetha
and Zhang Liang-Jie Wang Qingyang
and Xia},
   isbn = {978-3-030-59592-0},
   booktitle = {Services Computing – SCC 2020},
   pages = {103-119},
   publisher = {Springer International Publishing},
   title = {Automated Web Service Specification Generation Through a Transformation-Based Learning},
   year = {2020}
}
@inproceedings{LiuLeiandBahrami2020,
   abstract = {In recent years, Web Application Programming Interfaces (APIs) are becoming more and more popular with the development of the Internet industry and software engineering. Many companies provide public Web APIs for their services, and developers can greatly accelerate the development of new applications by relying on such APIs to execute complex tasks without implementing the corresponding functionalities themselves. The proliferation of web APIs, however, also introduces a challenge for developers to search and discover the desired API and its endpoint. This is a practical and crucial problem because according to ProgrammableWeb, there are more than 22,000 public Web APIs each of which may have tens or hundreds of endpoints. Therefore, it is difficult and time-consuming for developers to find the desired API and its endpoint to satisfy their development needs. In this paper, we present an intelligent system for Web API searches based on natural language queries by using a two-step transfer learning. To train the model, we collect a significant amount of sentences from crowdsourcing and utilize an ensemble deep learning model to predict the correct description sentences for an API and its endpoint. A training dataset is built by synthesizing the correct description sentences and then is used to train the two-step transfer learning model for Web API search. Extensive evaluation results show that the proposed methods and system can achieve high accuracy to search a Web API and its endpoint.},
   author = {Mehdi
and Park Junhee
and Chen Wei-Peng Liu Lei
and Bahrami},
   city = {Cham},
   editor = {Yasuhiko
and Serhani Mohamed Adel
and Zhang Liang-Jie Ku Wei-Shinn
and Kanemasa},
   isbn = {978-3-030-59618-7},
   booktitle = {Web Services – ICWS 2020},
   pages = {96-113},
   publisher = {Springer International Publishing},
   title = {Web API Search: Discover Web API and Its Endpoint with Natural Language Queries},
   year = {2020}
}
@inproceedings{YanWeiandWang2020,
   abstract = {It has been found that very often long queries are more challenging than short queries for information search engines to obtain good results. In this paper, we present a word embedding-based approach. First short queries or concepts are extracted from the original query. Then with the help of a trained word embedding model, all of the query elements go through a series of reformulation operations including deletion, substitution, and addition of terms so as to obtain more profitable query representations. Finally all the reformulated elements are linearly combined with the original query. Experiments are conducted on three TREC collections, and the experimental results show that the proposed method is able to improve retrieval performance on average and especially effective for long queries. Compared with several state-of-the-art baseline methods, the proposed method is very good.},
   author = {Yarong
and Huang Chunlan
and Wu Shengli Yan Wei
and Wang},
   city = {Cham},
   editor = {Xuemin
and Hendler James
and Song Wei
and Xu Zhuoming
and Liu Genggeng Wang Guojun
and Lin},
   isbn = {978-3-030-60029-7},
   booktitle = {Web Information Systems and Applications},
   pages = {202-214},
   publisher = {Springer International Publishing},
   title = {Word Embedding-Based Reformulation for Long Queries in Information Search},
   year = {2020}
}
@inproceedings{DaswaniMohinishandDesai2020,
   abstract = {In an organization as big as a university that has many distinct departments and administrative bodies, it becomes almost impossible to easily obtain information online or by other means. Assistance over the phone or in-person is often limited to office hours and the information online is scattered through numerous (often nested) web pages, often independently administered and maintained by each sub-division. In this work, we present CollegeBot, a conversational AI agent that uses natural language processing and machine learning to assist visitors of a university's web site in easily locating information related to their queries. We discuss how we create the knowledge base by collecting and appropriately preprocessing information that is used to train the conversational agent for answering domain-specific questions. We have evaluated two different algorithms for training the conversational model for the chatbot, namely a semantic similarity model and a deep learning one leveraging Sequence-to-Sequence learning model. The proposed system is able to capture the user's intent and switch context appropriately. It also leverages the open source AIML chatbot ALICE to answer any generic (non domain-specific) questions. We present a proof-of-concept prototype for San Jose State University, to demonstrate how such an approach can be easily adopted by other academic institutions as well.},
   author = {Kavina
and Patel Mili
and Vani Reeya
and Eirinaki Magdalini Daswani Mohinish
and Desai},
   city = {Cham},
   editor = {Masaaki
and Degen Helmut
and Reinerman-Jones Lauren Stephanidis Constantine
and Kurosu},
   isbn = {978-3-030-60117-1},
   booktitle = {HCI International 2020 - Late Breaking Papers: Multimodality and Intelligence},
   pages = {44-63},
   publisher = {Springer International Publishing},
   title = {CollegeBot: A Conversational AI Approach to Help Students Navigate College},
   year = {2020}
}
@inproceedings{RughbeerYastilandPillay2020,
   abstract = {Information Retrieval is the task of satisfying an information need by retrieving relevant information from large collections. Recently, deep neural networks have achieved several performance breakthroughs in the field, owing to the availability of large-scale training sets. When training data is limited, however, neural retrieval systems vastly underperform. To compensate for the lack of training data, researchers have turned to transfer learning by relying on labelled data from other search domains. Despite having access to several publicly available datasets, researchers are currently unguided in selecting the best training set for their particular applications. To address this knowledge gap, we propose a rigorous method to select an optimal training set for a specific search domain. We validate this method on the TREC-COVID challenge, which was organized by the Allen Institute for Artificial Intelligence and the National Institute of Standards and Technology. Our neural model ranked first from 143 competing systems. More importantly, it was able to achieve this result by training on a dataset that was selected using our proposed method. This work highlights the performance gains that may be achieved through careful dataset selection in transfer learning.},
   author = {Anban W.
and Jembere Edgar Rughbeer Yastil
and Pillay},
   city = {Cham},
   editor = {Aurona Gerber},
   isbn = {978-3-030-66151-9},
   booktitle = {Artificial Intelligence Research},
   pages = {53-65},
   publisher = {Springer International Publishing},
   title = {Dataset Selection for Transfer Learning in Information Retrieval},
   year = {2020}
}
@inproceedings{Gonzlez-TablasAnaIandRashed2022,
   abstract = {Several frameworks that cover cyber security education and professional development have been introduced as a guidance for learners, educators and professionals to the different knowledge areas of the field. One of the most important frameworks is the Cyber Security Body of Knowledge (CyBOK). In this paper, we apply the BERTopic topic modeling technique to CyBOK. We aim, by using this technique, to identify the most relevant topics related to each CyBOK's knowledge area in an automated way. Our results indicate that it is possible to find a meaningful topic model describing CyBOK and, thus, suggests the possibility of applying related techniques to texts to identify their main themes.},
   author = {Mohammed González-Tablas Ana I.
and Rashed},
   city = {Cham},
   editor = {Steven Clarke Nathan
and Furnell},
   isbn = {978-3-031-12172-2},
   booktitle = {Human Aspects of Information Security and Assurance},
   pages = {49-65},
   publisher = {Springer International Publishing},
   title = {Exploring CyBOK with Topic Modeling Techniques},
   year = {2022}
}
@inproceedings{ZhuFangyiandTan2022,
   abstract = {Large neural language models are steadily contributing state-of-the-art performance to question answering and other natural language and information processing tasks. These models are expensive to train. We propose to evaluate whether such pre-trained models can benefit from the addition of explicit linguistics information without requiring retraining from scratch.},
   author = {Lok You
and Ng See-Kiong
and Bressan Stéphane Zhu Fangyi
and Tan},
   city = {Cham},
   editor = {Alfredo
and Kotsis Gabriele
and Tjoa A Min
and Khalil Ismail Strauss Christine
and Cuzzocrea},
   isbn = {978-3-031-12423-5},
   booktitle = {Database and Expert Systems Applications},
   pages = {17-31},
   publisher = {Springer International Publishing},
   title = {Syntax-Informed Question Answering with Heterogeneous Graph Transformer},
   year = {2022}
}
@inproceedings{SasakiTaikiandIto2022,
   abstract = {In recent years, scholarly databases have made many scientific papers available on the Internet. While these databases facilitate access to excellent papers, they also increase the possibility of encountering inferior papers. However, it is difficult to predict the quality of a paper just from a glance at the paper. In this paper, we propose a machine learning approach to predicting the quality of scientific papers. Specifically, we predict the quality of an article by classifying for the abstract of the paper whether the article is included in a superior journal or not. The proposed model is trained using a BERT-based model widely used in natural language processing. After training, we achieved a test accuracy of 95.1\% and 89.6\% in medicine and computer science, respectively. In addition, the results of the classification are visualized by evaluating the sentence combinations in the abstract to clarify the details of the classification.},
   author = {Yasuaki
and Nakano Koji
and Kasagi Akihiko Sasaki Taiki
and Ito},
   city = {Cham},
   editor = {Plamen
and Jayne Chrisina
and Papaleonidas Antonios
and Aydin Mehmet Pimenidis Elias
and Angelov},
   isbn = {978-3-031-15937-4},
   booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2022},
   pages = {212-223},
   publisher = {Springer Nature Switzerland},
   title = {BERT-Based Scientific Paper Quality Prediction},
   year = {2022}
}
@inproceedings{Benito-SantiagoHermiloandCrdova-Esparza2022,
   abstract = {This research conducted a systematic review of related works on machine translation of languages with low digital resources. First, we carried out the information search in the databases: ScienceDirect, IEEE Xplore, ACM Digital Library. Eighteen articles were collected following inclusion and exclusion criteria, considering a search period from 2016 to 2022. Subsequently, we analyzed and classified these articles according to the libraries developed and/or used based on machine learning, statistics, or grammar. The results indicate that pre-training and morphological segmentation techniques with finite state machines and machine learning techniques improve the translation of languages with low digital resources. In addition, according to the articles compiled in the specialized databases, in Mexico, unlike other countries that we analyzed, there are few publications on the translation of languages with low digital resources, and we mostly found research papers published in international conferences.},
   author = {Diana Margarita
and Castro-Sánchez Noé Alejandro
and Herrera-Navarro Ana-Marcela Benito-Santiago Hermilo
and Córdova-Esparza},
   city = {Cham},
   editor = {Juan
and Martínez Seis Bella Pichardo Lagunas Obdulia
and Martínez-Miranda},
   isbn = {978-3-031-19496-2},
   booktitle = {Advances in Computational Intelligence},
   pages = {41-56},
   publisher = {Springer Nature Switzerland},
   title = {Machine Translation of Texts from Languages with Low Digital Resources: A Systematic Review},
   year = {2022}
}
@inproceedings{Morales-HernndezRobertoCarlosandBecerra-Alonso2022,
   abstract = {The scientific articles identification with the 17 sustainable development goals of the UN 2030 Agenda is a valuable task for research and educational institutions. Finding an efficient and practical multi-label classification model using machine or deep learning remains relevant. This work refers to the performance comparison of a text classification model that combines Label Powerset (LP) and Support Vector Machine (SVM) against a transfer learning language model such as DistilBERT in 5 different imbalanced and balanced dataset scenarios of scientific papers. A proposed classification process was implemented with performance metrics, which have confirmed that the combination LP-SVM continues to be an option with remarkable results in multi-label text classification.},
   author = {David
and Vivas Eduardo Romero
and Gutiérrez Joaquín Morales-Hernández Roberto Carlos
and Becerra-Alonso},
   city = {Cham},
   editor = {Juan
and Martínez Seis Bella Pichardo Lagunas Obdulia
and Martínez-Miranda},
   isbn = {978-3-031-19496-2},
   booktitle = {Advances in Computational Intelligence},
   pages = {57-67},
   publisher = {Springer Nature Switzerland},
   title = {Comparison Between SVM and DistilBERT for Multi-label Text Classification of Scientific Papers Aligned with Sustainable Development Goals},
   year = {2022}
}
@inproceedings{Nez-RobinsonDanielandTalavera-Montalto2022,
   abstract = {Transformer models have evolved natural language processing tasks in machine learning and set a new standard for the state of the art. Thanks to the self-attention component, these models have achieved significant improvements in text generation tasks (such as extractive and abstractive text summarization). However, research works involving text summarization and the legal domain are still in their infancy, and as such, benchmarks and a comparative analysis of these state of the art models is important for the future of text summarization of this highly specialized task. In order to contribute to these research works, the researchers propose a comparative analysis of different, fine-tuned Transformer models and datasets in order to provide a better understanding of the task at hand and the challenges ahead. The results show that Transformer models have improved upon the text summarization task, however, consistent and generalized learning is a challenge that still exists when training the models with large text dimensions. Finally, after analyzing the correlation between objective results and human opinion, the team concludes that the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [13] metrics used in the current state of the art are limited and do not reflect the precise quality of a generated summary.},
   author = {Jose
and Ugarte Willy Núñez-Robinson Daniel
and Talavera-Montalto},
   city = {Cham},
   editor = {Filipe
and Augusto Maria Fernanda Guarda Teresa
and Portela},
   isbn = {978-3-031-20319-0},
   booktitle = {Advanced Research in Technologies, Information, Innovation and Sustainability},
   pages = {372-386},
   publisher = {Springer Nature Switzerland},
   title = {A Comparative Analysis on the Summarization of Legal Texts Using Transformer Models},
   year = {2022}
}
@inproceedings{LiMinandLi2022,
   abstract = {Provide auxiliary tools for Tibetan vocabulary learning, and solve the problems of long response time and poor application performance of existing online learning systems. Using the multi-terminal fusion technology, the optimized design of the Tibetan vocabulary online learning system is realized from the three aspects of hardware, database and software functions. Adjust the connection mode of the server, and modify hardware devices such as embedded processors and resource collectors. Use the optimized system circuit to connect hardware devices to complete the optimization of the hardware system. Collect Tibetan vocabulary online learning resource data, and connect each database table according to the logical relationship between the data. With the support of hardware devices and databases, set permissions for different users and choose the online learning mode of Tibetan vocabulary. Use multi-terminal fusion technology to achieve resource integration. Realize system functions such as Tibetan vocabulary learning information retrieval, online interactive practice, and online testing. Through system testing, it is found that the operating success rate of the designed system reaches 99.2\%, and the response time is less than 6000ms. And through the application of the design system, the students' Tibetan language scores have been significantly improved.},
   author = {Nan Li Min
and Li},
   city = {Cham},
   editor = {Guanglu Fu Weina
and Sun},
   isbn = {978-3-031-21161-4},
   booktitle = {e-Learning, e-Education, and Online Training},
   pages = {211-228},
   publisher = {Springer Nature Switzerland},
   title = {Design of Tibetan Vocabulary Online Learning System Based on Multi-terminal Integration},
   year = {2022}
}
@inproceedings{MarquezBogartYailandAlanis2022,
   abstract = {In the last decade, Artificial Intelligence (AI) has become lead on the field of information generation and processing tasks through the emergence of Machine Learning (ML), as well as the data specialist mentions Machine Learning is a master of pattern recognition, and is capable of transform a data sample into a computer program capable of drawing inferences from new data sets for which it has not been previously trained, based on artificial neural networks (ANN) processing in academic texts, which are used to identify patterns and classify different types of information, currently treated as Deep Learning (DL) which is a subset of Machine Learning, this algorithm tries to imitate the human brain by continuously analyzing data with a given logical structure, which has led to its applicability to different fields such as robotics, voice processing, artificial vision, natural language processing (NLP), with the intention to provide computer systems with the ability to learn. Natural language processing has traditionally been a complex and non-trivial task in algorithm design. Making use of AI, new thresholds are being reached in the state of the art of different problems and with constant advances in the models in use, they are being reached faster and faster.},
   author = {Arnulfo
and Magdaleno-Palencia Jose Sergio
and Quezada Angeles Marquez Bogart Yail
and Alanis},
   city = {Cham},
   editor = {Filipe
and Augusto Maria Fernanda Guarda Teresa
and Portela},
   isbn = {978-3-031-20319-0},
   booktitle = {Advanced Research in Technologies, Information, Innovation and Sustainability},
   pages = {535-545},
   publisher = {Springer Nature Switzerland},
   title = {Artificial Neural Networks Applied to Natural Language Processing in Academic Texts},
   year = {2022}
}
@inproceedings{Ardimento2022,
   abstract = {The problem of bug-fixing time can be treated as a supervised text categorization task in Natural Language Processing. In recent years, following the use of deep learning also in the field of Natural Language Processing, pre-trained contextualized representations of words have become widespread. One of the most used pre-trained language representations models is named Google BERT (hereinafter, for brevity, BERT). BERT uses a self-attention mechanism that allows learning the bidirectional context representation of a word in a sentence, which constitutes one of the main advantages over the previously proposed solutions. However, due to the large size of BERT, it is difficult for it to put it into production. To address this issue, a smaller, faster, cheaper and lighter version of BERT, named DistilBERT, has been introduced at the end of 2019. This paper compares the efficacy of BERT and DistilBERT, combined with the Logistic Regression, in predicting bug-fixing time from bug reports of a large-scale open-source software project, LiveCode. In the experimentation carried out, DistilBERT retains almost 100\% of its language understanding capabilities and, in the best case, it is 63.28\% faster than BERT. Moreover, with a not time-consuming tuning of the C parameter in Logistic Regression, the DistilBERT provides an accuracy value even better than BERT.},
   author = {Pasquale Ardimento},
   city = {Cham},
   editor = {Marco
and Mikkonen Tommi
and Klünder Jil
and Abrahamsson Pekka Taibi Davide
and Kuhrmann},
   isbn = {978-3-031-21388-5},
   booktitle = {Product-Focused Software Process Improvement},
   pages = {610-620},
   publisher = {Springer International Publishing},
   title = {Predicting Bug-Fixing Time: DistilBERT Versus Google BERT},
   year = {2022}
}
@inbook{Evrard2023,
   abstract = {The Transformer, a model relying entirely on the attention mechanism, brought significant improvements in performance on several natural language processing tasks. This chapter presents its impact on the speech processing domain and, more specifically, on the automatic speech recognition task. A short history of the evolution of automatic speech recognition systems is also given. A selection of important works making use of transformers are presented, as well as pretraining self-supervised architectures.},
   author = {Marc Evrard},
   city = {Cham},
   doi = {10.1007/978-3-031-24349-3_8},
   editor = {Virginia
and Lukowicz Paul
and Sierra Carles Chetouani Mohamed
and Dignum},
   isbn = {978-3-031-24349-3},
   booktitle = {Human-Centered Artificial Intelligence: Advanced Lectures},
   pages = {123-139},
   publisher = {Springer International Publishing},
   title = {Transformers in Automatic Speech Recognition},
   url = {https://doi.org/10.1007/978-3-031-24349-3_8},
   year = {2023}
}
@inproceedings{KwonSunjaeandJang2023-2,
   abstract = {Edge-cloud system is a crucial computing infrastructure for the innovations of modern society. In addition, the high interest in the edge-cloud system leads to various studies for testing to ensure the reliability of the system. However, like traditional software systems, the amount of resources for testing is always limited. Thus, we suggest CodeBERT Based Just-In-Time (JIT) Software Defect Prediction (SDP) model to address the limitation. This method helps practitioners prioritize the limited testing resources for the defect-prone functions in commits and improves the system's reliability. We generate GitHub Pull-Request (GHPR) datasets on two open-source framework projects for edge-cloud system in GitHub. After that, we evaluate the performance of the proposed model on the GHPR datasets in within-project environment and cross-project environment. To the best of our knowledge, it is the first attempt to apply SDP to edge-cloud systems, and as a result of the evaluation, we can confirm the applicability of JIT SDP in edge-cloud project. In addition, we expect the proposed method would be helpful for the effective allocation of limited resources when developing edge-cloud systems.},
   author = {Jong-In
and Lee Sungu
and Ryu Duksan
and Baik Jongmoon Kwon Sunjae
and Jang},
   city = {Cham},
   editor = {Anna
and Cappiello Cinzia
and Khattak Hasan Ali
and Ko InYoung
and Loseto Giuseppe
and Mrissa Michael
and Nanni Luca
and Pinoli Pietro
and Ragone Azzurra
and Ruta Michele
and Scioscia Floriano
and Srivastava Abhishek Agapito Giuseppe
and Bernasconi},
   isbn = {978-3-031-25380-5},
   booktitle = {Current Trends in Web Engineering},
   pages = {11-21},
   publisher = {Springer Nature Switzerland},
   title = {CodeBERT Based Software Defect Prediction for Edge-Cloud Systems},
   year = {2023}
}
@inbook{Reddy2023,
   abstract = {Modern enterprises are evolving into a complex system of systems that need to deliver stated goals while operating in a dynamic and uncertain environment. Given the ever-increasing pervasiveness of software, enterprises are relying heavily on software systems to address a variety of adaptive needs at strategy, process and system levels. The demand-supply situation for trained software developers is already skewed, and the skew is likely to increase further with time. Recent advances in AI techniques in general and Generative AI in particular may lead to a promising solution wherein Subject Matter Experts are empowered to play a greater and more direct role in software development. This chapter motivates the need, proposes a pragmatic line of attack and discusses technology enablers to support this line of attack.},
   author = {Sreedhar Reddy},
   city = {Cham},
   doi = {10.1007/978-3-031-29053-4_5},
   isbn = {978-3-031-29053-4},
   booktitle = {The AI-Enabled Enterprise},
   pages = {85-100},
   publisher = {Springer International Publishing},
   title = {Democratized Hyper-automated Software Development},
   url = {https://doi.org/10.1007/978-3-031-29053-4_5},
   year = {2023}
}
@inbook{SharoffSergeandRapp2023,
   abstract = {In a parallel corpus we know which document is a translation of what by design. If the link between documents in different languages is not known, it needs to be established. In this chapter we will discuss methods for measuring document similarity across languages and how to evaluate the results. Then, we will proceed to discussing methods for building comparable corpora of different degrees of comparability and for different tasks.},
   author = {Reinhard
and Zweigenbaum Pierre Sharoff Serge
and Rapp},
   city = {Cham},
   doi = {10.1007/978-3-031-31384-4_3},
   isbn = {978-3-031-31384-4},
   booktitle = {Building and Using Comparable Corpora for Multilingual Natural Language Processing},
   pages = {17-37},
   publisher = {Springer International Publishing},
   title = {Building Comparable Corpora},
   url = {https://doi.org/10.1007/978-3-031-31384-4_3},
   year = {2023}
}
@inproceedings{Horng-JyhPaulWuandCheng2023,
   abstract = {Lab-based teaching and learning is essential for STEM education as proficiency in the practice of programming is critical; be it for the development or operation of software applications or the provisioning of cloud services. This enables learners to practice conceptual understanding by applying it to solve real-world problems. In the face of rapid advancement and disruptions of technologies, computer lab curriculum needs to be agile to keep pace with ever changing education landscape. In this paper, we describe a case study on the journey taken by the STEM programme of Singapore University of Social Sciences to evolve its lab teaching via developing virtual lab infrastructure and continuously adapting it to teaching needs via the DevOps process. First, lab environments are migrated to cloud-based virtual machines, providing continuous access within or outside of the physical labs. Infrastructure-as-code and automation are applied to continuously develop and to deploy incremental adaptation through containerized apps in a unified workspace. These apps are customized to suit the specific requirements of the lab curriculum, be it for programming, analytics, database management or even cloud computing. Next, special purpose interactive lab guides are developed to provide automatic and interactive feedback to students who are engaged in the lab exercises. Finally, all online activities in the unified workspace are captured for analysis to assess the pedagogical and operational effectiveness of the unified lab workspace. As of date, the virtual lab infrastructure supports around 3500 students in 14 STEM courses annually.},
   author = {Casey How Kiam
and Tah Bryan Lim Yong
and Lie Toh Hong
and Beng Justin See Tiong
and Guan Roy Ong Ban
and Yi Jane Tan Jing
and Ziwen Liu
and Arcuino Conejos Sheila Maria
and Chang Luke Peh Lu
and Yongqing Zhu Horng-Jyh Paul Wu
and Cheng},
   city = {Cham},
   editor = {Dario Uden Lorna
and Liberona},
   isbn = {978-3-031-34754-2},
   booktitle = {Learning Technology for Education Challenges},
   pages = {55-68},
   publisher = {Springer Nature Switzerland},
   title = {Virtual Lab Workspace for Programming Computers – Towards Agile STEM Education},
   year = {2023}
}
@inproceedings{KozachekDianaandYaqin2023,
   abstract = {Collaborative brainstorming harbours various positive effects: Enhancement of creativity and social skills, broader discussions and contributions, and instant feedback [28, 46], while the technique of mind mapping simultaneously visualises the results of this process. Using a Virtual Reality (VR) application, a technology increasingly adopted for ideation [23, 44], this study creates a setting that allows collaborators to produce meaningful results in an immersive digital environment. By nature, group settings remain complex and dynamic, with emotions playing a significant role in the outcome [2]. So far, emotional responses have mainly been researched through biophysical responses on a single-user basis [13].},
   author = {Muhammad Ainul
and Prasad Kunal
and Wang Sheng-Ming Kozachek Diana
and Yaqin},
   city = {Cham},
   editor = {Ayako Kurosu Masaaki
and Hashizume},
   isbn = {978-3-031-35599-8},
   booktitle = {Human-Computer Interaction},
   pages = {245-263},
   publisher = {Springer Nature Switzerland},
   title = {Evaluating the Outcome of Collaborative VR Mind Mapping Sessions with Sentiment Analysis and Emotional Intelligence},
   year = {2023}
}
@inproceedings{BhattacharyaDebasisandIto2023,
   abstract = {Since the start of the COVID-19 pandemic in Spring 2020 many institutions of higher secondary education have resorted to distance education, allowing courses to be taught in an online modality and allowing students, faculty, and staff to work from home or remote locations. Various educational and distance learning tools and technologies, such as electronic mail, learning management systems, video-teleconferencing systems and content management systems have evolved to support to support distant learners. This distributed nature of higher education has led to a greater reliance for authentication tools and processes to identify students and faculty. In addition, university researchers who conduct their research from home or remote locations have an increased threat or vulnerability to hackers and criminal organizations. Finally, the educational organization itself is under greater stress to comply with local, state, and federal regulations to conform to student confidentiality and privacy laws. This paper studies the impact that Working from Home (WFH) since the start of the COVID-19 pandemic has had to the workings of the Information Technology organization at a large public university in the State of Hawaii, with a focus on privacy and confidentiality issues and the resilience of the university.},
   author = {Jodi Bhattacharya Debasis
and Ito},
   city = {Cham},
   editor = {Abbas Moallem},
   isbn = {978-3-031-35822-7},
   booktitle = {HCI for Cybersecurity, Privacy and Trust},
   pages = {435-446},
   publisher = {Springer Nature Switzerland},
   title = {Working for Home – Privacy and Confidentiality Issues in University Education},
   year = {2023}
}
@inproceedings{BrcknerAnjaandHein2023,
   abstract = {In the course of digital transformation and advancing automation through artificial intelligence (AI), the manufacturing industry, in particular small- and medium-sized enterprises (SMEs), is under pressure to move forward and face the associated challenge of redesigning both their business processes and their work organization. The importance of this transformation process increases with the introduction of new forms of human-machine collaboration, like Human-AI collaboration. To design digital transformation in a sustainable and value-adding way, well-established human-computer interaction (HCI) practices can be used as a framework. This paper explores the question of how human-centered HCI practices can be adapted to the context of the manufacturing industry, considering their unique characteristics. As a basis for this research, a literature review was conducted, examining English- and German-language articles on common HCI practices with a focus on practical use cases and empirical research in an industrial context. The review's findings show that while traditional human-centered design principles such as user-oriented design and usability engineering are broadly considered in HCI, there are several organizational conditions that need to be considered more strongly, such as the limited resources and expertise concerning digitalization available in SMEs. Therefore, this study examines the extent to which human-centered HCI practices in the manufacturing industry can be tailored to the specific characteristics of SMEs. The findings from this review can be used in future research as a basis for developing guidelines for implementing human-centered HCI practices in SMEs. Building on this, guidelines and frameworks can be developed that consider SME specifics such as flexibility and limited resources and are application-ready for practical use. This will support a human-centric digitalization process that is sustainable and value-adding in the long term.},
   author = {Philipp
and Hein-Pensel Franziska
and Mayan Jasmin
and Wölke Mandy Brückner Anja
and Hein},
   city = {Cham},
   editor = {Margherita
and Ntoa Stavroula
and Salvendy Gavriel Stephanidis Constantine
and Antona},
   isbn = {978-3-031-35989-7},
   booktitle = {HCI International 2023 Posters},
   pages = {3-15},
   publisher = {Springer Nature Switzerland},
   title = {Human-Centered HCI Practices Leading the Path to Industry 5.0: A Systematic Literature Review},
   year = {2023}
}
@inproceedings{TsigkanosChristosandRani2023,
   abstract = {When testing scientific software, it is often challenging or even impossible to craft a test oracle for checking whether the program under test produces the expected output when being executed on a given input – also known as the oracle problem in software engineering. Metamorphic testing mitigates the oracle problem by reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over three case studies of scientific software documentation, and compare variables extracted to ground truth manually labelled by experts.},
   author = {Pooja
and Müller Sebastian
and Kehrer Timo Tsigkanos Christos
and Rani},
   city = {Cham},
   editor = {Clélia
and Paszynski Maciej
and Krzhizhanovskaya Valeria V.
and Dongarra Jack J.
and Sloot Peter M A Mikyška Jiří
and de Mulatier},
   isbn = {978-3-031-35995-8},
   booktitle = {Computational Science – ICCS 2023},
   pages = {321-335},
   publisher = {Springer Nature Switzerland},
   title = {Variable Discovery with Large Language Models for Metamorphic Testing of Scientific Software},
   year = {2023}
}
@inproceedings{VidgofMaximandBachhofner2023,
   abstract = {Large language models are deep learning models with a large number of parameters. The models made noticeable progress on a large number of tasks, and as a consequence allowing them to serve as valuable and versatile tools for a diverse range of applications. Their capabilities also offer opportunities for business process management, however, these opportunities have not yet been systematically investigated. In this paper, we address this research problem by foregrounding various management tasks of the BPM lifecycle. We investigate six research directions highlighting problems that need to be addressed when using large language models, including usage guidelines for practitioners.},
   author = {Stefan
and Mendling Jan Vidgof Maxim
and Bachhofner},
   city = {Cham},
   editor = {Andrea
and Janiesch Christian
and Sadiq Shazia Di Francescomarino Chiara
and Burattin},
   isbn = {978-3-031-41623-1},
   booktitle = {Business Process Management Forum},
   pages = {107-123},
   publisher = {Springer Nature Switzerland},
   title = {Large Language Models for Business Process Management: Opportunities and Challenges},
   year = {2023}
}
@inproceedings{KlievtsovaNataliiaandBenzin2023,
   abstract = {Chatbots such as ChatGPT have caused tremendous hype lately. For BPM applications, it is often not clear how to apply chatbots to generate business value. Hence, this work aims at the systematic analysis of existing chatbots for their support of conversational process modelling as a process-oriented capability. Application scenarios are identified along the process life cycle. Then a systematic literature review on conversational process modelling is performed. The resulting taxonomy serves as input for the identification of application scenarios for conversational process modelling, including paraphrasing and improvement of process descriptions. The application scenarios are evaluated for existing chatbots based on a real-world test set from the higher education domain. It contains process descriptions as well as corresponding process models, together with an assessment of the model quality. Based on the literature and application scenario analyses, recommendations for the usage (practical implications) and further development (research directions) of conversational process modelling are derived.},
   author = {Janik-Vasily
and Kampik Timotheus
and Mangler Juergen
and Rinderle-Ma Stefanie Klievtsova Nataliia
and Benzin},
   city = {Cham},
   editor = {Andrea
and Janiesch Christian
and Sadiq Shazia Di Francescomarino Chiara
and Burattin},
   isbn = {978-3-031-41623-1},
   booktitle = {Business Process Management Forum},
   pages = {319-336},
   publisher = {Springer Nature Switzerland},
   title = {Conversational Process Modelling: State of the Art, Applications, and Implications in Practice},
   year = {2023}
}
@inproceedings{DeBuitlearCaoimheandByrne2023,
   abstract = {AI-based systems are becoming increasingly prominent in everyday life, from smart assistants like Amazon's Alexa to their use in the healthcare industry. With this rise, the evidence of bias in AI-based systems has also been witnessed. The effects of this bias on the groups of people targeted can range from inconvenient to life-threatening. As AI-based systems continue to be developed and used, it is important that this bias should be eliminated as much as possible. Through the findings of a multivocal literature review (MLR), we aim to understand what AI-based systems are, what bias is and the types of bias these systems have, the potential risks and effects of this bias, and how to reduce bias in AI-based systems. In conclusion, addressing and mitigating biases in AI-based systems is crucial for fostering equitable and trustworthy applications; by proactively identifying these biases and implementing strategies to counteract them, we can contribute to the development of more responsible and inclusive AI technologies that benefit all users.},
   author = {Ailbhe
and McEvoy Eric
and Camara Abasse
and Yilmaz Murat
and McCarren Andrew
and Clarke Paul M De Buitlear Caoimhe
and Byrne},
   city = {Cham},
   editor = {Paul
and Riel Andreas
and Messnarz Richard Yilmaz Murat
and Clarke},
   isbn = {978-3-031-42307-9},
   booktitle = {Systems, Software and Services Process Improvement},
   pages = {20-35},
   publisher = {Springer Nature Switzerland},
   title = {Investigating Sources and Effects of Bias in AI-Based Systems – Results from an MLR},
   year = {2023}
}
@inproceedings{KathHannesandLers2023,
   abstract = {Dialogue systems are an important and very active research area with many practical applications. However, researchers and practitioners new to the field may have difficulty with the categorisation, number and terminology of existing free and commercial systems. Our paper aims to achieve two main objectives. Firstly, based on our structured literature review, we provide a categorisation of dialogue systems according to the objective, modality, domain, architecture, and model, and provide information on the correlations among these categories. Secondly, we summarise and compare frameworks and applications of intelligent virtual assistants, commercial frameworks, research dialogue systems, and large language models according to these categories and provide system recommendations for researchers new to the field.},
   author = {Bengt
and Gouvêa Thiago S.
and Sonntag Daniel Kath Hannes
and Lüers},
   city = {Cham},
   editor = {Alexander Seipel Dietmar
and Steen},
   isbn = {978-3-031-42608-7},
   booktitle = {KI 2023: Advances in Artificial Intelligence},
   pages = {98-113},
   publisher = {Springer Nature Switzerland},
   title = {Lost in Dialogue: A Review and Categorisation of Current Dialogue System Approaches and Technical Solutions},
   year = {2023}
}
@inproceedings{JainRaghavandVerma2023,
   abstract = {Customer reviews often contain valuable feedback about a product or service, but it can be challenging to extract specific complaints and their underlying causes from the text. Despite the use of various methods to detect and analyze complaints, no studies have concentrated on thoroughly examining complaints at the aspect-level and the underlying reasons for such aspect-level complaints. We add the rationale annotation for the aspect-based complaint classes in a publicly available benchmark multimodal complaint dataset (CESAMARD), which spans five domains (books, electronics, edibles, fashion, and miscellaneous). Current multimodal complaint detection methods treat these tasks as classification problems and do not utilize external knowledge. The present study aims to tackle these concerns. We propose a knowledge-infused unified Multimodal Generative framework for Aspect-based complaint and Cause detection (MuGACD) by reframing the multitasking problem as a multimodal text-to-text generation task. Our proposed methodology established a benchmark performance in the novel aspect-based complaint and cause detection task based on extensive evaluation. We also demonstrated that our model consistently outperformed all other baselines and state-of-the-art models in both full and few-shot settings (The dataset and code are available at https://github.com/Raghav10j/ECML23).},
   author = {Apoorv
and Singh Apoorva
and Gangwar Vivek
and Saha Sriparna Jain Raghav
and Verma},
   city = {Cham},
   editor = {Claudia
and Ruchansky Natali
and Kourtellis Nicolas
and Baralis Elena
and Bonchi Francesco De Francisci Morales Gianmarco
and Perlich},
   isbn = {978-3-031-43427-3},
   booktitle = {Machine Learning and Knowledge Discovery in Databases: Applied Data Science and Demo Track},
   pages = {88-104},
   publisher = {Springer Nature Switzerland},
   title = {Aspect-Based Complaint and Cause Detection: A Multimodal Generative Framework with External Knowledge Infusion},
   year = {2023}
}
@inproceedings{KrollHermannandKreutz2023,
   abstract = {Digital libraries oftentimes provide access to historical newspaper archives via keyword-based search. Historical figures and their roles are particularly interesting cognitive access points in historical research. Structuring and clustering news articles would allow more sophisticated access for users to explore such information. However, real-world limitations such as the lack of training data, licensing restrictions and non-English text with OCR errors make the composition of such a system difficult and cost-intensive in practice. In this work we tackle these issues with the showcase of the National Library of the Netherlands by introducing a role-based interface that structures news articles on historical persons. In-depth, component-wise evaluations and interviews with domain experts highlighted our prototype's effectiveness and appropriateness for a real-world digital library collection.},
   author = {Christin Katharina
and Cuper Mirjam
and Thang Bill Matthias
and Balke Wolf-Tilo Kroll Hermann
and Kreutz},
   city = {Cham},
   editor = {Helena
and Silvello Gianmaria
and Marrero Mónica
and Teixeira Lopes Carla
and Marchesin Stefano Alonso Omar
and Cousijn},
   isbn = {978-3-031-43849-3},
   booktitle = {Linking Theory and Practice of Digital Libraries},
   pages = {31-46},
   publisher = {Springer Nature Switzerland},
   title = {Aspect-Driven Structuring of Historical Dutch Newspaper Archives},
   year = {2023}
}
@inproceedings{SaitoKenjiandKobayashi2023,
   abstract = {Through simulated experiences, RPGs aid children's exploration of issues, but their design can be labor-intensive. In our recent Academy Camp, children rapidly created, played, and reflected on their own RPGs using a large language model (GPT-4) for topics in which they were interested. This fast-paced, cyclical process has a double impact on children's comprehension of social issues: first through game design, then through game play. We anticipate further applications for this method.},
   author = {Kayo
and Takekoshi Waki
and Hashimoto Atsuki
and Hirai Nobukazu
and Kimura Akifumi
and Takahashi Asuka
and Yoshioka Naoki
and Mano Asuto Saito Kenji
and Kobayashi},
   city = {Cham},
   editor = {Alberto
and Göbel Stefan Haahr Mads
and Rojas-Salazar},
   isbn = {978-3-031-44751-8},
   booktitle = {Serious Games},
   pages = {274-289},
   publisher = {Springer Nature Switzerland},
   title = {Double Impact: Children's Serious RPG Generation/Play with a Large Language Model for Their Deeper Engagement in Social Issues},
   year = {2023}
}
@inproceedings{PavanelliLucasandGumiel2023,
   abstract = {The biomedical NLP community has seen great advances in dataset development mostly for the English language, which has hindered progress in the field, as other languages are still underrepresented. This study introduces a dataset of Brazilian Portuguese annotated for named entity recognition and relation extraction in the healthcare domain. We compiled and annotated a corpus of health professionals' responses to frequently asked questions in online healthcare forums on diabetes. We measured inter-annotator agreement and conducted initial experiments using up-to-date methods to recognize entities and extract relations, such as BERT-based ones. Data, models, and results are publicly available at https://github.com/pavalucas/Bete.},
   author = {Yohan Bonescki
and Ferreira Thiago
and Pagano Adriana
and Laber Eduardo Pavanelli Lucas
and Gumiel},
   city = {Cham},
   editor = {Reinaldo A C Naldi Murilo C.
and Bianchi},
   isbn = {978-3-031-45392-2},
   booktitle = {Intelligent Systems},
   pages = {256-267},
   publisher = {Springer Nature Switzerland},
   title = {Bete: A Brazilian Portuguese Dataset for Named Entity Recognition and Relation Extraction in the Diabetes Healthcare Domain},
   year = {2023}
}
@inproceedings{BuschDanielandNolte2024,
   abstract = {This paper presents an approach to no-code development based on the interplay of formally defined (graphical) Domain-Specific Languages and informal, intuitive Natural Language which is enriched with contextual information to enable referencing of formally defined entities. The paper focuses on the use and automated integration of these enriched intuitive languages via ChatGPT-based code generation to exploit the best of both language paradigms for domain-specific application development. To compensate for the lack of control over the intuitive languages we apply automated system-level validation via automata learning and subsequent model checking. All this is illustrated using the development of point-and-click adventures as a minimal viable example.},
   author = {Gerrit
and Bainczyk Alexander
and Steffen Bernhard Busch Daniel
and Nolte},
   city = {Cham},
   editor = {Bernhard Steffen},
   isbn = {978-3-031-46002-9},
   booktitle = {Bridging the Gap Between AI and Reality},
   pages = {375-390},
   publisher = {Springer Nature Switzerland},
   title = {ChatGPT in the Loop: A Natural Language Extension for Domain-Specific Modeling Languages},
   year = {2024}
}
@inproceedings{KarchiRashmiPandHatture2023,
   abstract = {A chatbot is computer software that uses artificial intelligence to communicate with people in their native tongues. These chatbots often speak by text or audio, and they are adept at mimicking human speech to interact with people in a way that is human-like. One of the most effective uses of natural language processing is a chatbot. The main purpose of chat bots is to offer customer service. It assists in serving a sizable target audience simultaneously and around-the-clock. May broadcast newsletters, auto-sequences, and plan meetings. It can also gather leads from comments. Create dialog-based forms and save all of the data in spreadsheets. A chatbot can be used to purchase food, propose products, provide customer service, provide weather information, help with personal finances, plan meetings, find and track flights, send money, and many other things.},
   author = {Sanjeevakumar M.
and Tushar T S.
and Prathibha B N Karchi Rashmi P.
and Hatture},
   city = {Cham},
   editor = {Nuno
and Elngar Ahmed A.
and Aneja Nagender
and Sharma Pavika Whig Pawan
and Silva},
   isbn = {978-3-031-47055-4},
   booktitle = {Sustainable Development through Machine Learning, AI and IoT},
   pages = {195-203},
   publisher = {Springer Nature Switzerland},
   title = {AI-Enabled Sustainable Development: An Intelligent Interactive Quotes Chatbot System Utilizing IoT and ML},
   year = {2023}
}
@inproceedings{TsengRaydenandVerberne2023,
   abstract = {ChatGPT, GPT-3.5, and other large language models (LLMs) have drawn significant attention since their release, and the abilities of these models have been investigated for a wide variety of tasks. In this research we investigate to what extent GPT-3.5 can generate human-like comments on Dutch news articles. We define human likeness as `not distinguishable from human comments', approximated by the difficulty of automatic classification between human and GPT comments. We analyze human likeness across multiple prompting techniques. In particular, we utilize zero-shot, few-shot and context prompts, for two generated personas. We found that our fine-tuned BERT models can easily distinguish human-written comments from GPT-3.5 generated comments, with none of the used prompting methods performing noticeably better. We further analyzed that human comments consistently showed higher lexical diversity than GPT-generated comments. This indicates that although generative LLMs can generate fluent text, their capability to create human-like opinionated comments is still limited.},
   author = {Suzan
and van der Putten Peter Tseng Rayden
and Verberne},
   city = {Cham},
   editor = {Tommaso
and Tulin Marina Ceolin Davide
and Caselli},
   isbn = {978-3-031-47896-3},
   booktitle = {Disinformation in Open Online Media},
   pages = {160-174},
   publisher = {Springer Nature Switzerland},
   title = {ChatGPT as a Commenter to the News: Can LLMs Generate Human-Like Opinions?},
   year = {2023}
}
@inproceedings{HuangCheng-WenandColeman2024,
   abstract = {Generative AI is about to radically transform the way intellectual and creative work is being done. Since the release of ChatGPT in late 2022, the potential impact of generative AI tools on higher education has been intensively debated. ChatGPT can generate well-formulated human-like text passages and conversations that is often, but not always, of a surprisingly high quality. This paper reports on an early experiment to explore ways in which ChatGPT can be used in the higher education context. The experiment involved a written assignment which required postgraduate Information Systems students to formulate a critique of the outputs of ChatGPT to a specific question in Information Systems project management. The paper investigates the level of criticality that the students demonstrated in working with ChatGPT and assessing the quality of its outputs. It further explores the claim that ChatGPT can be used to generate rubrics and assess students' assignments by asking ChatGPT to produce a rubric for critical thinking and assess the students' assignments against the rubric produced. The findings indicate that students perceive the ChatGPT produced responses as generally accurate, although they tend to lack depth, with some key information omitted, produced biased responses and have limitations with academic writing conventions. The rubric that ChatGPT produced for assessing critical thinking is lacking in certain areas and the reliability of using it as an assessment tool is questionable given the inconsistency in the results. Overall, the paper concludes that while ChatGPT and other text generative AI can be useful learning and teaching companions for both students and lectures, human expertise and judgement is needed in working with ChatGPT.},
   author = {Max
and Gachago Daniela
and Van Belle Jean-Paul Huang Cheng-Wen
and Coleman},
   city = {Cham},
   editor = {Dirk Petrus
and Drevin Lynette
and Drevin Günther Richard Van Rensburg Henri Emil
and Snyman},
   isbn = {978-3-031-48536-7},
   booktitle = {ICT Education},
   pages = {105-118},
   publisher = {Springer Nature Switzerland},
   title = {Using ChatGPT to Encourage Critical AI Literacy Skills and for Assessment in Higher Education},
   year = {2024}
}
@inproceedings{CunhaPauloRupinoandEstima2023,
   abstract = {Artificial intelligence (AI) has been widely used in many fields, from intelligent virtual assistants to medical diagnosis. However, there is no consensus on how to deal with ethical issues. Using a systematic literature review and an analysis of recent real-world news about AI-infused systems, we cluster existing and emerging AI ethics and responsibility issues into six groups  - broken systems, hallucinations, intellectual property rights violations, privacy and regulation violations, enabling malicious actors and harmful actions, environmental and socioeconomic harms - discuss implications, and conclude that the problem needs to be reflected upon and addressed across five partially overlapping dimensions: Research, Education, Development, Operation, and Business Model. This reflection may be relevant to caution of potential dangers and frame further research at a time when products and services based on AI exhibit explosive growth. Moreover, exploring effective ways to involve users and civil society in discussions on the impact and role of AI systems could help increase trust and understanding of these technologies.},
   author = {Jacinto Cunha Paulo Rupino
and Estima},
   city = {Cham},
   editor = {Zita
and Cascalho José
and Silva Catarina
and Sebastião Raquel Moniz Nuno
and Vale},
   isbn = {978-3-031-49008-8},
   booktitle = {Progress in Artificial Intelligence},
   pages = {92-105},
   publisher = {Springer Nature Switzerland},
   title = {Navigating the Landscape of AI Ethics and Responsibility},
   year = {2023}
}
@inproceedings{AguirreMaiaandMndez2023,
   abstract = {Conversational speech interface development, maintenance and evolution is challenging for non-experts as it requires linguistic knowledge and proficiency in chatbot design and implementation. To address this issue, this work proposes the use of Dialogue Templates, compact conversational interfaces intended to cater specific interaction capabilities which can be easily adapted to a particular use case by non-expert end-users, just with knowledge of the application domain. Our implementation of Dialogue Templates is presented and detailed for three relevant conversational spoken interaction use cases in the industrial environment: navigating maintenance management systems, recording manufacturing plant activity data and registering warehouse inventory. In addition, a comparative analysis is also conducted to assess the effort required to develop sample conversational assistants in such scenarios using our conventional development platform versus Dialogue Templates. Results show that Dialogue Templates significantly simplify the development of conversational speech interfaces, without demanding linguistic expertise.},
   author = {Ariane
and Torralbo Manuel
and del Pozo Arantza Aguirre Maia
and Méndez},
   city = {Cham},
   editor = {Pietro da Silva Hugo Plácido
and Cipresso},
   isbn = {978-3-031-49425-3},
   booktitle = {Computer-Human Interaction Research and Applications},
   pages = {89-109},
   publisher = {Springer Nature Switzerland},
   title = {Simplifying the Development of Conversational Speech Interfaces by Non-Expert End-Users Through Dialogue Templates},
   year = {2023}
}
@inproceedings{Jiang2024-2,
   abstract = {With the rapid development of mobile technology, the coverage of Wlan, 3G and 4G networks is expanding day by day, and intelligent mobile terminal assisted English teaching and learning has become a hot research field. This study explores the application of intelligent mobile terminals in mixed listening and speaking teaching of college English from three aspects. The first aspect analyzes the application of mobile terminals in the collection of listening and speaking teaching resources. The second aspect analyzes the application of mobile terminals in the recommendation of listening and speaking teaching resources. The third aspect analyzes the application of intelligent mobile terminals in listening and speaking teaching scoring: intelligent mobile terminals extract the relevant features of students' input voice, and use SVR to give students' listening and speaking practice scores, which are presented on the mobile terminal learning page. The results show that the average absolute error is less than 1, indicating that the application of intelligent mobile terminals in the recommendation of college English mixed listening and speaking teaching resources is better. The correlation degree is more than 0.5, which indicates that the accuracy of the evaluation results is high, and the resource recommendation time is always below 80ms, proves the application effect of intelligent mobile terminals in college English mixed listening and speaking teaching.},
   author = {Bo Jiang},
   city = {Cham},
   editor = {Jiang
and Han Yu Yun Lin
and Han},
   isbn = {978-3-031-50546-1},
   booktitle = {Advanced Hybrid Information Processing},
   pages = {277-294},
   publisher = {Springer Nature Switzerland},
   title = {The Application and Research of Intelligent Mobile Terminal in Mixed Listening and Speaking Teaching of College English},
   year = {2024}
}
@inproceedings{Lepik2024,
   abstract = {This article focuses on students' reflective assignments (n=26) on using AI chatbots in the short literature review writing process. The opacity of text-generating AI software is impenetrable, yet the flaws in the output provided by AI chatbots sooner or later drive users from trusting to verify the information. The affective component of using AI chatbots is noteworthy as students experiencing time stress or lack of ideas need a `helping hand.' However, with too few experiences in using AI chatbots, they do not anticipate that seemingly fluent text contains hallucinated references and false or machine-like content. In that case, the need to verify the information comes by a surprise, as the time initially reserved for the home assignment is no longer sufficient. Practice and peer-to-peer discussions about AI chatbots in the framework of information literacy related courses can help reduce problems related to using AI chatbots.},
   author = {Krista Lepik},
   city = {Cham},
   editor = {Sonja
and Boustany Joumana
and Ünal Yurdagül
and Şencan \.\{I\}pek
and Kos Denis
and Grassian Esther
and Mizrachi Diane
and Roy Loriene Kurbanoğlu Serap
and Špiranec},
   isbn = {978-3-031-53001-2},
   booktitle = {Information Experience and Information Literacy},
   pages = {27-38},
   publisher = {Springer Nature Switzerland},
   title = {Trust, but Verify: Students' Reflections on Using Artificial Intelligence in Written Assignments},
   year = {2024}
}
@inproceedings{SunXiaohuiandSu2024,
   abstract = {In order to improve the efficiency and quality of automatic generation of English teaching information and achieve the ideal effect of generating accurate teaching information in a fast time, automatic extraction of Web information is introduced, and the research on automatic generation of English teaching information based on automatic extraction of Web information is carried out. First, we use data amplification method to preprocess English teaching information and construct pseudo parallel sentences. Secondly, the context of teaching information is coded, and the distance calculation between any two positions of English teaching information is reduced to a constant through the attention mechanism to solve the problem of long-distance dependence of information. Then, a corpus of English teaching information is built to store and manage the relevant contents, such as morphological reduction, parts-of-speech tagging, syntactic structure tagging, etc. Finally, the semantic relation of English teaching information is automatically extracted from Web information, and the teaching information is automatically generated. The experimental results show that with the increase of test data in English corpus, the F-value reaches more than 96\%, and the accuracy rate of information generation and recall rate are higher.},
   author = {Jing Sun Xiaohui
and Su},
   city = {Cham},
   editor = {Ying
and Lin Yun Gui Guan
and Li},
   isbn = {978-3-031-51503-3},
   booktitle = {e-Learning, e-Education, and Online Training},
   pages = {371-386},
   publisher = {Springer Nature Switzerland},
   title = {Research on Automatic Generation of English Teaching Information Based on Automatic Extraction of Web Information},
   year = {2024}
}
@inbook{Sarferaz2024,
   abstract = {In this chapter, we specify the business requirements and propose the solution concept for embedding Generative AI into ERP software. Generative AI has the potential to radically change the way we apply artificial intelligence in various industries and fields. By levering the strength of these sophisticated models, users without technical backgrounds can address their business challenges just by expressing them in everyday language. This creates a vast range of opportunities for both companies and individuals. In the context of ERP systems, the key questions are: What is the added value of Generative AI, what are the technical requirements, and how to embed this technology into business applications? Providing answers for those questions is the objective of this chapter.},
   author = {Siar Sarferaz},
   city = {Cham},
   doi = {10.1007/978-3-031-54249-7_19},
   isbn = {978-3-031-54249-7},
   booktitle = {Embedding Artificial Intelligence into ERP Software : A Conceptual View on Business AI with Examples from SAP S/4HANA},
   pages = {277-288},
   publisher = {Springer Nature Switzerland},
   title = {Embedding Generative AI},
   url = {https://doi.org/10.1007/978-3-031-54249-7_19},
   year = {2024}
}
@inbook{WhiteJulesandHays2024,
   abstract = {This chapter presents design techniques for software engineering, in the form of prompt patterns, to solve common problems that arise when using large language models (LLMs) to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and creating API specifications from lists of requirements. This chapter provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classifies patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, deployment, and testing.},
   author = {Sam
and Fu Quchen
and Spencer-Smith Jesse
and Schmidt Douglas C White Jules
and Hays},
   city = {Cham},
   doi = {10.1007/978-3-031-55642-5_4},
   editor = {Pekka
and Khomh Foutse Nguyen-Duc Anh
and Abrahamsson},
   isbn = {978-3-031-55642-5},
   booktitle = {Generative AI for Effective Software Development},
   pages = {71-108},
   publisher = {Springer Nature Switzerland},
   title = {ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design},
   url = {https://doi.org/10.1007/978-3-031-55642-5_4},
   year = {2024}
}
@inbook{Cabrero-DanielBeatrizandFazelidehkordi2024,
   abstract = {Software development teams often deviate from their adopted framework, such as Scrum, and these deviations can sometimes bring consequences with different impact levels if the adaptations are not tailored for the specific teams' needs and circumstances. For instance, agile developers sometimes oversimplify crucial Agile steps, such as estimating needed effort for a specific task or lack of explicit assessment of the criteria for ``Definition of Done.'' This information, though, is useful for subsequent planning activities. We hypothesise that generative AI could be used to help Agile teams conduct a number of software management tasks in a systematic and effective way. A family of experiments to compare the performance of humans and generative AI tools, namely, GPT-models and Bard, will be conducted. The findings from these experiments will serve as a foundation for a discussion on the role of artificial intelligence in software engineering tasks. This discussion will primarily focus on the balance between performance (perfect?) and efficiency (done?) and the importance of human oversight in Agile environments.},
   author = {Yasamin
and Nouri Ali Cabrero-Daniel Beatriz
and Fazelidehkordi},
   city = {Cham},
   doi = {10.1007/978-3-031-55642-5_11},
   editor = {Pekka
and Khomh Foutse Nguyen-Duc Anh
and Abrahamsson},
   isbn = {978-3-031-55642-5},
   booktitle = {Generative AI for Effective Software Development},
   pages = {235-255},
   publisher = {Springer Nature Switzerland},
   title = {How Can Generative AI Enhance Software Management? Is It Better Done than Perfect?},
   url = {https://doi.org/10.1007/978-3-031-55642-5_11},
   year = {2024}
}
@inproceedings{BagumaRehemaandNamuwaya2024,
   abstract = {Large language models (LLMs) have the potential to generate significant benefits, but their blanket application in Africa could exacerbate existing social and economic inequalities. This is due to a number of factors, including limited technological advancement, historical injustice and marginalization, and underrepresentation of African languages, values, and norms in training data. Despite comprising nearly one-third of the world's languages, most African languages are underrepresented on the internet: they are primarily oral with little available in written and digitized form. Additionally, most African languages have conflicting orthographic standards. While Africa is undergoing a digital transformation, both internet connectivity and digital literacy remain relatively low and unevenly distributed. This lack of online representation for African languages limits the availability of natural language data for training inclusive language models. This paper examines the potential harms of LLMs in Africa, covering harms already documented for the African context; harms studied and documented for the Western context, but previously unapplied to Africa; and novel potential harms based on the norms, values, practices, and contextual factors of the African continent. This work aims to contribute to a better understanding of potential harms of LLMs in Africa, which in turn could inform and support the development of more inclusive LLMs.},
   author = {Hajarah
and Nakatumba-Nabende Joyce
and Rashid Qazi Mamunur Baguma Rehema
and Namuwaya},
   city = {Cham},
   editor = {Marcellin
and Rajagopalan Rajeswari Pillai Tchakounte Franklin
and Atemkeng},
   isbn = {978-3-031-56396-6},
   booktitle = {Safe, Secure, Ethical, Responsible Technologies and Emerging Applications},
   pages = {3-19},
   publisher = {Springer Nature Switzerland},
   title = {Examining Potential Harms of Large Language Models (LLMs) in Africa},
   year = {2024}
}
@inproceedings{TolulopeEzekielOlayideandTchakounte2024,
   abstract = {Effective interaction between medical practitioners and patients is critical in ensuring positive health outcomes. The primary goal of this systematic review is to identify different strategies for improving medical practitionerpatient interaction and to analyze their effectiveness in enhancing patient satisfaction, adherence to treatment, and health outcomes. A systematic review of the literature was conducted using electronic databases including IEEE, ACM Digital Library, Springer, Science Direct and Wiley Online Library, using keywords such as ``doctorpatient communication,'' ``physician-patient interactions,'' and ``Patient-doctor interaction+Technology.'' ``Patientdoctor interaction+Technology+Issues.''Articles were included if they were published in English and contained strategies for improving medical practitioner-patient interaction. A total of 34 articles were included in this systematic review. The strategies identified were categorized into four themes: training programs, communication skills, patient-centered care, and technology-based interventions. Technology-based interventions, such as virtual consultations and electronic health records, were shown to enhance communication and information sharing between medical practitioners and patients. Improving medical practitioner-patient interaction is crucial in achieving positive health outcomes. The strategies identified in this review can be used to design interventions that improve communication skills, promote patient-centered care, and incorporate technology-based solutions to enhance communication and information sharing in clinical settings.},
   author = {Franklin Tolulope Ezekiel Olayide
and Tchakounte},
   city = {Cham},
   editor = {Marcellin
and Rajagopalan Rajeswari Pillai Tchakounte Franklin
and Atemkeng},
   isbn = {978-3-031-56396-6},
   booktitle = {Safe, Secure, Ethical, Responsible Technologies and Emerging Applications},
   pages = {380-408},
   publisher = {Springer Nature Switzerland},
   title = {Support to Interaction Between Medical Practitioners and Patients: A Systematic Review},
   year = {2024}
}
@inproceedings{LauleLucaandBick2024,
   abstract = {Transformation competencies are essential for individuals tasked with executing organizational transformations, whom we refer to in this study as transformation managers. The transformation manager plays a crucial role as a mediator to successful organizational transformation. Consequently, it is critical to understand what competencies the transformation manager must possess. In this study, we conducted a systematic literature review of transformation competencies by screening an initial sample of 1,932 articles, resulting in a final sample of 11. We examined the characteristics of the final sample and subsequently interpreted the results. Our findings emphasize the novelty of the topic, given the limited extent of prior research in this domain. We further provide a list of competencies necessary for executing transformations where communication and competency in teamwork and collaboration were identified as the most important competencies for the transformation manager. Finally, we conclude with a comprehensive research agenda and provide future research directions.},
   author = {Markus Laule Luca
and Bick},
   city = {Cham},
   editor = {Marinos
and Al Marri Khalid
and Al Zarouni Marwan Papadaki Maria
and Themistocleous},
   isbn = {978-3-031-56481-9},
   booktitle = {Information Systems},
   pages = {148-163},
   publisher = {Springer Nature Switzerland},
   title = {Communication is Key: A Systematic Literature Review of Transformation Competencies},
   year = {2024}
}
@inproceedings{MehrajAliandZhang2024,
   abstract = {Context and Motivation: Rapid advancements in Artificial Intelligence (AI) have significantly influenced requirements engineering (RE) practices. Problem: While many recent secondary studies have explored AI's role in RE, a thorough understanding of the use of AI for RE (AI4RE) and its inherent challenges remains in its early stages.Principal Ideas: To fill this knowledge gap, we conducted a tertiary review on understanding how AI assists RE practices. Contribution: We analyzed 28 secondary studies from 2017 to September 2023 about using AI in RE tasks such as elicitation, classification, analysis, specification, management, and tracing. Our study reveals a trend of combining natural language process techniques with machine learning models like Latent Dirichlet Allocation (LDA) and Naive Bayes, and a surge in using large language models (LLMs) for RE. The study also identified challenges of AI4RE related to ambiguity, language, data, algorithm, and evaluation. The study gives topics for future research, particularly for researchers who want to start new research in this field.},
   author = {Zheying
and Systä Kari Mehraj Ali
and Zhang},
   city = {Cham},
   editor = {Ana Mendez Daniel
and Moreira},
   isbn = {978-3-031-57327-9},
   booktitle = {Requirements Engineering: Foundation for Software Quality},
   pages = {159-177},
   publisher = {Springer Nature Switzerland},
   title = {A Tertiary Study on AI for Requirements Engineering},
   year = {2024}
}
@inproceedings{KedzierskaMariaandSpytek2024,
   abstract = {Text data is widely used for both commercial and research purposes. While extensive sources of text data are available within Internet forums, such as Reddit, their volume is vast and, typically, only a small subset of posts is studied. To overcame problem of data size, topic modeling can be applied, to extract the main ideas from the documents. However, as it will be shown, different modeling techniques may produce very different results. Specifically, in this contribution, an overview of the most popular topic models, used in natural language processing, and methods for their comparison, is provided. Moreover, a software solution for downloading, modeling, exploring, and comparing topics, contained in Reddit posts, is introduced. The proposed application is experimentally validated, by showing that the extracted topics reflect real-world events. Finally, obtained results are compared to these originating from a different tool, used for investigating topic popularity.},
   author = {Mikołaj
and Kurek Marcelina
and Sawicki Jan
and Ganzha Maria
and Paprzycki Marcin Kedzierska Maria
and Spytek},
   city = {Cham},
   editor = {Yutaka Sachdeva Shelly
and Watanobe},
   isbn = {978-3-031-58502-9},
   booktitle = {Big Data Analytics in Astronomy, Science, and Engineering},
   pages = {17-44},
   publisher = {Springer Nature Switzerland},
   title = {Topic Modeling Applied to Reddit Posts},
   year = {2024}
}
@inproceedings{Rozo-TorresAlexanderandSarmiento2024,
   abstract = {This position paper presents some relevant considerations regarding the challenges involved in the integration of ``prompt engineering'' tools into the video game development industry. The core of this work is a general framework for developing video games using ``prompt engineering'' tools, which includes incorporating them into a traditional workflow comprising pre-production, production, and post-production stages. Additionally, this paper provides an example of developing a video game with minimal intervention of prompt outputs. It showcases a recommender tool, prompt inputs, their corresponding outputs, and how they are integrated into the final product at each stage. Thus, the trends and challenges presented in this paper are based on the insights gained from this experience.},
   author = {Wilson J Rozo-Torres Alexander
and Sarmiento},
   city = {Cham},
   editor = {Vanessa
and Mon Alicia Ruiz Pablo H.
and Agredo-Delgado},
   isbn = {978-3-031-57982-0},
   booktitle = {Human-Computer Interaction},
   pages = {242-256},
   publisher = {Springer Nature Switzerland},
   title = {Prompt Engineering, An Alternative for Video Game Development?},
   year = {2024}
}
@inproceedings{CascianiAngeloandBernardi2024,
   abstract = {AI-augmented Business Process Management Systems (ABPMSs) are an emerging class of process-aware information systems empowered by AI technology for autonomously unfolding and adapting the execution flow of business processes (BPs). A central characteristic of an ABPMS is the ability to be conversationally actionable, i.e., to proactively interact with human users about BP-related actions, goals, and intentions. While today's trend is to support BP automation using reactive conversational agents, an ABPMS is required to create dynamic conversations that not only respond to user queries but even initiate conversations with users to inform them of the BP progression and make recommendations to improve BP performance. In this paper, we explore the extent to which state-of-the-art conversational systems (CSs) can be used to develop such proactive conversation features, and we discuss the research challenges and opportunities within this area.},
   author = {Mario L.
and Cimitile Marta
and Marrella Andrea Casciani Angelo
and Bernardi},
   city = {Cham},
   editor = {Jose Luis
and Santos Maribel Yasmina
and Assar Saïd Araújo João
and de la Vara},
   isbn = {978-3-031-59465-6},
   booktitle = {Research Challenges in Information Science},
   pages = {183-200},
   publisher = {Springer Nature Switzerland},
   title = {Conversational Systems for AI-Augmented Business Process Management},
   year = {2024}
}
@inproceedings{BaumSandraandManikas2024,
   abstract = {Artificial intelligence (AI) has the potential to revolutionize healthcare in the EU by addressing challenges, such as shortages of healthcare personnel and more effective diagnosis and care. However, the safety concerns surrounding AI-based medical devices have been a major roadblock to the technology's wider adoption. This study aims to further investigate these concerns in the European context by analysing the AI-enabled Medical devices currently available in the European Union market along with their potential safety risks. We do this by applying a combination of three research methods: (1) a survey of the safety risks of AI-enabled Medical Devices published between 2012 and 2023, (2) an analysis of AI-based medical devices in the EUDAMED database, and (3) a survey on the perceptions of the EU Medical AI ecosystem stakeholders. Our study analyzed the state-of-the-art with a literature body of 29 papers and summarized a number of risks related to the use of AI in medical devices along with the reported mitigation strategies. Furthermore, we analyzed the approved medical devices (71 devices) that use AI in the EUDAMED database and found that there is a lack of transparency in whether the devices use AI along with the lack of crucial information necessary to asses the devices' safety risks, such information on training data. Finally, when we survey a number of medical device stakeholders (7 out of 130 respondents) we find that there is a disconnect between the industry and regulators: the medical device representatives emphasize the need for better guidance on post-market surveillance while the regulation representatives feel that they lack expertise in AI.},
   author = {Konstantinos Baum Sandra
and Manikas},
   city = {Cham},
   editor = {Pieter
and Shah Syed Ahmar Salvi Dario
and Van Gorp},
   isbn = {978-3-031-59717-6},
   booktitle = {Pervasive Computing Technologies for Healthcare},
   pages = {51-69},
   publisher = {Springer Nature Switzerland},
   title = {Investigating AI in Medical Devices: The Need for Better Establishment of Risk-Assessment and Regulatory Foundations},
   year = {2024}
}
@inbook{VanLooy2024,
   abstract = {This chapter covers one of the most omnipresent yet varied digital technologies, namely, artificial intelligence (AI). Although AI already helps describe and predict all types of situations, the AI future is quite unpredictable because this digital technology evolves extremely fast. Because AI has captured the interest and imagination of many people and organizations, this chapter explains in an accessible way the complex notion of AI and its technical background, without compromising the AI essentials. Although many AI applications exist (such as robotics, speech recognition, and recently generative AI with large language models, just to name a few), this chapter continues by zooming into a solution for agriculture as a success story. Overall, we invite the reader to learn more about the many possibilities but also pitfalls of AI. Because AI facilitates many other digital technologies, this chapter serves as a basis for the coming chapters as well.},
   author = {Amy Van Looy},
   city = {Cham},
   doi = {10.1007/978-3-031-59770-1_3},
   isbn = {978-3-031-59770-1},
   booktitle = {From Emerging Technologies to Business Opportunities: Interviews with Academics and Business Experts},
   pages = {33-58},
   publisher = {Springer Nature Switzerland},
   title = {Artificial Intelligence},
   url = {https://doi.org/10.1007/978-3-031-59770-1_3},
   year = {2024}
}
@inproceedings{CaoCuicuiandZhao2024,
   abstract = {The proliferation of generative artificial intelligence (GAI) spawns a new type of freelance digital platform: prompt marketplace, on which prompt engineers upload their prompts and sell them to potential buyers. As only freelancers themselves know about the prompt in detail and they can only provide limited textual information, great information asymmetry exists that may hinder exchange. Besides, one distinct feature of prompt marketplace is that prompt engineers can display part of their prompt to potential buyers. Thus, drawing on the signaling theory, this study investigates the joint effects of linguistic and demonstration signals on prompt sales. Data from 16,890 prompts were collected and analyzed. Results show that linguistic signals and their interaction with demonstration signals significantly influence prompt sales. Interestingly, we found the negative direct influence of demonstration signals on prompt sales. Our study has theoretical contributions to freelance digital platform literature and extends signaling theory to prompt marketplace. This study can also provide practical suggestions to platforms and freelancers alike.},
   author = {Ling
and Li Yuni
and Xie Chongyang Cao Cuicui
and Zhao},
   city = {Cham},
   editor = {Maomao Tu Yiliu Paul
and Chi},
   isbn = {978-3-031-60260-3},
   booktitle = {E-Business. New Challenges and Opportunities for Digital-Enabled Intelligent Future},
   pages = {264-275},
   publisher = {Springer Nature Switzerland},
   title = {Selling in Prompt Marketplace: An Empirical Study on the Joint Effects of Linguistic and Demonstration Signals on Prompt Sales},
   year = {2024}
}
@inbook{Hoda2024,
   abstract = {In this chapter, we will learn about two groups of data collection techniques: custom-data and existing-data collection. Then we will delve into the details of popular collection techniques such as pre-interview questionnaires, semi-structured interviews, and observations. We will look into other sources and collection techniques such as focus groups, surveys, recordings, texts, social media, artefacts, data mining, and immersive experiences in extended realities. Researchers will benefit from reading this chapter in conjunction with the Basics of Qualitative Data Collection, Qualitative Data Preparation and Filtering, and Socio-technical Grounded Theory for Qualitative Data Analysis chapters. Collectively, they cover socio-technical grounded theory's Basic Stage.},
   author = {Rashina Hoda},
   city = {Cham},
   doi = {10.1007/978-3-031-60533-8_8},
   isbn = {978-3-031-60533-8},
   booktitle = {Qualitative Research with Socio-Technical Grounded Theory: A Practical Guide to Qualitative Data Analysis and Theory Development in the Digital World},
   pages = {167-200},
   publisher = {Springer International Publishing},
   title = {Techniques of Qualitative Data Collection},
   url = {https://doi.org/10.1007/978-3-031-60533-8_8},
   year = {2024}
}
@inproceedings{ReichertTimandMiftari2024,
   abstract = {This paper presents a novel approach to empower female founders by integrating a Large Language Model (LLM) into a serious game featuring player-generated content. The game leverages interactive visual novels to enhance resilience and awareness of gender-based discrimination in startup environments. The implementation of generative AI for bias detection in the conversations encountered by founders enables the provision of insightful feedback on discriminatory elements within the visual novels in which players immerse themselves. The game allows players to create personalized visual novels, reflecting their own entrepreneurial experiences. The system selects backgrounds, characters, and dialogue options, transforming players into creators while identifying and providing feedback on discriminatory elements within stories. The game's architecture, designed for modularity, supports the integration of various LLMs, enhancing its adaptability and future-proofing. Preliminary results demonstrate the game's feasibility, with expert evaluations affirming the effectiveness of its bias and discrimination detection mechanisms. This research contributes to the fields of gender equality and entrepreneurship in the digital domain, highlighting the potential of serious games and AI in addressing social challenges.},
   author = {Mergim
and Herling Claudia
and Marsden Nicola Reichert Tim
and Miftari},
   city = {Cham},
   editor = {Xiaowen Fang},
   isbn = {978-3-031-60695-3},
   booktitle = {HCI in Games},
   pages = {69-83},
   publisher = {Springer Nature Switzerland},
   title = {Empowering Female Founders with AI and Play: Integration of a Large Language Model into a Serious Game with Player-Generated Content},
   year = {2024}
}
@inproceedings{RittelmeyerJackDanielandSandkuhl2024,
   abstract = {Artificial Intelligence (AI) is a key technology driving digital transformation in enterprises worldwide. However, the implementation of AI projects often faces hurdles, primarily due to misconceptions about AI's capabilities and suitable applications. This research represents a step in a broader research process aimed at creating an artifact to help companies, particularly those outside the IT sector, navigate these challenges. We present an evaluation of a morphological box that outlines critical factors for successful AI integration. The evaluation was carried out by conducting and analyzing a survey in which participants rated the individual features and values, as well as their structure. We also conducted the Kaiser-Meyer-Olkin and Bartlett tests and created a correlation matrix as statistical measures to identify overlap between the elements of the box. After analyzing the data, we found some further room for improvement but that the box seems to include the generally most important aspects. Overall, the participants' ratings for the individual elements were also satisfactory.},
   author = {Kurt Rittelmeyer Jack Daniel
and Sandkuhl},
   city = {Cham},
   editor = {Claudio
and Kalloniatis Christos Almeida João Paulo A.
and Di Ciccio},
   isbn = {978-3-031-61003-5},
   booktitle = {Advanced Information Systems Engineering Workshops},
   pages = {115-129},
   publisher = {Springer Nature Switzerland},
   title = {A Survey to Evaluate the Completeness and Correctness of a Morphological Box for AI Solutions},
   year = {2024}
}
@inproceedings{Estrada-TorresBediliaanddel-Ro-Ortega2024,
   abstract = {The irruption of large language models (LLMs) during the last year has prompted researchers and practitioners to explore novel scenarios for integrating LLMs, enhancing task execution efficiency across diverse domains. Among these, business process management (BPM) stands out as a fertile ground for leveraging LLM features. As organizations strive to streamline their processes throughout the BPM lifecycle, the potential benefits of incorporating LLMs become increasingly evident. In this sense, over the past year, several approaches have been proposed to incorporate LLMs in BPM-related tasks. Concurrently, research efforts have identified key research directions that can help guide the adoption of LLMs in the BPM lifecycle phases. In this article, we perform a comprehensive literature review to assess the impact and coverage of existing approaches in addressing these research directions. In addition, we deem it particularly relevant to analyze the evaluation criteria followed. By analyzing existing proposals and techniques, we aim to shed light on the most addressed BPM lifecycle phases, pinpointing the research directions they entail and the evaluation criteria utilized. Through this analysis, we provide valuable insights and recommendations to inform future research endeavors.},
   author = {Adela
and Resinas Manuel Estrada-Torres Bedilia
and del-Río-Ortega},
   city = {Cham},
   editor = {Dominik
and Schmidt Rainer
and Sturm Arnon van der Aa Han
and Bork},
   isbn = {978-3-031-61007-3},
   booktitle = {Enterprise, Business-Process and Information Systems Modeling},
   pages = {22-31},
   publisher = {Springer Nature Switzerland},
   title = {Mapping the Landscape: Exploring Large Language Model Applications in Business Process Management},
   year = {2024}
}
@inproceedings{Ching2024,
   abstract = {In this study, five barriers are identified for comprehension of Classical Chinese poems in modern times, namely linguistics, literacy, culture, time and geography. Chinese languages are considered hard to learn for foreign communities due to their completely different scripting systems, sound systems and culture. Classical Chinese, as an ancient language with the grammar aiming for extreme conciseness, is even harder to comprehend. Currently, it is no longer as actively in use as in the old days by the Chinese communities although a huge number of important original texts have been passed on. Poems alone are also generally regarded as difficult to consume due to their abstractness. Meanwhile, reading poems in virtual reality (VR) was underexplored although there is a potential in improving comprehension through embodied reading experience [1]. VR experience for a Tang poem was created to formulate a methodology that can reduce the quintuple complexities in comprehending and resonating with Classical Chinese poems through reading in VR. As it is research through design and design decisions vary with designers, considerations in the VR design process are elucidated in this paper with a semi-autoethnographical approach. After pilot test of the design, a VR design methodology is derived for enabling reader to grasp the poem's essence, thus enhancing their comprehension of the text and the language. The methodology is deducibly applicable for any forms of poems in any languages. Fostered understanding of Classical Chinese poems may in return revitalize the ancient language.},
   author = {Carrie Ching},
   city = {Cham},
   editor = {Gino Chen Jessie Y. C.
and Fragomeni},
   isbn = {978-3-031-61047-9},
   booktitle = {Virtual, Augmented and Mixed Reality},
   pages = {19-43},
   publisher = {Springer Nature Switzerland},
   title = {Breaking Barriers for Classical Chinese: Tang Poetry in Virtual Reality},
   year = {2024}
}
@inproceedings{JiangNinaandDuffy2024,
   abstract = {With the rapid advancement of artificial intelligence, it has been widely applied in various domains to assist training, including education, medical, automation, and industrial fields. The purpose of this study is to deep dive into the use of artificial intelligence for training by conducting a systematic literature review. Bibliometric data were collected from multiple databases, including Web of Science, Scopus, and Google Scholar through Publish or Perish. The metadata analysis was performed by using VOSviewer, CiteSpace, etc. The results of the analysis indicated the importance of the research topic and the interdisciplinary nature of artificial intelligence-assisted training by clustering the prevalent keywords, such as human factor, deep learning, explainable artificial intelligence, generative artificial intelligence, education, and medical. The research underscores the potential of artificial intelligence in transforming traditional training paradigms. Also emphasizes the limitations of state-of-the-art applications and proposes the need for further exploration in various domains, including the comprehensive evaluation of the system, and personalized artificial intelligence training systems in specific domains.},
   author = {Vincent G Jiang Nina
and Duffy},
   city = {Cham},
   editor = {Vincent G Duffy},
   isbn = {978-3-031-61066-0},
   booktitle = {Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management},
   pages = {346-363},
   publisher = {Springer Nature Switzerland},
   title = {Use of Artificial Intelligence for Training: A Systematic Review},
   year = {2024}
}
@inproceedings{StratigakisIakovosandKatsanos2024,
   abstract = {Existing academic social networks for researchers do not currently address the multitude of their tasks and diverse needs. This paper presents the EPICommunity platform, a social network for researchers designed by researchers. The EPICommunity platform aims to support researchers, particularly early career researchers, to connect with peers, showcase their work and collaborate and create groups. To this end, it provides unique features compared to existing systems, such as Europass profile interoperability, recommendations for researchers with similar interests based on various criteria, gamification mechanics, analytics to monitor researchers' own progress and a multi-dimensional set of researchers' assessment criteria, both quantitative and qualitative. The EPICommunity platform follows a human-centered development model. This paper presents its first iteration. Real-world user requirements were produced from three workshops with 17 researchers from 8 European Universities. Prototypes were also developed and evaluated in these workshops. Finally, a formative expert-based usability evaluation study was conducted. To this end, the heuristic evaluation method was employed involving five experienced usability experts.},
   author = {Christos
and Tso Anouk
and Kovaios Dimitrios
and Tsiatsos Thrasyvoulos Stratigakis Iakovos
and Katsanos},
   city = {Cham},
   editor = {Simona Coman Adela
and Vasilache},
   isbn = {978-3-031-61305-0},
   booktitle = {Social Computing and Social Media},
   pages = {201-218},
   publisher = {Springer Nature Switzerland},
   title = {EPICommunity Platform: Towards an Academic Social Network Designed for Researchers by Researchers},
   year = {2024}
}
@inproceedings{NazirCassiniandCourtney2024,
   abstract = {This paper shows how a tool that explore future possibilities, ReadySetFuture_, helped a major automotive maker understand how shifts in consumer values may impact the features and use cases that surprise and delight future vehicle consumers in the year 2033. It contrasts two common mindsets when thinking about the future: a there is no alternative (TINA) mindset and a there are many alternatives (TAMA) mindset. The future contains numerous alternatives and possibilities, ripe for exploration and proactive planning. ReadySetFuture_ was included in a series of workshops to cultivate a heightened sense of curiosity about future possibilities. The paper examines the planning and development stages of ReadySetFuture_ and highlights key findings that demonstrates how the tool enhances users' curiosity. The paper details the design, development, and testing processes, emphasizing its approaches to evoke curiosity. Additionally, it discusses the strategic integration of various curiosity-invoking techniques, which contributes to a more extensive approach to futures thinking. The game's success in generating over 100 scenarios across workshops highlights its effectiveness in encouraging participants to explore diverse perspectives on potential futures.},
   author = {Mike
and Lee Kuo Wei Nazir Cassini
and Courtney},
   city = {Cham},
   editor = {Elizabeth
and Soares Marcelo M Marcus Aaron
and Rosenzweig},
   isbn = {978-3-031-61353-1},
   booktitle = {Design, User Experience, and Usability},
   pages = {336-352},
   publisher = {Springer Nature Switzerland},
   title = {Becoming More Curious About the Future: ReadySetFuture_},
   year = {2024}
}
@inproceedings{Sood2024,
   abstract = {The relative value of automation is debated. On one hand, it is lauded for making mundane work doable by machines. Not only can such machines carry out such work more efficiently than humans: the latter are then empowered and opened up to fulfill endeavors [57] (e.g., creative). Furthermore, automation can itself inspire innovations and the creation of work, such as in the creation of driverless vehicles (including software and hardware, as well as new regulatory practice). In an ideal scenario where work is fully automated, humans would be able to maximize leisure and not be bound by economic demands. Automation fits into a vision for societal AugCog where humans put machines to work so that the former can invest in more rewarding cognitive activities. Humans can do so by deepening their respective cultures with furthered devotion to art, philosophy, science, engineering, mathematics, humanities, politics, spirituality, and religion. In day-to-day life, automation can augment cognition via reduced stress and increased life satisfaction as people's lives become more centered on relationships and meaning. Automation of work should proceed in a manner sensitive to overall economic and human impacts.},
   author = {Suraj Sood},
   city = {Cham},
   editor = {Cali M Schmorrow Dylan D.
and Fidopiastis},
   isbn = {978-3-031-61572-6},
   booktitle = {Augmented Cognition},
   pages = {213-235},
   publisher = {Springer Nature Switzerland},
   title = {The AugCog of Work},
   year = {2024}
}
@inproceedings{SioziouKyriakiandZervas2024,
   abstract = {The recent progress in Large Language Models has opened up new possibilities for their application in different domains. This study focuses on exploring the potential of LLMs in structured information extraction, specifically in the context of job postings. We compare commercial and open-source LLMs to see how well they can extract key information from job postings in Greece's tourism sector. Our goal is to understand the performance differences between these models and assess their general applicability in real-world information extraction tasks. We aim to evaluate and compare the capability of these models in accurately identifying and extracting specific data points such as Job Title, Company, Industry, Location, Soft Skills, and Hard Skills. This research contributes to our understanding of how practical LLMs are in real-world information extraction tasks and highlights the differences in performance among various state-of-the-art models.},
   author = {Panagiotis
and Giotopoulos Kostas
and Tzimas Giannis Sioziou Kyriaki
and Zervas},
   city = {Cham},
   editor = {Ilias
and Papaleonidas Antonios
and Pimenidis Elias
and Jayne Chrisina Iliadis Lazaros
and Maglogiannis},
   isbn = {978-3-031-62495-7},
   booktitle = {Engineering Applications of Neural Networks},
   pages = {82-92},
   publisher = {Springer Nature Switzerland},
   title = {Comparative Analysis of Large Language Models in Structured Information Extraction from Job Postings},
   year = {2024}
}
@inproceedings{Al-DhamariNabilandClarke2024,
   abstract = {This study explores the limitations of traditional Cybersecurity Awareness and Training (CSAT) programs and proposes an innovative solution using Generative Pre-Trained Transformers (GPT) to address these shortcomings. Traditional approaches lack personalization and adaptability to individual learning styles. To overcome these challenges, the study integrates GPT models to deliver highly tailored and dynamic cybersecurity learning experiences. Leveraging natural language processing capabilities, the proposed approach personalizes training modules based on individual trainee profiles, helping to ensure engagement and effectiveness. An experiment using a GPT model to provide a real-time and adaptive CSAT experience through generating customized training content. The findings have demonstrated a significant improvement over traditional programs, addressing issues of engagement, dynamicity, and relevance. GPT-powered CSAT programs offer a scalable and effective solution to enhance cybersecurity awareness, providing personalized training content that better prepares individuals to mitigate cybersecurity risks in their specific roles within the organization.},
   author = {Nathan Al-Dhamari Nabil
and Clarke},
   city = {Cham},
   editor = {Wai Sze
and von Solms Suné Drevin Lynette
and Leung},
   isbn = {978-3-031-62918-1},
   booktitle = {Information Security Education - Challenges in the Digital Age},
   pages = {3-20},
   publisher = {Springer Nature Switzerland},
   title = {GPT-Enabled Cybersecurity Training: A Tailored Approach for Effective Awareness},
   year = {2024}
}
@inproceedings{GiarelisNikolaosandMastrokostas2024,
   abstract = {Text summarization is a natural language processing subtask pertaining to the automatic formulation of a concise and coherent summary that covers the major concepts and topics from one or multiple documents. Recent advancements in deep learning have led to the development of abstractive summarization Transformer-based models, which outperform classical approaches. In any case, research in this field focuses on high resource languages such as English, while the corresponding work for low resource languages is limited. Dealing with modern Greek, this paper proposes a series of new abstractive models for news article summarization. The proposed models were thoroughly evaluated on the same dataset against GreekBART, the only existing model for Greek abstractive news summarization. Our evaluation results reveal that most of the proposed models perform better than GreekBART on various evaluation metrics. Our experiments indicate that multilingual Seq2Seq models, fine-tuned for a specific language and task, can achieve similar or even better performance compared to monolingual models pre-trained and fine-tuned for the same language and task, while requiring significantly less computational resources. We make our evaluation code public, aiming to increase the reproducibility of this work and facilitate future research in the field.},
   author = {Charalampos
and Karacapilidis Nikos Giarelis Nikolaos
and Mastrokostas},
   city = {Cham},
   editor = {Lazaros
and Macintyre John
and Avlonitis Markos
and Papaleonidas Antonios Maglogiannis Ilias
and Iliadis},
   isbn = {978-3-031-63215-0},
   booktitle = {Artificial Intelligence Applications and Innovations},
   pages = {60-73},
   publisher = {Springer Nature Switzerland},
   title = {GreekT5: Sequence-to-Sequence Models for Greek News Summarization},
   year = {2024}
}
@inproceedings{Avigad2024,
   abstract = {Throughout the history of automated reasoning, mathematics has been viewed as a prototypical domain of application. It is therefore surprising that the technology has had almost no impact on mathematics to date and plays almost no role in the subject today. This article presents an optimistic view that the situation is about to change. It describes some recent developments in the Lean programming language and proof assistant that support this optimism, and it reflects on the role that automated reasoning can and should play in mathematics in the years to come.},
   author = {Jeremy Avigad},
   city = {Cham},
   editor = {Marijn J.H.
and Schmidt Renate A Benzmüller Christoph
and Heule},
   isbn = {978-3-031-63498-7},
   booktitle = {Automated Reasoning},
   pages = {3-20},
   publisher = {Springer Nature Switzerland},
   title = {Automated Reasoning for Mathematics},
   year = {2024}
}
@inproceedings{AlphenaarGrantandRafiq2024,
   abstract = {Massively open online courses (MOOCs) and platforms such as Udemy have proliferated in recent years. These courses run the gamut from highly successful and high-rated to courses with very low ratings and little engagement. This research aims to address the challenge of preemptively identifying potentially low-rated courses by leveraging instructor-provided textual information. Our approach involves a two-stage process. First, we employ transformer-based Large Language Models (LLMs) to extract semantic information from the text provided by the instructors on the Udemy platform. In the second stage, we incorporate the extracted information as additional features into an upstream predictive model. To the best of our knowledge, this is the first attempt to use extracted semantic information from MOOC courses as features in a predictive model. In general, we find that existing consumer research findings hold and identify three key takeaways. First, we find that an instructor's prior performance is a strong indicator of future ratings. Second, we see that including semantic information contained in instructor-provided text can have an additive effect on model performance. Finally, we demonstrate that fine-tuning language models on Udemy-specific text have an appreciable positive effect on upstream model performance.},
   author = {Rahat Ibn Alphenaar Grant
and Rafiq},
   city = {Cham},
   editor = {Eduardo
and Vargas-Solar Genoveva
and Marcacini Ricardo
and Tadonki Claude
and Calvo Hiram
and Alatrista-Salas Hugo Lossio-Ventura Juan Antonio
and Ceh-Varela},
   isbn = {978-3-031-63616-5},
   booktitle = {Information Management and Big Data},
   pages = {199-216},
   publisher = {Springer Nature Switzerland},
   title = {Predicting Course Performance on a Massive Open Online Course Platform: A Natural Language Processing Approach},
   year = {2024}
}
@inproceedings{NaikAtharvaandYin2024,
   abstract = {An advantage of Large Language Models (LLMs) is their contextualization capability – providing different responses based on student inputs like solution strategy or prior discussion, to potentially better engage students than standard feedback. We present a design and evaluation of a proof-of-concept LLM application to offer students dynamic and contextualized feedback. Specifically, we augment an Online Programming Exercise bot for a college-level Cloud Computing course with ChatGPT, which offers students contextualized reflection triggers during a collaborative query optimization task in database design. We demonstrate that LLMs can be used to generate highly situated reflection triggers that incorporate details of the collaborative discussion happening in context. We discuss in depth the exploration of the design space of the triggers and their correspondence with the learning objectives as well as the impact on student learning in a pilot study with 34 students. },
   author = {Jessica Ruhan
and Kamath Anusha
and Ma Qianou
and Wu Sherry Tongshuang
and Murray Charles
and Bogart Christopher
and Sakr Majd
and Rose Carolyn P Naik Atharva
and Yin},
   city = {Cham},
   editor = {Irene-Angelica
and Liu Zitao
and Santos Olga C.
and Bittencourt Ig Ibert Olney Andrew M.
and Chounta},
   isbn = {978-3-031-64302-6},
   booktitle = {Artificial Intelligence in Education},
   pages = {46-59},
   publisher = {Springer Nature Switzerland},
   title = {Generating Situated Reflection Triggers About Alternative Solution Paths: A Case Study of Generative AI for Computer-Supported Collaborative Learning},
   year = {2024}
}
@inproceedings{AOliveiraEduardoandMohoni2024,
   abstract = {Academic misconduct poses a growing challenge for higher education institutions worldwide. While AI presents valuable opportunities for learning enhancement, Unauthorized Content Generation (UCG) poses a significant threat to academic integrity. This paper addresses the challenges posed by UCG and explores innovative approaches to detection, focusing on the underutilised concept of authorship verification (AV). Despite the recognition of AV's potential, its application in education has been limited. This study investigates the feasibility of utilising students' academic writing profiles for AV to detect contract cheating and unacknowledged AI usage in academic contexts. Building upon previous research, this study enhances the existing Feature Vector Difference (FVD) AV method by introducing improvements to support better analysis, explainability, and interpretability of the classification process in an educational context. The refined classifier provides probability-based outputs, offering a transparent alternative to traditional ``black box'' binary outputs, and is able to identify stylometric features suitable for differentiating student's writing profiles. Through this research, we contribute to the advancement of AV technology in education towards explainability, providing educators with a valuable tool to uphold academic integrity and combat the proliferation of UCG in educational environments.},
   author = {Madhavi
and Rios Shannon A. Oliveira Eduardo
and Mohoni},
   city = {Cham},
   editor = {Irene-Angelica
and Liu Zitao
and Santos Olga C.
and Bittencourt Ig Ibert Olney Andrew M.
and Chounta},
   isbn = {978-3-031-64315-6},
   booktitle = {Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky},
   pages = {87-100},
   publisher = {Springer Nature Switzerland},
   title = {Towards Explainable Authorship Verification: An Approach to Minimise Academic Misconduct in Higher Education},
   year = {2024}
}
@inproceedings{GorissenSimonCorneliusandSauer2024,
   abstract = {The natural-language understanding, code generation, and reasoning abilities of Large Language Model (LLMs) have the potential to speed up development times, especially when combined with Low-Code Development Platform (LCDPs). They could also enable end-users to make small to medium-sized changes themselves, while experienced developers can focus on the more complicated development tasks. This paper demos a prototype implementation of this concept. It enables end-users to edit Oracle Application Express (APEX) low-code applications using natural language in a chat-like user interface (UI) powered by the GPT-4 Turbo LLM. We also evaluate this prototype in a qualitative user study with APEX customers from the industry and find that they generally like both the concept and the prototype. The main problem that the study uncovered is a lack of a common vocabulary between the LLM and the users. Participants suggest to solve this by integrating support features like a glossary and an element type and name inspector into the prototype.},
   author = {Stefan
and Beckmann Wolf G Gorissen Simon Cornelius
and Sauer},
   city = {Cham},
   editor = {Bilal
and Bernhaupt Regina
and Ardito Carmelo
and Sauer Stefan Lárusdóttir Marta Kristín
and Naqvi},
   isbn = {978-3-031-64576-1},
   booktitle = {Human-Centered Software Engineering},
   pages = {312-320},
   publisher = {Springer Nature Switzerland},
   title = {End-User Development of Oracle APEX Low-Code Applications Using Large Language Models},
   year = {2024}
}
@inproceedings{PereiraMoraesLeonardoMauroandJardim2024,
   abstract = {Querying massive and heterogeneous text data is challenging, transcending different business domains. Our study outlines the BigQA architecture, which is specifically designed to support text data queries on Big Data systems using natural language. The architectural design comprises several layers that are intentionally built to be independent of the programming language, technology, and querying algorithm utilized. Nevertheless, the implementation of this architecture remains unclear. In this study, we showcase the versatility and adaptability of BigQA by offering a comprehensive set of guidelines and three practical implementation pipelines. In addition, we performed 60 experiments on four different datasets and compared the recall results of three popular algorithms: BM25, TF-IDF, and DPR. Based on our experiments, BM25 had the best overall performance as a document query algorithm.},
   author = {Pedro Calciolari
and Aguiar Cristina Dutra Pereira Moraes Leonardo Mauro
and Jardim},
   city = {Cham},
   editor = {Michał
and Brodsky Alexander
and Hammoudi Slimane Filipe Joaquim
and Śmiałek},
   isbn = {978-3-031-64748-2},
   booktitle = {Enterprise Information Systems},
   pages = {42-65},
   publisher = {Springer Nature Switzerland},
   title = {BigQA: A Software Reference Architecture for Big Data Question Answering Systems},
   year = {2024}
}
@inproceedings{IonPatrickDFandWatt2024,
   abstract = {In this article we report on an initial exploration to assess the viability of using the general large language models (LLMs), recently made public, to classify mathematical documents. Automated classification would be useful from the applied perspective of improving the navigation of the literature and the more open-ended goal of identifying relations among mathematical results. The Mathematical Subject Classification MSC 2020, from MathSciNet and zbMATH, is widely used and there is a significant corpus of ground truth material in the open literature. We have evaluated the classification of preprint articles from arXiv.org according to MSC 2020. The experiment used only the title and abstract alone — not the entire paper. Since this was early in the use of chatbots and the development of their APIs, we report here on what was carried out by hand. Of course, the automation of the process will have to follow if it is to be generally useful. We found that in about 60\% of our sample the LLM produced a primary classification matching that already reported on arXiv. In about half of those instances, there were additional primary classifications that were not detected. In about 40\% of our sample, the LLM suggested a different classification than what was provided. A detailed examination of these cases, however, showed that the LLM-suggested classifications were in most cases better than those provided.},
   author = {Stephen M Ion Patrick D. F.
and Watt},
   city = {Cham},
   editor = {Laura Kohlhase Andrea
and Kovács},
   isbn = {978-3-031-66997-2},
   booktitle = {Intelligent Computer Mathematics},
   pages = {42-57},
   publisher = {Springer Nature Switzerland},
   title = {Using General Large Language Models to Classify Mathematical Documents},
   year = {2024}
}
@inproceedings{HemidAhmadandKhiat2024,
   abstract = {Tools for creating and managing knowledge graphs are a crucial ingredient of data management in various fields such as healthcare or manufacturing. Knowledge graphs comprise entities, attributes, and relationships, which help machines understand the meaning of data and facilitate data sharing. Although knowledge graphs are a key concept for many applications, their creation remains complex and challenging, especially for novice users. To address this limitation, we have developed KGraphX, an easy-to-use visual editor to make knowledge graph creation for users with limited knowledge of Semantic Web technology more engaging. KGraphX uses visual elements, predictive typing, and augmented information from existing knowledge repositories to create knowledge graphs easily. In a task-based evaluation, we have proven the usability and quality of the created knowledge graphs using KGraphX. Compared to other tools, our tool significantly enhanced knowledge graph creation by 80\%, while also improving its clarity by 91\%.},
   author = {Abderrahmane
and Jayakumar Megha
and Lange Christoph
and Quix Christoph
and Decker Stefan Hemid Ahmad
and Khiat},
   city = {Cham},
   editor = {Toshiyuki
and Manco Giuseppe
and Kotsis Gabriele
and Tjoa A Min
and Khalil Ismail Strauss Christine
and Amagasa},
   isbn = {978-3-031-68312-1},
   booktitle = {Database and Expert Systems Applications},
   pages = {53-68},
   publisher = {Springer Nature Switzerland},
   title = {Knowledge Graph Creation and Management Made Easy with KGraphX},
   year = {2024}
}
@inproceedings{BappyArupDuttaandMahmud2024,
   abstract = {The aftermath of primary cancer treatment presents a multitude of challenges for patients, necessitating prolonged recovery periods that can span months or even years. Survivors contend with a range of debilitating side effects, including fatigue, constant pain, lymphedema, weight fluctuations, swallowing difficulties, and menopause symptoms. These adversities often demand consistent counseling, compelling patients to visit hospitals regularly. However, the advent of advanced Artificial Intelligence (AI) offers a transformative solution. AI-powered chatbots, also known as conversational agents, emerge as potent allies capable of providing mental support, answering queries, diagnosing issues based on texted symptoms, and more. This paper explores the potential of AI-driven conversational agents to address the unique needs of cancer patients during their recovery phase. Drawing inspiration from successful applications in other chronic diseases, the proposed model capitalizes on the capabilities of the Bert-large-cased-whole-word-masking model. Demonstrating an impressive F1 score of 93.46\%, this model signifies its efficacy in serving as an intelligent ally for cancer survivors. By facilitating mental support and information dissemination, the proposed conversational agent system aligns with the evolving landscape of AI-driven healthcare, forging a path toward enhanced patient care and improved quality of life during the recovery journey.},
   author = {Tanjim
and Kaiser M Shamim
and Shahadat Hossain Mohammad
and Andersson Karl Bappy Arup Dutta
and Mahmud},
   city = {Cham},
   editor = {Hanene
and Kaiser M Shamim
and Ahmed Muhammad Raisuddin
and Zhong Ning Mahmud Mufti
and Ben-Abdallah},
   isbn = {978-3-031-68639-9},
   booktitle = {Applied Intelligence and Informatics},
   pages = {47-64},
   publisher = {Springer Nature Switzerland},
   title = {A BERT-Based Chatbot to Support Cancer Treatment Follow-Up},
   year = {2024}
}
@inproceedings{SumathiSandKevinHarris2024,
   abstract = {In the realm of job interviews, real-time challenges exist for both interviewers and job seekers. Interviewers often grapple with the time-consuming nature of sifting through numerous resumes, manually screening candidates, and conducting. Repetitive initial interview stages. Additionally, they encounter challenges related to ensuring standardisation and objectivity in evaluating candidates, thus potentially leading to bias and subjectivity in the process. Conversely, job seekers commonly face issues such as lack of feedback, limited preparation opportunities, and difficulties in showcasing their skills effectively within the confines of a traditional interview format. The AI-Driven Virtual Interviewer project resolves these challenges by integrating advanced technologies. It streamlines the initial screening process for interviewers, saving time and resources by automating the extraction of relevant information from resumes and generating standardised interview questions. Moreover, the project mitigates bias by offering an objective evaluation process, ensuring every candidate is assessed against the same criteria. For job seekers, the project provides a more realistic and immersive interview experience, thereby addressing the lack of adequate preparation opportunities. It allows candidates to practise in a lifelike environment, receive immediate feedback, and consequently improve their interview skills and confidence.},
   author = {D.
and Jeyanth A K Sumathi S.
and Kevin Harris},
   city = {Cham},
   editor = {Felix Enigo
and Rajaram Kanchana
and Balasundaram Prabavathy Owoc Mieczyslaw Lech
and Varghese Sicily},
   isbn = {978-3-031-69986-3},
   booktitle = {Computational Intelligence in Data Science},
   pages = {180-196},
   publisher = {Springer Nature Switzerland},
   title = {AI-Driven Interviewer: Enhancing Interview Experience Through Conversational AI},
   year = {2024}
}
@inproceedings{SampsonJenniferandKoczka2024,
   abstract = {This paper presents a novel multilingual NLP framework tailored for Brazilian offshore operations, combining data engineering, alignment techniques, and information extraction to offer safety recommendations based on a decade of Brazilian offshore experience. NLP models provide platform staff with relevant historical incident lessons, integrated into a microservice architecture for streamlined access. The paper also discusses future work using LLMs for improvement.},
   author = {Péter Sampson Jennifer
and Koczka},
   city = {Cham},
   editor = {Luigi
and Meziane Farid
and Sugumaran Vijayan Rapp Amon
and Di Caro},
   isbn = {978-3-031-70242-6},
   booktitle = {Natural Language Processing and Information Systems},
   pages = {367-377},
   publisher = {Springer Nature Switzerland},
   title = {A Multilingual NLP Framework for Offshore Installations},
   year = {2024}
}
@inproceedings{BanerjeeSomnathandDutta2024,
   abstract = {As the AI revolution unfolds, the push toward automating support systems in diverse professional fields ranging from open-source software to healthcare, and banking to transportation has become more pronounced. Central to the automation of these systems is the early detection of named entities, a task that is foundational yet fraught with challenges due to the need for domain-specific expert annotations amid a backdrop of specialized terminologies, making the process both costly and complex. In response to this challenge, our paper presents an innovative named entity recognition (NER) framework (https://github.com/NeuralSentinel/DistALANER) tailored for the open-source software domain. Our method stands out by employing a distantly supervised, two-step annotation process that cleverly exploits language heuristics, bespoke lookup tables, external knowledge bases, and an active learning model. This multifaceted strategy not only elevates model performance but also addresses the critical hurdles of high costs and the dearth of expert annotators. A notable achievement of our approach is its capability to enable pre-large language models (pre-LLMs) to significantly outperform specially designed generic/domain specific LLMs for NER tasks. We also show the effectiveness of NER in the downstream task of relation extraction.},
   author = {Avik
and Agrawal Aaditya
and Hazra Rima
and Mukherjee Animesh Banerjee Somnath
and Dutta},
   city = {Cham},
   editor = {Tomas
and Miliou Ioanna
and Nowaczyk Slawomir Bifet Albert
and Krilavičius},
   isbn = {978-3-031-70381-2},
   booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track},
   pages = {313-331},
   publisher = {Springer Nature Switzerland},
   title = {DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem},
   year = {2024}
}
@inproceedings{LaakmannLukasandCiftci2024,
   abstract = {Robotic process automation (RPA) is a lightweight approach to automating business processes using software robots that emulate user actions at the graphical user interface level. While RPA has gained popularity for its cost-effective and timely automation of rule-based, well-structured tasks, its symbolic nature has inherent limitations when approaching more complex tasks currently performed by human agents. Machine learning concepts enabling intelligent RPA provide an opportunity to broaden the range of automatable tasks. In this paper, we conduct a literature review to explore the connections between RPA and machine learning and organize the joint concept intelligent RPA into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML integration and RPA-ML interaction. Together, they comprise eight dimensions: architecture and ecosystem, capabilities, data basis, intelligence level, and technical depth of integration as well as deployment environment, lifecycle phase, and user-robot relation.},
   author = {Seyyid A.
and Janiesch Christian Laakmann Lukas
and Ciftci},
   city = {Cham},
   editor = {Manuel
and Jans Mieke
and Rosemann Michael Marrella Andrea
and Resinas},
   isbn = {978-3-031-70418-5},
   booktitle = {Business Process Management Forum},
   pages = {319-336},
   publisher = {Springer Nature Switzerland},
   title = {A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation},
   year = {2024}
}
@inproceedings{BazzanTuomasandOlojo2024,
   abstract = {Generative Artificial Intelligence (GenAI) has become a practical tool that exhibits the potential to revolutionize numerous industries through publicly available systems with simple yet effective interfaces. This paper outlines the findings of research conducted in a multivocal literature review (MLR) with the aim of exploring the impact of GenAI in software engineering, with a focus on the fundamental aspects, use cases, benefits, and risks associated with contemporary GenAI models leveraged in key industries and practices. Key findings indicate that GenAI is adopted in software engineering, with various reported benefits in areas including requirement engineering, estimation and testing. However, there are also some risks associated with GenAI-based Software Engineering, such as in the context of generated data consistency and accuracy (sometimes referred to as the Hallucination problem), plagiarism, bias, and security. GenAI-assisted software engineering is becoming more mainstream, but resolving all the associated issues is going to take some time.},
   author = {Benjamin
and Majda Przemysław
and Kelly Thomas
and Yilmaz Murat
and Marks Gerard
and Clarke Paul M Bazzan Tuomas
and Olojo},
   city = {Cham},
   editor = {Paul
and Riel Andreas
and Messnarz Richard
and Greiner Christian
and Peisl Thomas Yilmaz Murat
and Clarke},
   isbn = {978-3-031-71139-8},
   booktitle = {Systems, Software and Services Process Improvement},
   pages = {163-180},
   publisher = {Springer Nature Switzerland},
   title = {Analysing the Role of Generative AI in Software Engineering - Results from an MLR},
   year = {2024}
}
@inproceedings{Sandkuhl2024,
   abstract = {Large language models (LLM) have been successfully applied in enterprise modelling (EM) for various tasks, such as supporting modellers and domain experts in modelling the current situation in enterprises. An important factor for the successful application of LLM is the quality of the LLM output. This paper's research investigates whether LLM can be used as a tool for the quality control of LLM output. Starting from an analysis of LLM evaluation approaches, the paper focuses on investigating scenarios for LLM use and relevant quality criteria. The main contributions of this paper are (1) an approach for using LLM as support for quality control of LLM output in enterprise modelling consisting of quality criteria for defined application scenarios and their operationalization and (2) quasi-experiments showing the applicability of the approach.},
   author = {Kurt Sandkuhl},
   city = {Cham},
   editor = {Raimundas
and Laurenzi Emanuele Řepa Václav
and Matulevičius},
   isbn = {978-3-031-71333-0},
   booktitle = {Perspectives in Business Informatics Research},
   pages = {36-50},
   publisher = {Springer Nature Switzerland},
   title = {LLM-Assistance for Quality Control of LLM Output},
   year = {2024}
}
@inproceedings{EconomidesAnastasiosAandPerifanou2024,
   abstract = {Recently, ChatGPT emerged as a disruptive technology that can also transform education. During a university course on e-commerce that is based on project-based learning, students were challenged to use ChatGPT in implementing their personal project. About half of the students chose to ask ChatGPT for help in constructing their questionnaire for their personal online market research survey. Only few of them asked ChatGPT a sequence of multiple revised prompts. These students achieved excellent grades in their projects. Students who used ChatGPT believed that it is an easy-to-use, useful, and helpful assistance. However, they would like extra training on how to efficiently use ChatGPT and other generative artificial intelligence tools.},
   author = {Maria Economides Anastasios A.
and Perifanou},
   city = {Cham},
   editor = {Anastasios A Perifanou Maria
and Economides},
   isbn = {978-3-031-73990-3},
   booktitle = {Digital Transformation in Higher Education. Empowering Teachers and Students for Tomorrow's Challenges},
   pages = {27-39},
   publisher = {Springer Nature Switzerland},
   title = {University Students Using ChatGPT in Project-Based Learning},
   year = {2024}
}
@inbook{Zuckarelli2024,
   abstract = {In this chapter we will learn about the tools necessary for programming. These include code editors and Integrated Development Environments (IDEs). We discuss their features, advantages and disadvantages, and how to choose them. The chapter also explains where to find help and support if you have problems programming and how you can use artificial intelligence (AI) tools like ChatGPT.},
   author = {Joachim L Zuckarelli},
   city = {Wiesbaden},
   doi = {10.1007/978-3-658-42912-6_8},
   isbn = {978-3-658-42912-6},
   booktitle = {Learn coding with Python and JavaScript : A practical introduction for beginners },
   pages = {51-64},
   publisher = {Springer Fachmedien Wiesbaden},
   title = {What Do I Need for Programming?},
   url = {https://doi.org/10.1007/978-3-658-42912-6_8},
   year = {2024}
}
@inbook{Liang2020,
   abstract = {This chapter specifically introduces state-of-the-art artificial intelligence algorithms, such as deep learning, reinforcement learning, brother learning, and epiphany learning.},
   author = {Xun Liang},
   city = {Singapore},
   doi = {10.1007/978-981-15-7760-4_5},
   isbn = {978-981-15-7760-4},
   booktitle = {Social Computing with Artificial Intelligence },
   pages = {83-125},
   publisher = {Springer Singapore},
   title = {State-of-the-Art Artificial Intelligence Algorithms},
   url = {https://doi.org/10.1007/978-981-15-7760-4_5},
   year = {2020}
}
@inbook{Liang2020,
   abstract = {This chapter introduces readers online group behavior and psychology, and presents social computing applications in online crowd behavior and psychology.},
   author = {Xun Liang},
   city = {Singapore},
   doi = {10.1007/978-981-15-7760-4_12},
   isbn = {978-981-15-7760-4},
   booktitle = {Social Computing with Artificial Intelligence },
   pages = {257-276},
   publisher = {Springer Singapore},
   title = {Social Computing Application in Online Crowd Behavior and Psychology},
   url = {https://doi.org/10.1007/978-981-15-7760-4_12},
   year = {2020}
}
@inproceedings{XuZhihaoandShi2024,
   abstract = {The software system usually records important runtime information in the log for troubleshooting. Researchers mine large log data for anomalies. Many studies use log data to build deep-learning models for detecting system anomalies. Although progress has been made in log anomaly detection on high-performance computing platforms, it is still difficult to achieve real-time and accurate anomaly detection on mobile devices and Internet of Things devices, as these devices usually do not have high computational power. To solve the above limitations, we propose an efficient log anomaly detection based on dimension reduction and attention aware temporal convolutional network method, namely EfficientLog. The model achieves efficient and accurate log detection in two ways: (1) it reduces the communication cost between mobile devices and cloud computing platforms by reducing the log vector dimension through BERT-whitening, and (2) it detects log anomalies by using the attention aware temporal convolutional network to reduce model testing time and computational consumption. We evaluate the proposed method on two public datasets, and the experimental results show that EfficientLog can outperform existing popular log-based anomaly detection methods in terms of detection accuracy and computational consumption.},
   author = {Yuliang
and Su Zhiyuan
and Song Li
and Zhang Jianjun
and Wang Xinjun
and Li Hui Xu Zhihao
and Shi},
   city = {Singapore},
   editor = {Ruyi
and Chen Yunliang
and Li Jianxin
and Min Geyong Song Xiangyu
and Feng},
   isbn = {978-981-97-2303-4},
   booktitle = {Web and Big Data},
   pages = {498-512},
   publisher = {Springer Nature Singapore},
   title = {Efficient Log Anomaly Detection Based on Dimension Reduction and Attention Aware TCN},
   year = {2024}
}
@inproceedings{ZhuLiangandLiu2024,
   abstract = {Entity Resolution (ER) is one of the most important issues for improving data quality, which aims to identify the records from one and more datasets that refer to the same real-world entity. For the textual datasets with the attribute values of long word sequences, the traditional methods of ER may fail to capture accurately the semantic information of records, leading to poor effectiveness. To address this challenging problem, in this paper, by using pre-trained language model RoBERTa and by fine-tuning it in the training process, we propose a novel entity resolution model IGaBERT, in which interactive attention is applied to capture token-level differences between records and to break the restriction that the schema required identically, and then global attention is utilized to determine the importance of these differences. Extensive experiments without injecting domain knowledge are conducted to measure the effectiveness of the IGaBERT model over both structured datasets and textual datasets. The results indicate that IGaBERT significantly outperforms several state-of-the-art approaches over textual datasets, especially with small size of training data, and it is highly competitive with those approaches over structured datasets.},
   author = {Hao
and Song Xin
and Wei Yonggang
and Wang Yu Zhu Liang
and Liu},
   city = {Singapore},
   editor = {Ruyi
and Chen Yunliang
and Li Jianxin
and Min Geyong Song Xiangyu
and Feng},
   isbn = {978-981-97-2387-4},
   booktitle = {Web and Big Data},
   pages = {433-448},
   publisher = {Springer Nature Singapore},
   title = {Entity Resolution Based on Pre-trained Language Models with Two Attentions},
   year = {2024}
}
@inproceedings{Wong2024,
   abstract = {Learner engagement is essential for positive student learning experiences and outcomes. While the higher education sector has accumulated years of experience with learner engagement with conventional face-to-face instruction, it is also placing increasingly more emphasis on the combination of in-person and online teaching and learning (T&L) activities, which enhances T&L in multiple ways. Promoting and supporting learner engagement in blended learning environment is receiving greater attention and scrutiny in recent years. This keynote presentation will be situated in an AI-infused T&L landscape: the speaker will survey recent scholarship on generative artificial intelligence and blended learning and share her insights into strategies for delivering blended learning to maximize the benefit for instructors and learners in the emerging digital learning environments of higher education.},
   author = {Katrine K Wong},
   city = {Singapore},
   editor = {Chen
and Fan Chun Wai
and U Leong Hou
and Lu Angel Ma Will W. K.
and Li},
   isbn = {978-981-97-4442-8},
   booktitle = {Blended Learning. Intelligent Computing in Education},
   pages = {39-61},
   publisher = {Springer Nature Singapore},
   title = {Blended Learning and AI: Enhancing Teaching and Learning in Higher Education},
   year = {2024}
}
@inproceedings{ChenQiaoshengandShi2023,
   abstract = {The aggregation, fusion, sharing, opening, development, and utilization of public data provide a solid basis for promoting the development of e-government and digital economy. These activities rely on an infrastructure software called public data open platform (PDOP) to provide enabling services. While China's national PDOP has yet to be completed, one pathway is to integrate the existing hundreds of provincial-level and prefectural-level PDOPs. However, these local PDOPs exhibit high heterogeneity, e.g., using different data catalogs and different metadata formats. In this system paper, we meet the challenge by crawling and integrating metadata records for datasets registered in existing local PDOPs, and we develop a prototype PDOP that provides unified search services over the integrated metadata. We conduct experiments to evaluate the core components of our prototype.},
   author = {Qing
and Cheng Gong Chen Qiaosheng
and Shi},
   city = {Singapore},
   editor = {Yang
and Cao Longbing
and Xiao Fu
and Cui Yiping
and Gu Rong
and Wang Li
and Cui Laizhong
and Yang Wanqi Chen Enhong
and Gao},
   isbn = {978-981-99-8979-9},
   booktitle = {Big Data},
   pages = {32-43},
   publisher = {Springer Nature Singapore},
   title = {Dataset Search over Integrated Metadata from China's Public Data Open Platforms},
   year = {2023}
}
@inbook{Perry2024,
   abstract = {Throughout the rest of this book, we will be exploring many examples of historical models. To do so, we will need a language for describing them. This language will be part visual and part textual. The visual aspect of this language will aid in overall understanding, while the textual part will provide specificity.},
   author = {Michael L Perry},
   city = {Berkeley, CA},
   doi = {10.1007/979-8-8688-0288-1_3},
   isbn = {979-8-8688-0288-1},
   booktitle = {The Art of Immutable Architecture: Theory and Practice of Data Management in Distributed Systems},
   pages = {59-94},
   publisher = {Apress},
   title = {How to Read a Historical Model},
   url = {https://doi.org/10.1007/979-8-8688-0288-1_3},
   year = {2024}
}
@inbook{Cohan2024,
   abstract = {In early 2024, due to its rapid and widespread adoption and blend of opportunity and risk for companies, generative AI represented an exciting growth opportunity for consulting firms. Which firms had the best strategies to help companies profit from the technology's opportunities and protect against its risks? How high was the generative AI consulting industry's profit potential? What competitive strategies did leading generative AI consulting firms deploy to win and keep new clients? What principles for success emerged from analyzing these strategies? Read on for answers to these questions.},
   author = {Peter Cohan},
   city = {Berkeley, CA},
   doi = {10.1007/979-8-8688-0318-5_4},
   isbn = {979-8-8688-0318-5},
   booktitle = {Brain Rush: How to Invest and Compete in the Real World of Generative AI},
   pages = {67-109},
   publisher = {Apress},
   title = {Generative AI Consulting},
   url = {https://doi.org/10.1007/979-8-8688-0318-5_4},
   year = {2024}
}
@inbook{Cohan2024,
   abstract = {With apologies to Neil Young, this chapter concludes Brain Rush by recapping my thoughts on the answers to six key questions we explored in this book and sharing my perspective on how these answers could change in the future.},
   author = {Peter Cohan},
   city = {Berkeley, CA},
   doi = {10.1007/979-8-8688-0318-5_10},
   isbn = {979-8-8688-0318-5},
   booktitle = {Brain Rush: How to Invest and Compete in the Real World of Generative AI},
   pages = {377-395},
   publisher = {Apress},
   title = {After the Brain Rush: What Is Generative AI's Future?},
   url = {https://doi.org/10.1007/979-8-8688-0318-5_10},
   year = {2024}
}
@article{Fyfe2023,
   abstract = {This paper shares results from a pedagogical experiment that assigns undergraduates to “cheat” on a final class essay by requiring their use of text-generating AI software. For this assignment, students harvested content from an installation of GPT-2, then wove that content into their final essay. At the end, students offered a “revealed” version of the essay as well as their own reflections on the experiment. In this assignment, students were specifically asked to confront the oncoming availability of AI as a writing tool. What are the ethics of using AI this way? What counts as plagiarism? What are the conditions, if any, we should place on AI assistance for student writing? And how might working with AI change the way we think about writing, authenticity, and creativity? While students (and sometimes GPT-2) offered thoughtful reflections on these initial questions, actually composing with GPT-2 opened their perspectives more broadly on the ethics and practice of writing with AI. In this paper, I share how students experienced those issues, connect their insights to broader conversations in the humanities about writing and communication, and explain their relevance for the ethical use and evaluation of language models.},
   author = {Paul Fyfe},
   doi = {10.1007/s00146-022-01397-z},
   issn = {1435-5655},
   issue = {4},
   journal = {AI and SOCIETY},
   pages = {1395-1405},
   title = {How to cheat on your final paper: Assigning AI for student writing},
   volume = {38},
   url = {https://doi.org/10.1007/s00146-022-01397-z},
   year = {2023}
}
@article{Ball2023,
   abstract = {There is a deluge of AI-assisted decision-making systems, where our data serve as proxy to our actions, suggested by AI. The closer we investigate our data (raw input, or their learned representations, or the suggested actions), we begin to discover “bugs”. Outside of their test, controlled environments, AI systems may encounter situations investigated primarily by those in other disciplines, but experts in those fields are typically excluded from the design process and are only invited to attest to the ethical features of the resulting system or to comment on demonstrations of intelligence and aspects of craftmanship after the fact. This communicative impasse must be overcome. Our idea is that philosophical and engineering considerations interact and can be fruitfully combined in the AI design process from the very beginning. We embody this idea in the role of a philosopher engineer. We discuss the role of philosopher engineers in the three main design stages of an AI system: deployment management (what is the system’s intended use, in what environment?); objective setting (what should the system be trained to do, and how?); and training (what model should be used, and why?). We then exemplify the need for philosopher engineers with an illustrative example, investigating how the future decisions of an AI-based hiring system can be fairer than those contained in the biased input data on which it is trained; and we briefly sketch the kind of interdisciplinary education that we envision will help to bring about better AI.},
   author = {Brian Ball and Alexandros Koliousis},
   doi = {10.1007/s00146-022-01535-7},
   issn = {1435-5655},
   issue = {2},
   journal = {AI and SOCIETY},
   pages = {861-868},
   title = {Training philosopher engineers for better AI},
   volume = {38},
   url = {https://doi.org/10.1007/s00146-022-01535-7},
   year = {2023}
}
@article{Murphy2024,
   abstract = {Emergent digital technologies provide cultural heritage spaces with the opportunity to reassess their current user journey. An immersive user experience can be developed that is innovative, dynamic, and customised for each attendee. Museums have already begun to move towards interactive exhibitions utilising Artificial Intelligence (AI) and the Internet of Things (IOT), and more recently, the use of Virtual Reality (VR) and Augmented Reality (AR) has become more common in cultural heritage spaces to present items of historical significance. VR concentrates on the provision of full immersion within a digitised environment utilising a headset, whilst AR focuses on the inclusion of digitised content within the existing physical environment that can be accessed through a medium such as a mobile phone application. Machine learning techniques such as a recommender system can support an immersive user journey by issuing personalised recommendations regarding a user’s preferred future content based on their previous activity. An ethical approach is necessary to take the precautions required to protect the welfare of human participants and eliminate any aspect of stereotyping or biased behaviour. This paper sets out a human-centred manifesto intended to provide guidance when inducing smart digital immersion in cultural heritage spaces. A review of existing digital cultural heritage projects was conducted to determine their adherence to the manifesto with the findings indicating that Education was a primary focus across all projects and that Personalisation, Respect and Empathy, and Support were also highly valued. Additionally, the findings indicated that there were areas with room for improvement such as Fairness to ensure that a well-balanced human-centred system is implemented.},
   author = {Cian Murphy and Peter J Carew and Larry Stapleton},
   doi = {10.1007/s00146-023-01693-2},
   issn = {1435-5655},
   issue = {5},
   journal = {AI and SOCIETY},
   pages = {2401-2416},
   title = {A human-centred systems manifesto for smart digital immersion in Industry 5.0: a case study of cultural heritage},
   volume = {39},
   url = {https://doi.org/10.1007/s00146-023-01693-2},
   year = {2024}
}
@article{Lanyi2024,
   abstract = {Classical natural language processing endeavored to understand the language of native speakers. When this proved to lie beyond the horizon, a scaled-down version settled for text analysis and processing but retained the old name and acronym. But text ≠ language. Any combination of signs and symbols qualifies as text. Language presupposes meaning, which is what connects it to real life. Failing to distinguish between the two results in confusing humanoids (machines thinking like humans) with machinoids (humans thinking like machines). As scientific English (SciEng) became the lingua franca of science, it has acquired all the traits of a machine language: reduced vocabulary, where fewer and fewer words have taken on more and more meanings; prescribed use of pronouns; depersonalized rigid syntactic forms and rules of composition. Compliance with SciEng standards can be automatically verified, which means that Sci Eng can be automatically imitated, what is referred to as AI writing (ChatGPT). The article discusses an attempt to automatically correct deviations from the rules by what is touted as AI editing.},
   author = {Gabriel Lanyi},
   doi = {10.1007/s00146-023-01707-z},
   issn = {1435-5655},
   issue = {5},
   journal = {AI and SOCIETY},
   pages = {2457-2461},
   title = {The galloping editor},
   volume = {39},
   url = {https://doi.org/10.1007/s00146-023-01707-z},
   year = {2024}
}
@article{Wang2023-1,
   abstract = {This paper endeavors to appraise scholarly works from the 1940s to the contemporary era, examining the scientific quest to transpose human cognition and consciousness into a digital surrogate, while contemplating the potential ramifications should humanity attain such an abstract level of intellect. The discourse commences with an explication of theories concerning consciousness, progressing to the Turing Test apparatus, and intersecting with Damasio’s research on the human cerebrum, particularly in relation to consciousness, thereby establishing congruence between the Turing Test and Damasio’s notions of consciousness. Subsequently, the narrative traverses the evolutionary chronology of transmuting human cognition into machine sapience, and delves into the fervent endeavors to metamorphose human minds into synthetic counterparts. Additionally, theoretical perspectives from the domains of philosophy, psychology, and neuroscience provide insight into the constraints intrinsic to AI implementations, contentious hypotheses, the perils concealed within artificial networks, and the ethical considerations necessitated by AI frameworks. Furthermore, contemplation of prospective repercussions facilitates the refinement of strategic approaches to safeguard our future Augmented Age Realities within AI, circumventing the prospect of inhabiting an intimidating technopolis where a mere 30\% monopolize the intellect and ingenuity of the remaining 70\% of human minds.},
   author = {Zheng Wang and Di-tao Wu},
   doi = {10.1007/s00146-023-01753-7},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {The Digital Nexus: tracing the evolution of human consciousness and cognition within the artificial realm—a comprehensive review},
   url = {https://doi.org/10.1007/s00146-023-01753-7},
   year = {2023}
}
@article{Fecher2023,
   abstract = {The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 researchers specializing in AI and digitization. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and helps identify areas for future action.},
   author = {Benedikt Fecher and Marcel Hebing and Melissa Laufer and Jörg Pohle and Fabian Sofsky},
   doi = {10.1007/s00146-023-01791-1},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {Friend or foe? Exploring the implications of large language models on the science system},
   url = {https://doi.org/10.1007/s00146-023-01791-1},
   year = {2023}
}
@article{Eisenmann2023,
   abstract = {This paper examines Harold Garfinkel’s work with ELIZA and a related program LYRIC from 1967 to 1969. AI researchers have tended to treat successful human–machine interaction as if it relied primarily on non-human machine characteristics, and thus the often-reported attribution of human-like qualities to communication with computers has been criticized as a misperception—and humans who make such reports referred to as “deluded.” By contrast Garfinkel, building on two decades of prior research on information and communication, argued that the ELIZA and the LYRIC “chatbots” were achieving interactions that felt human to many users by exploiting human sense-making practices. In keeping with his long-term practice of using “trouble” as a way of discovering the taken-for-granted practices of human sense-making, Garfinkel designed scripts for ELIZA and LYRIC that he could disrupt in order to reveal how their success depended on human social practices. Hence, the announcement “Machine Down” by the chatbot was a desired result of Garfinkel’s interactions with it. This early (but largely unknown) research has implications not only for understanding contemporary AI chatbots, but also opens possibilities for respecifying current information systems design and computational practices to provide for the design of more flexible information objects.},
   author = {Clemens Eisenmann and Jakub Mlynář and Jason Turowetz and Anne W Rawls},
   doi = {10.1007/s00146-023-01793-z},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {“Machine Down”: making sense of human–computer interaction—Garfinkel’s research on ELIZA and LYRIC from 1967 to 1969 and its contemporary relevance},
   url = {https://doi.org/10.1007/s00146-023-01793-z},
   year = {2023}
}
@article{Murray-Rust2023,
   abstract = {Artificial intelligence (AI) and machine learning (ML) are increasingly integrated into the functioning of physical and digital products, creating unprecedented opportunities for interaction and functionality. However, there is a challenge for designers to ideate within this creative landscape, balancing the possibilities of technology with human interactional concerns. We investigate techniques for exploring and reflecting on the interactional affordances, the unique relational possibilities, and the wider social implications of AI systems. We introduced into an interaction design course (n = 100) nine ‘AI exercises’ that draw on more than human design, responsible AI, and speculative enactment to create experiential engagements around AI interaction design. We find that exercises around metaphors and enactments make questions of training and learning, privacy and consent, autonomy and agency more tangible, and thereby help students be more reflective and responsible on how to design with AI and its complex properties in both their design process and outcomes.},
   author = {Dave Murray-Rust and Maria Luce Lupetti and Iohanna Nicenboim and Wouter van der Hoog},
   doi = {10.1007/s00146-023-01794-y},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {Grasping AI: experiential exercises for designers},
   url = {https://doi.org/10.1007/s00146-023-01794-y},
   year = {2023}
}
@article{Marshall2023,
   abstract = {This paper examines the societal paradigm shift growing from the tension between traditional institutional structures, in law and medicine for example, and the expansion of the human population. Similarly, the definition of “reality” in relation to the technological ability to create “virtual reality” in this environment is examined as a cyberæsthetic component of this evolutionary process. The question is presented as to whether the mere algebraic expansion of the traditional systems is adequate to maintain the relationship between human beings and technology despite the pressure of increasing population numbers. At issue is the proposition of whether the quality of human existence is diminished by the rapid onset of technological innovation. The corollary concept is the potential subjugation of human beings to the machine in the name of efficiency, with the movie “Metropolis” as the example from popular culture. Set in the context of the Three Laws, the paper presents an interdisciplinary examination as to the relationship between technological progress and the quality of life of the human species.},
   author = {John McClellan Marshall},
   doi = {10.1007/s00146-023-01799-7},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {"Metropolis" Revisited. . .and Coming},
   url = {https://doi.org/10.1007/s00146-023-01799-7},
   year = {2023}
}
@article{Farina2024,
   abstract = {As we write this research paper, we notice an explosion in popularity of machine learning in numerous fields (ranging from governance, education, and management to criminal justice, fraud detection, and internet of things). In this contribution, rather than focusing on any of those fields, which have been well-reviewed already, we decided to concentrate on a series of more recent applications of deep learning models and technologies that have only recently gained significant track in the relevant literature. These applications are concerned with artistic production (Sect. 2.1), the writing process (Sect. 2.2), music production (Sect. 2.3), text recognition and attribution (Sect.  2.4). After reviewing and analyzing the positive contributions as well as some of the major limitations of these technologies in each of those fields, we critically reflect (Sect.  3) on how their widespread implementation may affect humans and their creativity. In Sect. 4, we notice that deep learning models are here to stay; so, rather than embracing a negative or pessimistic stance with respect to their future applications in creative domains, we suggest a balanced approach for their assessment and beneficial usage. Finally (Sect. 5), we conclude by summarising what we have achieved and by pointing out possible future research directions.},
   author = {Mirko Farina and Andrea Lavazza and Giuseppe Sartori and Witold Pedrycz},
   doi = {10.1007/s00146-023-01836-5},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {Machine learning in human creativity: status and perspectives},
   url = {https://doi.org/10.1007/s00146-023-01836-5},
   year = {2024}
}
@article{Mlyn2024,
   abstract = {Despite its elusiveness as a concept, ‘artificial intelligence’ (AI) is becoming part of everyday life, and a range of empirical and methodological approaches to social studies of AI now span many disciplines. This article reviews the scope of ethnomethodological and conversation analytic (EM/CA) approaches that treat AI as a phenomenon emerging in and through the situated organization of social interaction. Although this approach has been very influential in the field of computational technology since the 1980s, AI has only recently emerged as such a pervasive part of daily life to warrant a sustained empirical focus in EM/CA. Reviewing over 50 peer-reviewed publications, we find that the studies focus on various social and group activities such as task-oriented situations, semi-experimental setups, play, and everyday interactions. They also involve a range of participant categories including children, older participants, and people with disabilities. Most of the reviewed studies apply CA’s conceptual apparatus, its approach to data analysis, and core topics such as turn-taking and repair. We find that across this corpus, studies center on three key themes: openings and closing the interaction, miscommunication, and non-verbal aspects of interaction. In the discussion, we reflect on EM studies that differ from those in our corpus by focusing on praxeological respecifications of AI-related phenomena. Concurrently, we offer a critical reflection on the work of literature reviewing, and explore the tortuous relationship between EM and CA in the area of research on AI.},
   author = {Jakub Mlynář and Lynn de Rijk and Andreas Liesenfeld and Wyke Stommel and Saul Albert},
   doi = {10.1007/s00146-024-01919-x},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {AI in situated action: a scoping review of ethnomethodological and conversation analytic studies},
   url = {https://doi.org/10.1007/s00146-024-01919-x},
   year = {2024}
}
@article{de-Lima-Santos2024,
   abstract = {With the increasing adoption of artificial intelligence (AI) technologies in the news industry, media organizations have begun publishing guidelines that aim to promote the responsible, ethical, and unbiased implementation of AI-based technologies. These guidelines are expected to serve journalists and media workers by establishing best practices and a framework that helps them navigate ever-evolving AI tools. Drawing on institutional theory and digital inequality concepts, this study analyzes 37 AI guidelines for media purposes in 17 countries. Our analysis reveals key thematic areas, such as transparency, accountability, fairness, privacy, and the preservation of journalistic values. Results highlight shared principles and best practices that emerge from these guidelines, including the importance of human oversight, explainability of AI systems, disclosure of automated content, and protection of user data. However, the geographical distribution of these guidelines, highlighting the dominance of Western nations, particularly North America and Europe, can further ongoing concerns about power asymmetries in AI adoption and consequently isomorphism outside these regions. Our results may serve as a resource for news organizations, policymakers, and stakeholders looking to navigate the complex AI development toward creating a more inclusive and equitable digital future for the media industry worldwide.},
   author = {Mathias-Felipe de-Lima-Santos and Wang Ngai Yeung and Tomás Dodds},
   doi = {10.1007/s00146-024-01973-5},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {Guiding the way: a comprehensive examination of AI guidelines in global media},
   url = {https://doi.org/10.1007/s00146-024-01973-5},
   year = {2024}
}
@article{Pan2024,
   abstract = {In 2023, artificial intelligence was announced as a “game changer”—marking a rapid revolution in thinking technologies. A global debate began to emerge. By conducting a discourse analysis of 2023 US congressional testimonies and AI manifestos, we aim to map the emergence of debates over the start-up of a global governance controversy. Qualitative topical identification and semantic network analysis are deployed to identify the primary stakeholders and their contesting arguments. The resulting polylog exhibits sharp divisions among multiple, distinct pro-tech and pro-rights groups. Pro-tech stakeholders emphasize innovation and economic benefits, while pro-rights groups prioritize human rights and safety. Our linguistic and semantic network analyses provide micro-level insights into the polylogue entanglements of ethics and governance. The analysis of US AI advocacy discourse furnishes a baseline of contestation from which the ongoing development of the complex arguments of AI policies can be identified and evaluated.},
   author = {Shuya Pan and G Thomas Goodnight and Xingzhi Zhao and Yifan Wang and Lezi Xie and Jinxi Zhang},
   doi = {10.1007/s00146-024-02027-6},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {“Game changer”: the AI advocacy discourse of 2023 in the US},
   url = {https://doi.org/10.1007/s00146-024-02027-6},
   year = {2024}
}
@article{Benk2024,
   abstract = {Trust is widely regarded as a critical component to building artificial intelligence (AI) systems that people will use and safely rely upon. As research in this area continues to evolve, it becomes imperative that the research community synchronizes its empirical efforts and aligns on the path toward effective knowledge creation. To lay the groundwork toward achieving this objective, we performed a comprehensive bibliometric analysis, supplemented with a qualitative content analysis of over two decades of empirical research measuring trust in AI, comprising 1’156 core articles and 36’306 cited articles across multiple disciplines. Our analysis reveals several “elephants in the room” pertaining to missing perspectives in global discussions on trust in AI, a lack of contextualized theoretical models and a reliance on exploratory methodologies. We highlight strategies for the empirical research community that are aimed at fostering an in-depth understanding of trust in AI.},
   author = {Michaela Benk and Sophie Kerstan and Florian von Wangenheim and Andrea Ferrario},
   doi = {10.1007/s00146-024-02059-y},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {Twenty-four years of empirical research on trust in AI: a bibliometric review of trends, overlooked issues, and future directions},
   url = {https://doi.org/10.1007/s00146-024-02059-y},
   year = {2024}
}
@article{Bory2024,
   abstract = {The current debate on artificial intelligence (AI) tends to associate AI imaginaries with the vision of a future technology capable of emulating or surpassing human intelligence. This article advocates for a more nuanced analysis of AI imaginaries, distinguishing “strong AI narratives,” i.e., narratives that envision futurable AI technologies that are virtually indistinguishable from humans, from "weak" AI narratives, i.e., narratives that discuss and make sense of the functioning and implications of existing AI technologies. Drawing on the academic literature on AI narratives and imaginaries and examining examples drawn from the debate on Large Language Models and public policy, we underscore the critical role and interplay of weak and strong AI across public/private and fictional/non-fictional discourses. The resulting analytical framework aims to empower approaches that are more sensitive to the heterogeneity of AI narratives while also advocating normalising AI narratives, i.e., positioning weak AI narratives more firmly at the center stage of public debates about emerging technologies.},
   author = {Paolo Bory and Simone Natale and Christian Katzenbach},
   doi = {10.1007/s00146-024-02087-8},
   issn = {1435-5655},
   journal = {AI and SOCIETY},
   title = {Strong and weak AI narratives: an analytical framework},
   url = {https://doi.org/10.1007/s00146-024-02087-8},
   year = {2024}
}
@article{Jin2023,
   abstract = {With the rapid development of artificial intelligence, privacy threats are already getting the spotlight. One of the most common privacy threats is the membership inference attack (MIA). Existing MIAs can effectively explore the potential privacy leakage risks of deep neural networks. However, DNNs are usually compressed for practical use, especially for edge computing, MIA will fail due to changes in DNNs’ structure or parameters during the compression. To address this problem, we propose CM-MIA, an MIA against compression models, which can effectively determine their privacy leakage risks before deployment. In specific, firstly we use a variety of compression methods to help build shadow models for different target models. Then, we use these shadow models to construct sample features and identify abnormal samples by calculating the distance between each sample feature. Finally, based on the hypothesis test, we determine whether the abnormal sample is a member of the training dataset. Meanwhile, only abnormal samples are used for membership inference, which reduces time costs and improves attack efficiency. Extensive experiments are conducted on 6 datasets to evaluate CM-MIA’s attack capacity. The results show that CM-MIA achieves the state-of-the-art attack performance in most cases. Compared with baselines, the attack success rate of CM-MIA is increased by 10.5\% on average.},
   author = {Yong Jin and Weidong Lou and Yanghua Gao},
   doi = {10.1007/s00607-023-01180-y},
   issn = {1436-5057},
   issue = {11},
   journal = {Computing},
   pages = {2419-2442},
   title = {Membership inference attacks against compression models},
   volume = {105},
   url = {https://doi.org/10.1007/s00607-023-01180-y},
   year = {2023}
}
@article{delaTorre-Lpez2023,
   abstract = {Artificial intelligence (AI) has acquired notorious relevance in modern computing as it effectively solves complex tasks traditionally done by humans. AI provides methods to represent and infer knowledge, efficiently manipulate texts and learn from vast amount of data. These characteristics are applicable in many activities that human find laborious or repetitive, as is the case of the analysis of scientific literature. Manually preparing and writing a systematic literature review (SLR) takes considerable time and effort, since it requires planning a strategy, conducting the literature search and analysis, and reporting the findings. Depending on the area under study, the number of papers retrieved can be of hundreds or thousands, meaning that filtering those relevant ones and extracting the key information becomes a costly and error-prone process. However, some of the involved tasks are repetitive and, therefore, subject to automation by means of AI. In this paper, we present a survey of AI techniques proposed in the last 15 years to help researchers conduct systematic analyses of scientific literature. We describe the tasks currently supported, the types of algorithms applied, and available tools proposed in 34 primary studies. This survey also provides a historical perspective of the evolution of the field and the role that humans can play in an increasingly automated SLR process.},
   author = {José de la Torre-López and Aurora Ramírez and José Raúl Romero},
   doi = {10.1007/s00607-023-01181-x},
   issn = {1436-5057},
   issue = {10},
   journal = {Computing},
   pages = {2171-2194},
   title = {Artificial intelligence to automate the systematic review of scientific literature},
   volume = {105},
   url = {https://doi.org/10.1007/s00607-023-01181-x},
   year = {2023}
}
@article{Thomopoulos2024,
   abstract = {Phishing is one of the most important security threats in modern information systems causing different levels of damages to end-users and service providers such as financial and reputational losses. State-of-the-art anti-phishing research is highly fragmented and monolithic and does not address the problem from a pervasive computing perspective. In this survey, we aim to contribute to the existing literature by providing a systematic review of existing experimental phishing research that employs EEG and eye-tracking methods within multi-modal and multi-sensory interaction environments. The main research objective of this review is to examine articles that contain results of at least one EEG-based and/or eye-tracking-based experimental setup within a phishing context. The database search with specific search criteria yielded 651 articles from which, after the identification and the screening process, 42 articles were examined as per the execution of experiments using EEG or eye-tracking technologies in the context of phishing, resulting to a total of 18 distinct papers that were included in the analysis. This survey is approaching the subject across the following pillars: a) the experimental design practices with an emphasis on the applied EEG and eye-tracking acquisition protocols, b) the artificial intelligence and signal preprocessing techniques that were applied in those experiments, and finally, c) the phishing attack types examined. We also provide a roadmap for future research in the field by suggesting ideas on how to combine state-of-the-art gaze-based mechanisms with EEG technologies for advancing phishing research. This leads to a discussion on the best practices for designing EEG and gaze-based frameworks.},
   author = {George A Thomopoulos and Dimitrios P Lyras and Christos A Fidas},
   doi = {10.1007/s00779-024-01794-9},
   issn = {1617-4917},
   issue = {3},
   journal = {Personal and Ubiquitous Computing},
   pages = {449-470},
   title = {A systematic review and research challenges on phishing cyberattacks from an electroencephalography and gaze-based perspective},
   volume = {28},
   url = {https://doi.org/10.1007/s00779-024-01794-9},
   year = {2024}
}
@article{Chen2024-9,
   abstract = {During the past decade of the big data era, mobile crowdsourcing has emerged as a popular research area, leveraging the collective intelligence and engagement of a vast number of individuals using their mobile devices. Another actively evolving area is machine learning, which has recently been augmented by the mobile crowdsourcing approach, especially for data collection and labeling. However, what happens when these two prevailing concepts meet? What topics have been discussed in recent literature? This paper adopts a systematic methodology, leveraging Latent Dirichlet allocation topic modeling for topic discovery from recent publications, to provide a comprehensive and insightful review of the intersection of machine learning and mobile crowdsourcing. Moreover, the paper highlights the emerging federated learning technology that integrates elements from both concepts. Key research questions are answered by examining discovered topics. The paper thoroughly discusses state-of-the-art developments and trends in combining these two concepts and explains the role of one concept in the other. The paper also addresses remaining challenges and outlines a future research agenda, including the potential incorporation of large language models into mobile crowdsourcing systems.},
   author = {Weisi Chen and Walayat Hussain and Islam Al-Qudah and Ghazi Al-Naymat and Xu Zhang},
   doi = {10.1007/s00779-024-01820-w},
   issn = {1617-4917},
   journal = {Personal and Ubiquitous Computing},
   title = {Intersection of machine learning and mobile crowdsourcing: a systematic topic-driven review},
   url = {https://doi.org/10.1007/s00779-024-01820-w},
   year = {2024}
}
@article{Florindi2024,
   abstract = {In today’s business landscape, Chatbots play a pivotal role in innovation and process optimization. In this paper, we introduced a novel advanced Emotional Chatbot AI, introducing sentiment analysis for human chatbot conversations. Adding an emotional component within the human-computer interaction, can in fact dramatically improve the quality of the final conversation between Chatbots and humans. More specifically, in our paper, we provided a practical evaluation of the EmoROBERTA software, introducing it into a novel implementation of an Emotional Chatbot. The pipeline we present is novel, and we developed it within a business context in which the use of sentimental and emotional responses can act in a significant and fundamental way toward the final success and use of the Chatbot itself. The architecture enriches user experience with real-time updates on the topic of interest, maintaining a user-centric design, toward an affective-response enhancement of the interaction established between the Chatbot and the user. The source code is fully available on GitHub: https://github.com/filippoflorindi/F-One.},
   author = {Filippo Florindi and Pasquale Fedele and Giovanna Maria Dimitri},
   doi = {10.1007/s00779-024-01824-6},
   issn = {1617-4917},
   journal = {Personal and Ubiquitous Computing},
   title = {A novel solution for the development of a sentimental analysis chatbot integrating ChatGPT},
   url = {https://doi.org/10.1007/s00779-024-01824-6},
   year = {2024}
}
@article{MontoyaEsquer2023,
   abstract = {In recent years, virtual reality has moved from a fantasy or science fiction theme to a reality increasingly closer to our homes and computers. As is the case of various technological advances, from military and scientific use to casual and routine use, which are increasingly adapted to day-to-day use cases, as is observed with the announcement of a new fashionable term, the metaverse. In the same way, this possibility today in the sight of innovators, entrepreneurs, merchants, and businesspeople, among others, begins to generate applications that show benefits in different branches such as education, medicine, psychology, human resources, real estate, tourism. The detail is that within the innovation, very little is being done for an improvement in text input, which generates a stagnation for continuing with a method of capturing text, even already known with lack of intuition and not optimal, such as the provided by the QWERTY keyboard. This article presents a proposal for a new text input method, a three-dimensional (3D) text input prototype focused on immersion in virtual reality, Wordsphere, which takes advantage of the need for technological adoption for novice users in virtual reality and, in turn, leaves on the table future lines of work and research.},
   author = {Jesús Emmanuel Montoya Esquer and Graciela Lara López},
   doi = {10.1007/s10055-023-00842-8},
   issn = {1434-9957},
   issue = {3},
   journal = {Virtual Reality},
   pages = {2769-2785},
   title = {Wordsphere: virtual reality text input interface},
   volume = {27},
   url = {https://doi.org/10.1007/s10055-023-00842-8},
   year = {2023}
}
@article{Dong2021,
   abstract = {With the extensive development of big data and social networks, the user profile field has received much attention. User profiling is essential for understanding the characteristics of various users, contributing to better understanding of their requirements in specific scenarios. User-generated contents which directly reflect people’s thoughts and intention are a valuable source for profiling users, among which user reviews by nature are invaluable sources for acquiring user requirements and have drawn increasing attention from both academia and industry. However, review-based user profiling (RBUP), as an emerging research direction, has not been systematically reviewed, hindering researchers from further investigation. In this work, we carry out a systematic mapping study on review-based user profiling, with an emphasis on investigating the generic analysis process of RBUP and identifying potential research directions. Specifically, 51 out of 2478 papers were carefully selected for investigation under a standardized and systematic procedure. By carrying out in-depth analysis over such papers, we have identified a generic process that should be followed to perform review-based user profiling. In addition, we perform multi-dimensional analysis on each step of the process in order to review current research progress and identify challenges and potential research directions. The results show that although traditional methods have been continuously improved, they are not sufficient to unleash the full potential of large-scale user reviews, especially the use of heterogeneous data for multi-dimensional user profiling.},
   author = {Xin Dong and Tong Li and Rui Song and Zhiming Ding},
   doi = {10.1007/s10270-020-00790-w},
   issn = {1619-1374},
   issue = {1},
   journal = {Software and Systems Modeling},
   pages = {49-69},
   title = {Profiling users via their reviews: an extended systematic mapping study},
   volume = {20},
   url = {https://doi.org/10.1007/s10270-020-00790-w},
   year = {2021}
}
@article{Almonte2022,
   abstract = {Recommender systems are information filtering systems used in many online applications like music and video broadcasting and e-commerce platforms. They are also increasingly being applied to facilitate software engineering activities. Following this trend, we are witnessing a growing research interest on recommendation approaches that assist with modelling tasks and model-based development processes. In this paper, we report on a systematic mapping review (based on the analysis of 66 papers) that classifies the existing research work on recommender systems for model-driven engineering (MDE). This study aims to serve as a guide for tool builders and researchers in understanding the MDE tasks that might be subject to recommendations, the applicable recommendation techniques and evaluation methods, and the open challenges and opportunities in this field of research.},
   author = {Lissette Almonte and Esther Guerra and Iván Cantador and Juan de Lara},
   doi = {10.1007/s10270-021-00905-x},
   issn = {1619-1374},
   issue = {1},
   journal = {Software and Systems Modeling},
   pages = {249-280},
   title = {Recommender systems in model-driven engineering},
   volume = {21},
   url = {https://doi.org/10.1007/s10270-021-00905-x},
   year = {2022}
}
@article{Verbruggen2023,
   abstract = {The Object Management Group introduced the Model-Driven Architecture in 2001. Since then, the research community has embraced model-driven engineering (MDE), but to a lesser extent than practitioners had hoped. A good awareness of practitioners’ challenges, particularly with modeling, is required to ensure the relevance of a research agenda. Therefore, this study conducts a meta-review on the state of practice in using modeling languages for software engineering over the last five years using Kitchenham’s guidelines. This study serves as an orientation within the research field and a basis for further research. It contributes to the literature by focusing on publications discussing the practical use of modeling languages and the benefits and problems perceived by practitioners. The main finding of this review is that practitioners benefit from MDE in the following ways: it is beneficial for several stakeholders; it saves cost; it is easy to use; it improves productivity, quality, and understanding of the system; and it provides support for software development activities. However, practitioners continue to face several serious challenges. The most frequently reported issues are the missing tool functionalities. Many studies have found that adhering to the Physics of Notation principles would improve modeling languages. Other findings include that modeling is mostly used for documentation and requirements elicitation, and UML is the most often used.},
   author = {Charlotte Verbruggen and Monique Snoeck},
   doi = {10.1007/s10270-022-01020-1},
   issn = {1619-1374},
   issue = {1},
   journal = {Software and Systems Modeling},
   pages = {111-129},
   title = {Practitioners’ experiences with model-driven engineering: a meta-review},
   volume = {22},
   url = {https://doi.org/10.1007/s10270-022-01020-1},
   year = {2023}
}

@article{Chakraborty2024,
   abstract = {Despite potential benefits in Software Engineering, adoption of software modelling in industry is low. Technical issues such as tool support have gained significant research before, but individual guidance and training have received little attention. As a first step towards providing the necessary guidance in modelling, we conduct a systematic literature review to explore the current state of the art. We searched academic literature for guidance on model creation and selected 35 papers for full-text screening through three rounds of selection. We find research on model creation guidance to be fragmented, with inconsistent usage of terminology, and a lack of empirical validation or supporting evidence. We outline the different dimensions commonly used to provide guidance on software and system model creation. Additionally, we provide definitions of the three terms modelling method, style, and guideline as current literature lacks a well-defined distinction between them. These definitions can help distinguishing between important concepts and provide precise modelling guidance.},
   author = {Shalini Chakraborty and Grischa Liebel},
   doi = {10.1007/s10270-023-01117-1},
   issn = {1619-1374},
   issue = {1},
   journal = {Software and Systems Modeling},
   pages = {249-265},
   title = {Modelling guidance in software engineering: a systematic literature review},
   volume = {23},
   url = {https://doi.org/10.1007/s10270-023-01117-1},
   year = {2024}
}
@article{Michael2024,
   abstract = {Models are the key tools humans use to manage complexity in description, development, and analysis. This applies to all scientific and engineering disciplines and in particular to the development of software and data-intensive systems. However, different methods and terminologies have become established in the individual disciplines, even in the sub-fields of Informatics, which raises the need for a comprehensive and cross-sectional analysis of the past, present, and future of modeling research. This paper aims to shed some light on how different modeling disciplines emerged and what characterizes them with a discussion of the potential toward a common modeling future. It focuses on the areas of software, data, and process modeling and reports on an analysis of the research approaches, goals, and visions pursued in each, as well as the methods used. This analysis is based on the results of a survey conducted in the communities concerned, on a bibliometric study, and on interviews with a prominent representative of each of these communities. The paper discusses the different viewpoints of the communities, their commonalities and differences, and identifies possible starting points for further collaboration. It further discusses current challenges for the communities in general and modeling as a research topic in particular and highlights visions for the future.},
   author = {Judith Michael and Dominik Bork and Manuel Wimmer and Heinrich C Mayr},
   doi = {10.1007/s10270-023-01128-y},
   issn = {1619-1374},
   issue = {1},
   journal = {Software and Systems Modeling},
   pages = {7-28},
   title = {Quo Vadis modeling?},
   volume = {23},
   url = {https://doi.org/10.1007/s10270-023-01128-y},
   year = {2024}
}
@article{Bozyigit2024,
   abstract = {Software requirements specification describes users’ needs and expectations on some target system. Requirements documents are typically represented by unstructured natural language text. Such texts are the basis for the various subsequent activities in software development, such as software analysis and design. As part of software analysis, domain models are made that describe the key concepts and relations between them. Since the analysis process is performed manually by business analysts, it is time-consuming and may introduce mistakes. Recently, researchers have worked toward automating the synthesis of domain models from textual software requirements. Current studies on this topic have limitations in terms of the volume and heterogeneity of experimental datasets. To remedy this, we provide a curated dataset of software requirements to be utilized as a benchmark by algorithms that transform textual requirements documents into domain models. We present a detailed evaluation of two text-to-model approaches: one based on a large-language model (ChatGPT) and one building on grammatical rules (txt2Model). Our evaluation reveals that both tools yield promising results with relatively high F-scores for modeling the classes, attributes, methods, and relationships, with txt2Model performing better than ChatGPT on average. Both tools have relatively lower performance and high variance when it comes to the relation types. We believe our dataset and experimental evaluation pave to way to advance the field of automated model generation from requirements.},
   author = {Fatma Bozyigit and Tolgahan Bardakci and Alireza Khalilipour and Moharram Challenger and Guus Ramackers and Önder Babur and Michel R V Chaudron},
   doi = {10.1007/s10270-024-01176-y},
   issn = {1619-1374},
   journal = {Software and Systems Modeling},
   title = {Generating domain models from natural language text using NLP: a benchmark dataset and experimental comparison of tools},
   url = {https://doi.org/10.1007/s10270-024-01176-y},
   year = {2024}
}
@article{Kaur2020,
   abstract = {The cultural and regional diversity across the world and specifically in India has given birth to a large number of writing systems and scripts having a variety of character sets. For scripts having a larger character set, just a simple keyboard with limited character set is not the optimal way for providing inputs to the computer. Variations in individual handwriting due to mood swings, changes in medium of writing, changes in writing styles, etc. pose a challenge before the character recognition (CR) research community. Similar kinds of symbols in various scripts and languages act as a big barrier in multilingual CR. Lack of benchmark results and corpora for multilingual CR hinder the research in multilingual CR. There have been only a limited number of articles for optimal combination of features and classifiers to process multilingual data. Multilingual CR has least explored the Indic scripts. This paper presents a detailed review and analysis of the work done in multilingual online as well as offline CR for Indic and non-Indic scripts. The paper mainly contributes in two ways: Firstly, it provides a clear perspective about various phases of monolingual and multilingual CR; and secondly, identifies the major deficiencies in monolingual and multilingual CR for printed and handwritten text. It contributes by giving an in-depth view of work done at each phase including data acquisition, pre-processing, segmentation, feature extraction, recognition and post-processing of CR. Issues to be resolved at each phase have also been elaborated. The recent work done using Deep and Shallow architectures has been analysed. Tools used for these architectures have been compared to highlight their pros and cons. The present work also suggests how further research can be conducted in the field of monolingual and multilingual CR. The problems such as CR in hybrid documents, identifying more reliable features, resolving issues of similar characters, identifying optimal combination strategies for deep and shallow architectures, etc. need to be tackled in future research.},
   author = {Sukhandeep Kaur and Seema Bawa and Ravinder Kumar},
   doi = {10.1007/s10462-019-09720-9},
   issn = {1573-7462},
   issue = {3},
   journal = {Artificial Intelligence Review},
   pages = {1813-1872},
   title = {A survey of mono- and multi-lingual character recognition using deep and shallow architectures: indic and non-indic scripts},
   volume = {53},
   url = {https://doi.org/10.1007/s10462-019-09720-9},
   year = {2020}
}
@article{Lata2021,
   abstract = {In linguistics, the Anaphora Resolution (AR) is the method of identifying the antecedent for anaphora. In simple terms, this is the problem that helps to solve what the expression referring to a referent refers to. It is considered to be one of the tedious tasks in Natural Language Processing (NLP). AR’s burgeoning popularity among researchers is attributable to its strong relevance to machine translation, text summarization, chatbot, question answering, and many others. This paper presents a review of AR approaches based on significant features utilized to perform this task and presents the evaluation metrics for this field. The feature is a relevant term related to AR that provides vital information regarding anaphor, antecedent, and relation between them. In this context, features represent the lexical, syntactical, semantical, and positional relationship between anaphor and its possible candidate antecedent. The performance of the Anaphora resolution system is profoundly dependent on the features used in the AR system. Hence, the selection of features for the AR system is highly significant. The main emphasis is to provide an overview of the various features needed to extract both the Anaphora and the Antecedent, respectively, used in different AR systems, present in literature. It is observed that syntactical information enhances the correctness of determining the properties for the existence of an anaphor and antecedent identification. Nowadays the trend is changing from hand-crafted feature dependent methods to deep learning approaches which try to learn feature representation. The performance of deep learning is progressing due to the accessibility of additional data and more powerful computing resources. This survey will provide the state-of art for the better understanding of solving AR problem from the feature selection perspective. The findings of this survey are useful to provide valuable insight into present trends and are helpful for researchers who are looking for developing AR system within given constraints.},
   author = {Kusum Lata and Pardeep Singh and Kamlesh Dutta},
   doi = {10.1007/s10462-020-09917-3},
   issn = {1573-7462},
   issue = {4},
   journal = {Artificial Intelligence Review},
   pages = {2917-3006},
   title = {A comprehensive review on feature set used for anaphora resolution},
   volume = {54},
   url = {https://doi.org/10.1007/s10462-020-09917-3},
   year = {2021}
}
@article{Dhar2021,
   abstract = {Automatic text categorization is the operation of sorting out the text documents into pre-defined text categories using some machine learning algorithms. Normally, it defines the most important approaches to organizing and making the use of a large volume of information exists in unstructured form. Nowadays, text categorization is becoming an extensively researched field of text mining and processing of languages. Word sense, semantic relationships among terms, text documents and categories are quite essential in order of enhancing the performances of categorization. Various surveys on text categorization have already been available which involve techniques of various text representation schemes to such extent but do not include several approaches that have been explored in text categorization over the standard techniques. Here, an exhaustive analysis of different text categorization approaches over the conventional approaches has been undertaken. This survey paper explores a wide variety of algorithms used for categorizing text documents and tries to assemble the existing works into three basic fields: conventional methods, fuzzy logic-based methods, deep learning-based methods. Further, conventional methods have been categorized into three fields: text categorization using handcrafted features, text categorization using nature-inspired algorithms and text categorization using graph-based methods. Furthermore, this survey provides a clear idea about the available libraries used for different algorithms, availability of datasets, categorization technologies explored in various non-Indian and Indian languages as well.},
   author = {Ankita Dhar and Himadri Mukherjee and Niladri Sekhar Dash and Kaushik Roy},
   doi = {10.1007/s10462-020-09919-1},
   issn = {1573-7462},
   issue = {4},
   journal = {Artificial Intelligence Review},
   pages = {3007-3054},
   title = {Text categorization: past and present},
   volume = {54},
   url = {https://doi.org/10.1007/s10462-020-09919-1},
   year = {2021}
}
@article{Ligthart2021,
   abstract = {With advanced digitalisation, we can observe a massive increase of user-generated content on the web that provides opinions of people on different subjects. Sentiment analysis is the computational study of analysing people's feelings and opinions for an entity. The field of sentiment analysis has been the topic of extensive research in the past decades. In this paper, we present the results of a tertiary study, which aims to investigate the current state of the research in this field by synthesizing the results of published secondary studies (i.e., systematic literature review and systematic mapping study) on sentiment analysis. This tertiary study follows the guidelines of systematic literature reviews (SLR) and covers only secondary studies. The outcome of this tertiary study provides a comprehensive overview of the key topics and the different approaches for a variety of tasks in sentiment analysis. Different features, algorithms, and datasets used in sentiment analysis models are mapped. Challenges and open problems are identified that can help to identify points that require research efforts in sentiment analysis. In addition to the tertiary study, we also identified recent 112 deep learning-based sentiment analysis papers and categorized them based on the applied deep learning algorithms. According to this analysis, LSTM and CNN algorithms are the most used deep learning algorithms for sentiment analysis.},
   author = {Alexander Ligthart and Cagatay Catal and Bedir Tekinerdogan},
   doi = {10.1007/s10462-021-09973-3},
   issn = {1573-7462},
   issue = {7},
   journal = {Artificial Intelligence Review},
   pages = {4997-5053},
   title = {Systematic reviews in sentiment analysis: a tertiary study},
   volume = {54},
   url = {https://doi.org/10.1007/s10462-021-09973-3},
   year = {2021}
}
@article{Cortis2021,
   abstract = {Social media popularity and importance is on the increase due to people using it for various types of social interaction across multiple channels. This systematic review focuses on the evolving research area of Social Opinion Mining, tasked with the identification of multiple opinion dimensions, such as subjectivity, sentiment polarity, emotion, affect, sarcasm and irony, from user-generated content represented across multiple social media platforms and in various media formats, like text, image, video and audio. Through Social Opinion Mining, natural language can be understood in terms of the different opinion dimensions, as expressed by humans. This contributes towards the evolution of Artificial Intelligence which in turn helps the advancement of several real-world use cases, such as customer service and decision making. A thorough systematic review was carried out on Social Opinion Mining research which totals 485 published studies and spans a period of twelve years between 2007 and 2018. The in-depth analysis focuses on the social media platforms, techniques, social datasets, language, modality, tools and technologies, and other aspects derived. Social Opinion Mining can be utilised in many application areas, ranging from marketing, advertising and sales for product/service management, and in multiple domains and industries, such as politics, technology, finance, healthcare, sports and government. The latest developments in Social Opinion Mining beyond 2018 are also presented together with future research directions, with the aim of leaving a wider academic and societal impact in several real-world applications.},
   author = {Keith Cortis and Brian Davis},
   doi = {10.1007/s10462-021-10030-2},
   issn = {1573-7462},
   issue = {7},
   journal = {Artificial Intelligence Review},
   pages = {4873-4965},
   title = {Over a decade of social opinion mining: a systematic review},
   volume = {54},
   url = {https://doi.org/10.1007/s10462-021-10030-2},
   year = {2021}
}
@article{Sousa2023,
   abstract = {Deep learning (DL) models for natural language processing (NLP) tasks often handle private data, demanding protection against breaches and disclosures. Data protection laws, such as the European Union’s General Data Protection Regulation (GDPR), thereby enforce the need for privacy. Although many privacy-preserving NLP methods have been proposed in recent years, no categories to organize them have been introduced yet, making it hard to follow the progress of the literature. To close this gap, this article systematically reviews over sixty DL methods for privacy-preserving NLP published between 2016 and 2020, covering theoretical foundations, privacy-enhancing technologies, and analysis of their suitability for real-world scenarios. First, we introduce a novel taxonomy for classifying the existing methods into three categories: data safeguarding methods, trusted methods, and verification methods. Second, we present an extensive summary of privacy threats, datasets for applications, and metrics for privacy evaluation. Third, throughout the review, we describe privacy issues in the NLP pipeline in a holistic view. Further, we discuss open challenges in privacy-preserving NLP regarding data traceability, computation overhead, dataset size, the prevalence of human biases in embeddings, and the privacy-utility tradeoff. Finally, this review presents future research directions to guide successive research and development of privacy-preserving NLP models.},
   author = {Samuel Sousa and Roman Kern},
   doi = {10.1007/s10462-022-10204-6},
   issn = {1573-7462},
   issue = {2},
   journal = {Artificial Intelligence Review},
   pages = {1427-1492},
   title = {How to keep text private? A systematic review of deep learning methods for privacy-preserving natural language processing},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-022-10204-6},
   year = {2023}
}
@article{Murshed2023,
   abstract = {Social media platforms such as (Twitter, Facebook, and Weibo) are being increasingly embraced by individuals, groups, and organizations as a valuable source of information. This social media generated information comes in the form of tweets or posts, and normally characterized as short text, huge, sparse, and low density. Since many real-world applications need semantic interpretation of such short texts, research in Short Text Topic Modeling (STTM) has recently gained a lot of interest to reveal unique and cohesive latent topics. This article examines the current state of the art in STTM algorithms. It presents a comprehensive survey and taxonomy of STTM algorithms for short text topic modelling. The article also includes a qualitative and quantitative study of the STTM algorithms, as well as analyses of the various strengths and drawbacks of STTM techniques. Moreover, a comparative analysis of the topic quality and performance of representative STTM models is presented. The performance evaluation is conducted on two real-world Twitter datasets: the Real-World Pandemic Twitter (RW-Pand-Twitter) dataset and Real-world Cyberbullying Twitter (RW-CB-Twitter) dataset in terms of several metrics such as topic coherence, purity, NMI, and accuracy. Finally, the open challenges and future research directions in this promising field are discussed to highlight the trends of research in STTM. The work presented in this paper is useful for researchers interested in learning state-of-the-art short text topic modelling and researchers focusing on developing new algorithms for short text topic modelling.},
   author = {Belal Abdullah Hezam Murshed and Suresha Mallappa and Jemal Abawajy and Mufeed Ahmed Naji Saif and Hasib Daowd Esmail Al-ariki and Hudhaifa Mohammed Abdulwahab},
   doi = {10.1007/s10462-022-10254-w},
   issn = {1573-7462},
   issue = {6},
   journal = {Artificial Intelligence Review},
   pages = {5133-5260},
   title = {Short text topic modelling approaches in the context of big data: taxonomy, survey, and analysis},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-022-10254-w},
   year = {2023}
}
@article{Beddiar2023,
   abstract = {Automatically understanding the content of medical images and delivering accurate descriptions is an emerging field of artificial intelligence that combines skills in both computer vision and natural language processing fields. Medical image captioning is involved in various applications related to diagnosis, treatment, report generation and computer-aided diagnosis to facilitate the decision making and clinical workflows. Unlike generic image captioning, medical image captioning highlights the relationships between image objects and clinical findings, which makes it a very challenging task. Although few review papers have already been published in this field, their coverage is still quite limited and only particular problems are addressed. This motivates the current paper where a rapid review protocol was adopted to review the latest achievements in automatic medical image captioning from the medical domain perspective. We aim through this review to provide the reader with an up-to-date literature in this field by summarizing the key findings and approaches in this field, including the related datasets, applications and limitations as well as highlighting the main competitions, challenges and future directions.},
   author = {Djamila-Romaissa Beddiar and Mourad Oussalah and Tapio Seppänen},
   doi = {10.1007/s10462-022-10270-w},
   issn = {1573-7462},
   issue = {5},
   journal = {Artificial Intelligence Review},
   pages = {4019-4076},
   title = {Automatic captioning for medical imaging (MIC): a rapid review of literature},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-022-10270-w},
   year = {2023}
}
@article{Bashir2023,
   abstract = {The Qur’an is a fourteen centuries old divine book in Arabic language that is read and followed by almost two billion Muslims globally as their sacred religious text. With the rise of Islam, the Arabic language gained popularity and became the lingua franca for large swaths of the old world. Devout Muslims read the Qur’an daily seeking guidance and comfort. Though the Qur’an, as a text, is short, there is a huge volume of supporting work filling tens of thousands of volumes, e.g., commentaries, exegesis, etc. Recently, there has been a renewed interest in such religious texts by non-specialists. Many of which were fueled by the recent advances in computational and natural language processing (NLP) techniques. These techniques help the development of tools that benefit common people to gain knowledge easily. This paper surveys the different efforts in the field of Qur’anic NLP, serving as a synthesized compendium of works (tools, data sets, approaches) covering the gamut from automated morphological analysis to correction of Qur’anic recitation via speech recognition. Multiple approaches are discussed for several tasks, where appropriate. Finally, we outline future research directions in this field.},
   author = {Muhammad Huzaifa Bashir and Aqil M Azmi and Haq Nawaz and Wajdi Zaghouani and Mona Diab and Ala Al-Fuqaha and Junaid Qadir},
   doi = {10.1007/s10462-022-10313-2},
   issn = {1573-7462},
   issue = {7},
   journal = {Artificial Intelligence Review},
   pages = {6801-6854},
   title = {Arabic natural language processing for Qur’anic research: a systematic review},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-022-10313-2},
   year = {2023}
}
@article{Waikhom2023,
   abstract = {In the last decade, deep learning has reinvigorated the machine learning field. It has solved many problems in computer vision, speech recognition, natural language processing, and other domains with state-of-the-art performances. In these domains, the data is generally represented in the Euclidean space. Various other domains conform to non-Euclidean space, for which a graph is an ideal representation. Graphs are suitable for representing the dependencies and inter-relationships between various entities. Traditionally, handcrafted features for graphs are incapable of providing the necessary inference for various tasks from this complex data representation. Recently, there has been an emergence of employing various advances in deep learning for graph-based tasks (called Graph Neural Networks (GNNs)). This article introduces preliminary knowledge regarding GNNs and comprehensively surveys GNNs in different learning paradigms—supervised, unsupervised, semi-supervised, self-supervised, and few-shot or meta-learning. The taxonomy of each graph-based learning setting is provided with logical divisions of methods falling in the given learning setting. The approaches for each learning task are analyzed from theoretical and empirical standpoints. Further, we provide general architecture design guidelines for building GNN models. Various applications and benchmark datasets are also provided, along with open challenges still plaguing the general applicability of GNNs.},
   author = {Lilapati Waikhom and Ripon Patgiri},
   doi = {10.1007/s10462-022-10321-2},
   issn = {1573-7462},
   issue = {7},
   journal = {Artificial Intelligence Review},
   pages = {6295-6364},
   title = {A survey of graph neural networks in various learning paradigms: methods, applications, and challenges},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-022-10321-2},
   year = {2023}
}
@article{Asudani2023,
   abstract = {The selection of word embedding and deep learning models for better outcomes is vital. Word embeddings are an n-dimensional distributed representation of a text that attempts to capture the meanings of the words. Deep learning models utilize multiple computing layers to learn hierarchical representations of data. The word embedding technique represented by deep learning has received much attention. It is used in various natural language processing (NLP) applications, such as text classification, sentiment analysis, named entity recognition, topic modeling, etc. This paper reviews the representative methods of the most prominent word embedding and deep learning models. It presents an overview of recent research trends in NLP and a detailed understanding of how to use these models to achieve efficient results on text analytics tasks. The review summarizes, contrasts, and compares numerous word embedding and deep learning models and includes a list of prominent datasets, tools, APIs, and popular publications. A reference for selecting a suitable word embedding and deep learning approach is presented based on a comparative analysis of different techniques to perform text analytics tasks. This paper can serve as a quick reference for learning the basics, benefits, and challenges of various word representation approaches and deep learning models, with their application to text analytics and a future outlook on research. It can be concluded from the findings of this study that domain-specific word embedding and the long short term memory model can be employed to improve overall text analytics task performance.},
   author = {Deepak Suresh Asudani and Naresh Kumar Nagwani and Pradeep Singh},
   doi = {10.1007/s10462-023-10419-1},
   issn = {1573-7462},
   issue = {9},
   journal = {Artificial Intelligence Review},
   pages = {10345-10425},
   title = {Impact of word embedding models on text analytics in deep learning environment: a review},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-023-10419-1},
   year = {2023}
}
@article{Mondal2023,
   abstract = {Machine translation (namely MT) has been one of the most popular fields in computational linguistics and Artificial Intelligence (AI). As one of the most promising approaches, MT can potentially break the language barrier of people from all over the world. Despite a number of studies in MT, there are few studies in summarizing and comparing MT methods. To this end, in this paper, we principally focus on presenting the two mainstream MT schemes: statistical machine translation (SMT) and neural machine translation (NMT), including their basic rationales and developments. Meanwhile, the detailed translation models are also presented, such as the word-based model, syntax-based model, and phrase-based model in statistical machine translation. Similarly, approaches in NMT, such as the recurrent neural network-based, attention mechanism-based, and transformer-based models are presented. Last but not least, the evaluation approaches also play an important role in helping developers to improve their methods better in MT. The prevailing machine translation evaluation methodologies are also presented in this article.},
   author = {Subrota Kumar Mondal and Haoxi Zhang and H M Dipu Kabir and Kan Ni and Hong-Ning Dai},
   doi = {10.1007/s10462-023-10423-5},
   issn = {1573-7462},
   issue = {9},
   journal = {Artificial Intelligence Review},
   pages = {10137-10226},
   title = {Machine translation and its evaluation: a study},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-023-10423-5},
   year = {2023}
}
@article{Bordoloi2023,
   abstract = {Sentiment analysis is a solution that enables the extraction of a summarized opinion or minute sentimental details regarding any topic or context from a voluminous source of data. Even though several research papers address various sentiment analysis methods, implementations, and algorithms, a paper that includes a thorough analysis of the process for developing an efficient sentiment analysis model is highly desirable. Various factors such as extraction of relevant sentimental words, proper classification of sentiments, dataset, data cleansing, etc. heavily influence the performance of a sentiment analysis model. This survey presents a systematic and in-depth knowledge of different techniques, algorithms, and other factors associated with designing an effective sentiment analysis model. The paper performs a critical assessment of different modules of a sentiment analysis framework while discussing various shortcomings associated with the existing methods or systems. The paper proposes potential multidisciplinary application areas of sentiment analysis based on the contents of data and provides prospective research directions.},
   author = {Monali Bordoloi and Saroj Kumar Biswas},
   doi = {10.1007/s10462-023-10442-2},
   issn = {1573-7462},
   issue = {11},
   journal = {Artificial Intelligence Review},
   pages = {12505-12560},
   title = {Sentiment analysis: A survey on design framework, applications and future scopes},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-023-10442-2},
   year = {2023}
}
@article{Kusal2023,
   abstract = {Artificial Intelligence (AI) has been used for processing data to make decisions, Interact with humans, and understand their feelings and emotions. With the advent of the Internet, people share and express their thoughts on day-to-day activities and global and local events through text messaging applications. Hence, it is essential for machines to understand emotions in opinions, feedback, and textual dialogues to provide emotionally aware responses to users in today's online world. The field of text-based emotion detection (TBED) is advancing to provide automated solutions to various applications, such as business and finance, to name a few. TBED has gained a lot of attention in recent times. The paper presents a systematic literature review of the existing literature published between 2005 and 2021 in TBED. This review has meticulously examined 63 research papers from the IEEE, Science Direct, Scopus, and Web of Science databases to address four primary research questions. It also reviews the different applications of TBED across various research domains and highlights its use. An overview of various emotion models, techniques, feature extraction methods, datasets, and research challenges with future directions has also been represented.},
   author = {Sheetal Kusal and Shruti Patil and Jyoti Choudrie and Ketan Kotecha and Deepali Vora and Ilias Pappas},
   doi = {10.1007/s10462-023-10509-0},
   issn = {1573-7462},
   issue = {12},
   journal = {Artificial Intelligence Review},
   pages = {15129-15215},
   title = {A systematic review of applications of natural language processing and future challenges with special emphasis in text-based emotion detection},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-023-10509-0},
   year = {2023}
}
@article{Ghosh2023,
   abstract = {Computational perception has indeed been dramatically modified and reformed from handcrafted feature-based techniques to the advent of deep learning. Scene text identification and recognition have inexorably been touched by this bow effort of upheaval, ushering in the period of deep learning. It is an important aspect of machine vision. Society has seen significant improvements in thinking, approach, and effectiveness over time. The goal of this study is to summarize and analyze the important developments and notable advancements in scene text identification and recognition over the past decade. We have discussed the significant handcrafted feature-based techniques which had been regarded as flagship systems in the past. They were succeeded by deep learning-based techniques. We have discussed such approaches from their inception to the development of complex models which have taken scene text identification to the next stage.},
   author = {Mridul Ghosh and Himadri Mukherjee and Sk Md Obaidullah and Xiao-Zhi Gao and Kaushik Roy},
   doi = {10.1007/s10462-023-10530-3},
   issn = {1573-7462},
   issue = {12},
   journal = {Artificial Intelligence Review},
   pages = {15301-15373},
   title = {Scene text understanding: recapitulating the past decade},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-023-10530-3},
   year = {2023}
}
@article{Lu2023,
   abstract = {Automatic summarization is attracting increasing attention as one of the most promising research areas. This technology has been tried in various real-world applications in recent years and achieved a good response. However, the applicability of conventional evaluation metrics cannot keep up with rapidly evolving summarization task formats and ensuing indicator. After recent years of research, automatic summarization task requires not only readability and fluency, but also informativeness and consistency. Diversified application scenarios also bring new challenges both for generative language models and evaluation metrics. In this review, we analysis and specifically focus on the difference between the task format and the evaluation metrics.},
   author = {Lingfeng Lu and Yang Liu and Weiqiang Xu and Huakang Li and Guozi Sun},
   doi = {10.1007/s10462-023-10582-5},
   issn = {1573-7462},
   issue = {2},
   journal = {Artificial Intelligence Review},
   pages = {2477-2507},
   title = {From task to evaluation: an automatic text summarization review},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-023-10582-5},
   year = {2023}
}
@article{Kazi2023,
   abstract = {Reading comprehension involves the process of reading and understanding textual information in order to answer questions related to it. It finds practical applications in various domains such as domain-specific FAQs, search engines, and dialog systems. Resource-rich languages like English, Japanese, Chinese, and most European languages benefit from the availability of numerous datasets and resources, enabling the development of machine reading comprehension (MRC) systems. However, building MRC systems for low-resource languages (LRL) with limited datasets, such as Vietnamese, Urdu, Bengali, and Hindi, poses significant challenges. To address this issue, this study utilizes quantitative analysis to conduct a systematic literature review (SLR) with the aim of comprehending the recent global shift in MRC research from high-resource languages (HRL) to low-resource languages. Notably, existing literature reviews on MRC lack comprehensive studies that compare techniques specifically designed for rich and low-resource languages. Hence, this study provides a comprehensive overview of the MRC research landscape in low-resource languages, offering valuable insights and a list of suggestions to enhance LRL–MRC research.},
   author = {Samreen Kazi and Shakeel Khoja and Ali Daud},
   doi = {10.1007/s10462-023-10583-4},
   issn = {1573-7462},
   issue = {2},
   journal = {Artificial Intelligence Review},
   pages = {2509-2569},
   title = {A survey of deep learning techniques for machine reading comprehension},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-023-10583-4},
   year = {2023}
}
@article{Safari2023,
   abstract = {Emotion detection from text is a relatively new sub-field of artificial intelligence closely related to Sentiment Analysis (SA). SA detects positive, neutral, or negative emotions in text. In contrast, emotion analysis detects and distinguishes certain types of emotions expressed in textbooks, such as disgust, fear, anger, happiness, surprise and sadness. Meanwhile, personality is a critical psychological concept that accounts for unique characteristics. Identifying and validating an individual’s personality efficiently and reliably is an admirable goal. This article aims to present a simultaneous review of Emotion and Personality detection from texts and elaborates upon approaches in developing text-based Emotion and Personality detection systems. The studies’ essential contributions, methodologies, datasets, conclusions drawn, strengths, and limitations are also explored. Additionally, this article discusses some of the field’s state-of-the-art ideas. In conclusion, the study delves into specific challenges and possible future research directions for detecting emotions and personalities from the text.},
   author = {Faezeh Safari and Abdolah Chalechale},
   doi = {10.1007/s10462-023-10603-3},
   issn = {1573-7462},
   issue = {3},
   journal = {Artificial Intelligence Review},
   pages = {3273-3297},
   title = {Emotion and personality analysis and detection using natural language processing, advances, challenges and future scope},
   volume = {56},
   url = {https://doi.org/10.1007/s10462-023-10603-3},
   year = {2023}
}
@article{Rahman2024-1,
   abstract = {The rising pervasiveness of Artificial Intelligence (AI) has led applied linguists to combine it with language teaching and learning processes. In many cases, such implementation has significantly contributed to the field. The retrospective amount of literature dedicated on the use of AI in language learning (LL) is overwhelming. Thus, the objective of this paper is to map the existing literature on Artificial Intelligence in language learning through bibliometric and content analysis. From the Scopus database, we systematically explored, after keyword refinement, the prevailing literature of AI in LL. After excluding irrelevant articles, we conducted our study with 606 documents published between 2017 and 2023 for further investigation. This review reinforces our understanding by identifying and distilling the relationships between the content, the contributions, and the contributors. The findings of the study show a rising pattern of AI in LL. Along with the metrics of performance analysis, through VOSviewer and R studio (Biblioshiny), our findings uncovered the influential authors, institutions, countries, and the most influential documents in the field. Moreover, we identified 7 clusters and potential areas of related research through keyword analysis. In addition to the bibliographic details, this review aims to elucidate the content of the field. NVivo 14 and Atlas AI were used to perform content analysis to categorize and present the type of AI used in language learning, Language learning factors, and its participants.},
   author = {Abdur Rahman and Antony Raj and Prajeesh Tomy and Mohamed Sahul Hameed},
   doi = {10.1007/s10462-023-10643-9},
   issn = {1573-7462},
   issue = {4},
   journal = {Artificial Intelligence Review},
   pages = {107},
   title = {A comprehensive bibliometric and content analysis of artificial intelligence in language learning: tracing between the years 2017 and 2023},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-023-10643-9},
   year = {2024}
}
@article{Hu2024,
   abstract = {Temporal Action Detection (TAD) aims to accurately capture each action interval in an untrimmed video and to understand human actions. This paper comprehensively surveys the state-of-the-art techniques and models used for TAD task. Firstly, it conducts comprehensive research on this field through Citespace and comprehensively introduce relevant dataset. Secondly, it summarizes three types of methods, i.e., anchor-based, boundary-based, and query-based, from the design method level. Thirdly, it summarizes three types of supervised learning methods from the level of learning methods, i.e., fully supervised, weakly supervised, and unsupervised. Finally, this paper explores the current problems, and proposes prospects in TAD task.},
   author = {Kai Hu and Chaowen Shen and Tianyan Wang and Keer Xu and Qingfeng Xia and Min Xia and Chengxue Cai},
   doi = {10.1007/s10462-023-10650-w},
   issn = {1573-7462},
   issue = {2},
   journal = {Artificial Intelligence Review},
   pages = {26},
   title = {Overview of temporal action detection based on deep learning},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-023-10650-w},
   year = {2024}
}
@article{Kaur2024,
   abstract = {Requirement Analysis is the essential sub-field of requirements engineering (RE). From the last decade, numerous automatic techniques are widely exploited in requirements analysis. In this context, requirements identification and classification is challenging for RE community, especially in context of large corpus and app review. As a consequence, several Artificial Intelligence (AI) techniques such as Machine learning (ML), Deep learning (DL) and transfer learning (TL)) have been proposed to reduce the manual efforts of requirement engineer. Although, these approaches reported promising results than traditional automated techniques, but the knowledge of their applicability in real-life and actual use of these approaches is yet incomplete. The main objective of this paper is to systematically investigate and better understand the role of Artificial Intelligence (AI) techniques in identification and classification of software requirements. This study conducted a systematic literature review (SLR) and collect the primary studies on the use of AI techniques in requirements classification. (1) this study found that 60 studies are published that adopted automated techniques in requirements classification. The reported results indicate that transfer learning based approaches extensively used in classification and yielding most accurate results and outperforms the other ML and DL techniques. (2) The data extraction process of SLR indicates that Support Vector Machine (SVM) and Convolutional Neural Network (CNN) are widely used in selected studies. (3) Precision and Recall are the commonly used metrics for evaluating the performance of automated techniques. This paper revealed that while these AI approaches reported promising results in classification. The applicability of these existing techniques in complex and real-world settings has not been reported yet. This SLR calls for the urge for the close alliance between RE and AI techniques to handle the open issues confronted in the development of some real-world automated system.},
   author = {Kamaljit Kaur and Parminder Kaur},
   doi = {10.1007/s10462-023-10667-1},
   issn = {1573-7462},
   issue = {3},
   journal = {Artificial Intelligence Review},
   pages = {57},
   title = {The application of AI techniques in requirements classification: a systematic mapping},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-023-10667-1},
   year = {2024}
}
@article{Li2024-2,
   abstract = {As a multi-ethnic country with a large population, China is endowed with diverse dialects, which brings considerable challenges to speech recognition work. In fact, due to geographical location, population migration, and other factors, the research progress and practical application of Chinese dialect speech recognition are currently at different stages. Therefore, exploring the significant regional heterogeneities in specific recognition approaches and effects, dialect corpus, and other resources is of vital importance for Chinese speech recognition work. Based on this, we first start with the regional classification of dialects and analyze the pivotal acoustic characteristics of dialects, including specific vowels and tones patterns. Secondly, we comprehensively summarize the existing dialect phonetic corpus in China, which is of some assistance in exploring the general construction methods of dialect phonetic corpus. Moreover, we expound on the general process of dialect recognition. Several critical dialect recognition approaches are summarized and introduced in detail, especially the hybrid method of Artificial Neural Network (ANN) combined with the Hidden Markov Model(HMM), as well as the End-to-End (E2E). Thirdly, through the in-depth comparison of their principles, merits, disadvantages, and  recognition performance for different dialects, the development trends and challenges in dialect recognition in the future are pointed out. Finally, some application examples of dialect speech recognition are collected and discussed.},
   author = {Qiang Li and Qianyu Mai and Mandou Wang and Mingjuan Ma},
   doi = {10.1007/s10462-023-10668-0},
   issn = {1573-7462},
   issue = {2},
   journal = {Artificial Intelligence Review},
   pages = {25},
   title = {Chinese dialect speech recognition: a comprehensive survey},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-023-10668-0},
   year = {2024}
}
@article{Lim2024,
   abstract = {The rapidly growing research landscape in finance, encompassing environmental, social, and governance (ESG) topics and associated Artificial Intelligence (AI) applications, presents challenges for both new researchers and seasoned practitioners. This study aims to systematically map the research area, identify knowledge gaps, and examine potential research areas for researchers and practitioners. The investigation focuses on three primary research questions: the main research themes concerning ESG and AI in finance, the evolution of research intensity and interest in these areas, and the application and evolution of AI techniques specifically in research studies within the ESG and AI in finance domain. Eight archetypical research domains were identified: (i) Trading and Investment, (ii) ESG Disclosure, Measurement and Governance, (iii) Firm Governance, (iv) Financial Markets and Instruments, (v) Risk Management, (vi) Forecasting and Valuation, (vii) Data, and (viii) Responsible Use of AI. Distinctive AI techniques were found to be employed across these archetypes. The study contributes to consolidating knowledge on the intersection of ESG, AI, and finance, offering an ontological inquiry and key takeaways for practitioners and researchers. Important insights include the popularity and crowding of the Trading and Investment domain, the growth potential of the Data archetype, and the high potential of Responsible Use of AI, despite its low publication count. By understanding the nuances of different research archetypes, researchers and practitioners can better navigate this complex landscape and contribute to a more sustainable and responsible financial sector.},
   author = {Tristan Lim},
   doi = {10.1007/s10462-024-10708-3},
   issn = {1573-7462},
   issue = {4},
   journal = {Artificial Intelligence Review},
   pages = {76},
   title = {Environmental, social, and governance (ESG) and artificial intelligence in finance: State-of-the-art and research takeaways},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10708-3},
   year = {2024}
}
@article{Valencia2024,
   abstract = {In the field of art, machine learning models have been used to predict artistic styles in paintings. The foregoing is somewhat advantageous for analysts, as these tools can provide more valuable results and help reduce bias in the results and conclusions provided. Therefore, the objective of this research was to examine research trends in the use of machine learning to predict artistic styles from a bibliometric review based on the PRISMA methodology. From the search equations, 268 documents were found, out of which, following the application of inclusion and exclusion criteria, 128 documents were analyzed. Through quantitative analysis, a growing research interest in the subject is evident, progressing from user perception approaches to the utilization of tools like deep learning for art studies. Among the main results, it is possible to identify that one of the most used techniques in the field has been neural networks for pattern recognition. Also, a large part of the research focuses on the use of design software for image creation and manipulation. Finally, it is found that the number of studies focused on contemporary modern art is still limited, this is due to the fact that a large part of the investigations has focused on historical artistic styles.},
   author = {Jackeline Valencia and Geraldine García Pineda and Vanessa García Pineda and Alejandro Valencia-Arias and Juan Arcila-Diaz and Renata Teodori de la Puente},
   doi = {10.1007/s10462-024-10727-0},
   issn = {1573-7462},
   issue = {5},
   journal = {Artificial Intelligence Review},
   pages = {118},
   title = {Using machine learning to predict artistic styles: an analysis of trends and the research agenda},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10727-0},
   year = {2024}
}
@article{Battineni2024,
   abstract = {This narrative literature review has analyzed the integration of artificial intelligence (AI) and augmented reality (AR) in the field of maritime medicine. A comprehensive search was conducted in academic databases using relevant search terms, resulting in the identification of 257 records. After screening for relevance and quality, a final review was conducted on 17 papers. This review highlights the potential applications and benefits of AI and AR in enhancing medical practices and safety measures for seafarers. The integration of AI and AR technologies in maritime medicine shows promise in providing real-time medical assistance, remote consultations, augmented training, and improved diagnostic capabilities. Additionally, AI-driven predictive models can aid in early detection of health issues and support proactive health management onboard ships. Challenges related to data privacy, connectivity at sea, and the need for regulatory frameworks are also discussed. The data analysis reported in this review contributes to a better understanding of the current state and future potential of AI and AR in maritime medicine and provide insights into opportunities for further research and implementation in the maritime industry.},
   author = {Gopi Battineni and Nalini Chintalapudi and Giovanna Ricci and Ciro Ruocco and Francesco Amenta},
   doi = {10.1007/s10462-024-10735-0},
   issn = {1573-7462},
   issue = {4},
   journal = {Artificial Intelligence Review},
   pages = {100},
   title = {Exploring the integration of artificial intelligence (AI) and augmented reality (AR) in maritime medicine},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10735-0},
   year = {2024}
}
@article{Zhang2024,
   abstract = {With the support of advanced hardware and software technology, Artificial Intelligence (AI) techniques, especially the increasing number of deep learning algorithms, have spawned the popularization of online intelligent services and accelerated the contemporary development and applications of chatbot systems. The promise of providing 24/7 uninterrupted business services and minimizing workforce costs has made business chatbots a hot topic due to the impact of the pandemic. It has attracted considerable attention from academic researchers and business practitioners. However, a thorough technical review of advanced chatbot technologies and their relevance and applications to various business domains is rare in the literature. The main contribution of this review article is the critical analysis of various chatbot development approaches and the underlying deep learning computational methods in the context of some business applications. We first conceptualize current business chatbot architectures and illustrate the technical characteristics of two common structures. Next, we explore the mainstream deep learning technologies in chatbot design from the perspective of computational methods and usages. Then, we propose a new framework to classify chatbot construction architectures and differentiate the traditional retrieval-based and generation-based chatbots in terms of the modern pipeline and end-to-end structures. Finally, we highlight future research directions for business chatbots to enable researchers to devote their efforts to the most promising research topics and commercial scenarios and for practitioners to benefit from realizing the trend in business chatbot development and applications.},
   author = {Yongxiang Zhang and Raymond Y K Lau and Jingjun David Xu and Yanghui Rao and Yuefeng Li},
   doi = {10.1007/s10462-024-10744-z},
   issn = {1573-7462},
   issue = {5},
   journal = {Artificial Intelligence Review},
   pages = {113},
   title = {Business chatbots with deep learning technologies: state-of-the-art, taxonomies, and future research directions},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10744-z},
   year = {2024}
}
@article{Li2024-3,
   abstract = {Prognostics and health management (PHM) is critical for enhancing equipment reliability and reducing maintenance costs, and research on intelligent PHM has made significant progress driven by big data and deep learning techniques in recent years. However, complex working conditions and high-cost data collection inherent in real-world scenarios pose small-data challenges for the application of these methods. Given the urgent need for data-efficient PHM techniques in academia and industry, this paper aims to explore the fundamental concepts, ongoing research, and future trajectories of small data challenges in the PHM domain. This survey first elucidates the definition, causes, and impacts of small data on PHM tasks, and then analyzes the current mainstream approaches to solving small data problems, including data augmentation, transfer learning, and few-shot learning techniques, each of which has its advantages and disadvantages. In addition, this survey summarizes benchmark datasets and experimental paradigms to facilitate fair evaluations of diverse methodologies under small data conditions. Finally, some promising directions are pointed out to inspire future research.},
   author = {Chuanjiang Li and Shaobo Li and Yixiong Feng and Konstantinos Gryllias and Fengshou Gu and Michael Pecht},
   doi = {10.1007/s10462-024-10820-4},
   issn = {1573-7462},
   issue = {8},
   journal = {Artificial Intelligence Review},
   pages = {214},
   title = {Small data challenges for intelligent prognostics and health management: a review},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10820-4},
   year = {2024}
}
@article{Ofori-Boateng2024,
   abstract = {Systematic reviews (SRs) constitute a critical foundation for evidence-based decision-making and policy formulation across various disciplines, particularly in healthcare and beyond. However, the inherently rigorous and structured nature of the SR process renders it laborious for human reviewers. Moreover, the exponential growth in daily published literature exacerbates the challenge, as SRs risk missing out on incorporating recent studies that could potentially influence research outcomes. This pressing need to streamline and enhance the efficiency of SRs has prompted significant interest in leveraging Artificial Intelligence (AI) techniques to automate various stages of the SR process. This review paper provides a comprehensive overview of the current AI methods employed for SR automation, a subject area that has not been exhaustively covered in previous literature. Through an extensive analysis of 52 related works and an original online survey, the primary AI techniques and their applications in automating key SR stages, such as search, screening, data extraction, and risk of bias assessment, are identified. The survey results offer practical insights into the current practices, experiences, opinions, and expectations of SR practitioners and researchers regarding future SR automation. Synthesis of the literature review and survey findings highlights gaps and challenges in the current landscape of SR automation using AI techniques. Based on these insights, potential future directions are discussed. This review aims to equip researchers and practitioners with a foundational understanding of the basic concepts, primary methodologies, and recent advancements in AI-driven SR automation while guiding computer scientists in exploring novel techniques to invigorate further and advance this field.},
   author = {Regina Ofori-Boateng and Magaly Aceves-Martins and Nirmalie Wiratunga and Carlos Francisco Moreno-Garcia},
   doi = {10.1007/s10462-024-10844-w},
   issn = {1573-7462},
   issue = {8},
   journal = {Artificial Intelligence Review},
   pages = {200},
   title = {Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: a comprehensive review},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10844-w},
   year = {2024}
}
@article{Essam2024,
   abstract = {This survey reviews different research on question analysis, including other comparative studies of question analysis approaches and an evaluation of the questions by different NLP techniques that are used in question interpretation and categorization. Among these key findings noted includes the assessment of deep learning models such as M-BiGRU-CNN and M-TF-IDF, which come with high precision and accuracy when applied with the effectiveness of use in dealing with the complexities involved in a language. Some of the most mature machine learning algorithms, for example, SVM or logistic regression, remain powerful models, especially on the classification task, meaning that the latter continues to be relevant. This study further underlines the applicability of rule-based or hybrid methodologies in certain linguistic situations, and it must be said that custom design solutions are required. We could recommend, on this basis, directing future work towards the integration of these hybrid systems and towards the definition of more general methodologies of evaluation that are in line with the constant evolution of NLP technologies. It revealed that the underlying challenges and barriers in the domain are very complex syntactic and dialectic variations, unavailability of software tools, very critical standardization in Arabic datasets, benchmark creation, handling of translated data, and the integration of Large Language Models (LLMs). The paper discusses the lack of identity and processing of such structures through online systems for comparison. This comprehensive review highlights not only the diversified potential for the capabilities of NLP techniques in refining question analysis but also the potential way of great promises for further enhancements and improvements in this progressive domain.},
   author = {Mariam Essam and Mohanad A Deif and Rania Elgohary},
   doi = {10.1007/s10462-024-10880-6},
   issn = {1573-7462},
   issue = {9},
   journal = {Artificial Intelligence Review},
   pages = {251},
   title = {Deciphering Arabic question: a dedicated survey on Arabic question analysis methods, challenges, limitations and future pathways},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10880-6},
   year = {2024}
}
@article{Bolaos2024,
   abstract = {This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.},
   author = {Francisco Bolaños and Angelo Salatino and Francesco Osborne and Enrico Motta},
   doi = {10.1007/s10462-024-10902-3},
   issn = {1573-7462},
   issue = {10},
   journal = {Artificial Intelligence Review},
   pages = {259},
   title = {Artificial intelligence for literature reviews: opportunities and challenges},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10902-3},
   year = {2024}
}
@article{Budnik2024,
   abstract = {The discovery of non-coding RNAs (ncRNAs) has expanded our comprehension of RNAs’ inherent nature and capabilities. The intricate three-dimensional structures assumed by RNAs dictate their specific functions and molecular interactions. However, the limited number of mapped structures, partly due to experimental constraints of methods such as nuclear magnetic resonance (NMR), highlights the importance of in silico prediction solutions. This is particularly crucial in potential applications in therapeutic drug discovery. In this context, machine learning (ML) methods have emerged as prominent candidates, having previously demonstrated prowess in solving complex challenges across various domains. This review focuses on analyzing the development of ML-based solutions for RNA structure prediction, specifically oriented toward recent advancements in the deep learning (DL) domain. A systematic analysis of 33 works reveals insights into the representation of RNA structures, secondary structure motifs, and tertiary interactions. The review highlights current trends in ML methods used for RNA structure prediction, demonstrates the growing research involvement in this field, and summarizes the most valuable findings.},
   author = {Michał Budnik and Jakub Wawrzyniak and Łukasz Grala and Miłosz Kadziński and Natalia Szóstak},
   doi = {10.1007/s10462-024-10910-3},
   issn = {1573-7462},
   issue = {9},
   journal = {Artificial Intelligence Review},
   pages = {254},
   title = {Deep dive into RNA: a systematic literature review on RNA structure prediction using machine learning methods},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-10910-3},
   year = {2024}
}
@article{Abdallah2024,
   abstract = {This paper presents a comprehensive survey of over 100 research works on the topic of form understanding in the context of scanned documents. We delve into recent advancements and breakthroughs in the field, with particular focus on transformer-based models, which have been shown to improve performance in form understanding tasks by up to 25\% in accuracy compared to traditional methods. Our research methodology involves an in-depth analysis of popular documents and trends over the last decade, including 15 state-of-the-art models and 10 benchmark datasets. By examining these works, we offer novel insights into the evolution of this domain. Specifically, we highlight how transformers have revolutionized form-understanding techniques by enhancing the ability to process noisy scanned documents with significant improvements in OCR accuracy. Furthermore, we present an overview of the most relevant datasets, such as FUNSD, CORD, and SROIE, which serve as benchmarks for evaluating the performance of the models. By comparing the capabilities of these models and reporting an average improvement of 10–15\% in key form extraction tasks, we aim to provide researchers and practitioners with useful guidance in selecting the most suitable solutions for their form understanding applications.},
   author = {Abdelrahman Abdallah and Daniel Eberharter and Zoe Pfister and Adam Jatowt},
   doi = {10.1007/s10462-024-11000-0},
   issn = {1573-7462},
   issue = {12},
   journal = {Artificial Intelligence Review},
   pages = {342},
   title = {A survey of recent approaches to form understanding in scanned documents},
   volume = {57},
   url = {https://doi.org/10.1007/s10462-024-11000-0},
   year = {2024}
}
@article{Anila2024,
   abstract = {Smart aquaponics systems are gaining popularity as they contribute immensely to sustainable food production. These systems enhance traditional farming with advanced technologies like the Internet of Things (IoT), solar energy, and Artificial Intelligence (AI) for increased proficiency and productivity. However, assessing the performance and effectiveness of these systems is challenging. A systematic literature review (SLR) was conducted to examine the applications, technologies, and evaluation methods used in smart aquaponics. The study sourced peer-reviewed publications from IEEE Xplore, Scopus, SpringerLink and Science Direct. After applying inclusion and exclusion criteria, a total of 105 primary studies were selected for the SLR. The findings show that aquaponics predictions (27\%) have been under-explored compared to applications that involved monitoring or monitoring and controlling aquaponics (73\%). IoT technologies have been used to create prototype aquaponic systems and collect data, while machine learning/deep learning (predictive analytics) are used for prediction, abnormality detection, and intelligent decision-making. So far, predictive analytics solutions for aquaponics yield prediction, return-on-investment (ROI) estimates, resource optimisation, product marketing, security of aquaponics systems, and sustainability assessment have received very little attention. Also, few studies (37.7\%) incorporated any form of evaluation of the proposed solutions, while expert feedback and usability evaluation, which involved stakeholders and end-users of aquaponics solutions, have been rarely used for their assessment. In addition, existing smart aquaponics studies have limitations in terms of their short-term focus (monitoring and controlling of aquaponics not undertaken over a long time to assess performance and sustainability), being conducted mostly in controlled settings (which limits applicability to diverse conditions), and being focused on specific geographical contexts(which limits their generalizability). These limitations provide opportunities for future research. Generally, this study provides new insights and expands discussion on the topic of smart aquaponics.},
   author = {Mundackal Anila and Olawande Daramola},
   doi = {10.1007/s10462-024-11003-x},
   issn = {1573-7462},
   issue = {1},
   journal = {Artificial Intelligence Review},
   pages = {25},
   title = {Applications, technologies, and evaluation methods in smart aquaponics: a systematic literature review},
   volume = {58},
   url = {https://doi.org/10.1007/s10462-024-11003-x},
   year = {2024}
}
@article{Geng2022,
   abstract = {Software defined network (SDN) has gained a great attention in academic field for its separation of the control plane and the data plain to get a programmable network. In the study, we propose an SDN architecture for a scenario of intelligent patent prior art search system. Different from the current mainstream patent retrieval system, where patent prior art search is executed by means of traditional keywords matches under a fixed network topology, our proposed patent prior art search system based on SDN architecture can provide systematic and security analysis of patent text when encountering big data flows of patent applications. We also propose a new Phrase-based patent text representation model (PPTR), where the whole patent text is represented as a Bag of Phrases and then embedded into vector for patent prior art search, which could maintain the integrity of semantic units of patent text. Our experiments show that the proposed PPTR model achieves the best performance compared with traditional approaches of patent prior art search, and it is also expected that SDN architecture is a promising platform framework for other patent mining tasks.},
   author = {Boting Geng and Feng Wang},
   doi = {10.1007/s10515-022-00360-y},
   issn = {1573-7535},
   issue = {2},
   journal = {Automated Software Engineering},
   pages = {58},
   title = {An SDN architecture for patent prior art search system based on phrase embedding},
   volume = {29},
   url = {https://doi.org/10.1007/s10515-022-00360-y},
   year = {2022}
}
@article{Wang2024-7,
   abstract = {Developers frequently rely on APIs in their daily programming tasks, as APIs have become an indispensable tool for program development. However, with a vast number of open-source libraries available, selecting the appropriate API quickly can be a common challenge for programmers. Previous research on API recommendation primarily focused on designing better approaches to interpret user input. However, in practical applications, it is often difficult for users, especially novice programmers, to express their real intentions due to the limitations of language expression and programming capabilities. To address this issue, this paper introduces PTAPI, an approach that visualizes the user’s real intentions based on their query to enhance recommendation performance. Firstly, PTAPI identifies the prompt template from Stack Overflow (SO) posts based on the user’s input. Secondly, the obtained prompt template is combined with the user’s input to generate a new question. Finally, the newly generated question leverages dual information sources from SO posts and API official documentation to provide recommendations. To evaluate the effectiveness of PTAPI, we conducted experiments at both the class-level and method-level. The experimental results demonstrate the effectiveness of the proposed approach, with a significant improvement in the success rate.},
   author = {Yong Wang and Linjun Chen and Cuiyun Gao and Yingtao Fang and Yong Li},
   doi = {10.1007/s10515-024-00425-0},
   issn = {1573-7535},
   issue = {1},
   journal = {Automated Software Engineering},
   pages = {27},
   title = {Prompt enhance API recommendation: visualize the user’s real intention behind this query},
   volume = {31},
   url = {https://doi.org/10.1007/s10515-024-00425-0},
   year = {2024}
}
@article{Ahsan2024,
   abstract = {The security of an application is critical for its success, as breaches cause loss for organizations and individuals. Search-based software security testing (SBSST) is the field that utilizes metaheuristics to generate test cases for the software testing for some pre-specified security test adequacy criteria This paper conducts a systematic literature review to compare metaheuristics and fitness functions used in software security testing, exploring their distinctive capabilities and impact on vulnerability detection and code coverage. The aim is to provide insights for fortifying software systems against emerging threats in the rapidly evolving technological landscape. This paper examines how search-based algorithms have been explored in the context of code coverage and software security testing. Moreover, the study highlights different metaheuristics and fitness functions for security testing and code coverage. This paper follows the standard guidelines from Kitchenham to conduct SLR and obtained 122 primary studies related to SBSST after a multi-stage selection process. The papers were from different sources journals, conference proceedings, workshops, summits, and researchers’ webpages published between 2001 and 2022. The outcomes demonstrate that the main tackled vulnerabilities using metaheuristics are XSS, SQLI, program crash, and XMLI. The findings have suggested several areas for future research directions, including detecting server-side request forgery and security testing of third-party components. Moreover, new metaheuristics must also need to be explored to detect security vulnerabilities that are still unexplored or explored significantly less. Furthermore, metaheuristics can be combined with machine learning and reinforcement learning techniques for better results. Some metaheuristics can be designed by looking at the complexity of security testing and exploiting more fitness functions related to detecting different vulnerabilities.},
   author = {Fatma Ahsan and Faisal Anwer},
   doi = {10.1007/s10515-024-00433-0},
   issn = {1573-7535},
   issue = {2},
   journal = {Automated Software Engineering},
   pages = {44},
   title = {A systematic literature review on software security testing using metaheuristics},
   volume = {31},
   url = {https://doi.org/10.1007/s10515-024-00433-0},
   year = {2024}
}
@article{Ct2024,
   abstract = {Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. This paper’s objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. We summarize the content of 101 papers covering various data cleaning activities and provide 24 future work recommendations. Our review highlights many promising data cleaning techniques that can be further extended. We believe that our review of the literature will help the community develop better approaches to clean data.},
   author = {Pierre-Olivier Côté and Amin Nikanjam and Nafisa Ahmed and Dmytro Humeniuk and Foutse Khomh},
   doi = {10.1007/s10515-024-00453-w},
   issn = {1573-7535},
   issue = {2},
   journal = {Automated Software Engineering},
   pages = {54},
   title = {Data cleaning and machine learning: a systematic literature review},
   volume = {31},
   url = {https://doi.org/10.1007/s10515-024-00453-w},
   year = {2024}
}
@article{Casalnuovo2019,
   abstract = {Code corpora, as observed in large software systems, are now known to be far more repetitive and predictable than natural language corpora. But why? Does the difference simply arise from the syntactic limitations of programming languages? Or does it arise from the differences in authoring decisions made by the writers of these natural and programming language texts? We conjecture that the differences are not entirely due to syntax, but also from the fact that reading and writing code is un-natural for humans, and requires substantial mental effort; so, people prefer to write code in ways that are familiar to both reader and writer. To support this argument, we present results from two sets of studies: 1) a first set aimed at attenuating the effects of syntax, and 2) a second, aimed at measuring repetitiveness of text written in other settings (e.g. second language, technical/specialized jargon), which are also effortful to write. We find that this repetition in source code is not entirely the result of grammar constraints, and thus some repetition must result from human choice. While the evidence we find of similar repetitive behavior in technical and learner corpora does not conclusively show that such language is used by humans to mitigate difficulty, it is consistent with that theory. This discovery of “non-syntactic” repetitive behaviour is actionable, and can be leveraged for statistically significant improvements on the code suggestion task. We discuss this finding, and other future implications on practice, and for research.},
   author = {Casey Casalnuovo and Kenji Sagae and Prem Devanbu},
   doi = {10.1007/s10664-018-9669-7},
   issn = {1573-7616},
   issue = {4},
   journal = {Empirical Software Engineering},
   pages = {1823-1868},
   title = {Studying the difference between natural and programming language corpora},
   volume = {24},
   url = {https://doi.org/10.1007/s10664-018-9669-7},
   year = {2019}
}
@article{Nayebi2019,
   abstract = {Quality and market acceptance of software products is strongly influenced by responsiveness to customer requests. Once a customer request is received, a decision must be made whether to escalate it to the development team. Once escalated, the ticket must be formulated as a development task and assigned to a developer. To make the process more efficient and reduce the time between receiving and escalating the customer request, we aim to automate the complete customer request management process. We propose a holistic method called ESSMArT. The method performs text summarization, predicts ticket escalation, creates the ticket’s title and content, and ultimately assigns the ticket to an available developer. We began evaluating the method through an internal assessment of 4114 customer tickets from Brightsquid’s secure health care communication platform - Secure-Mail. Next, we conducted an external evaluation of the usefulness of the approach and concluded that: i) supervised learning based on context specific data performs best for extractive summarization; ii) Random Forest trained on a combination of conversation and extractive summarization works best for predicting escalation of tickets, with the highest precision (of 0.9) and recall (of 0.55). Through external evaluation, we furthermore found that ESSMArT provides suggestions that are 71\% aligned with human ones. Applying the prototype implementation to 315 customer requests resulted in an average time reduction of 9.2 min per request. ESSMArT helps to make ticket management faster and with reduced effort for human experts. We conclude that ESSMArT not only expedites ticket management, but furthermore reduces human effort. ESSMArT can help Brightsquid to (i) minimize the impact of staff turnover and (ii) shorten the cycle from an issue being reported to a developer being assigned to fix it.},
   author = {Maleknaz Nayebi and Liam Dicke and Ron Ittyipe and Chris Carlson and Guenther Ruhe},
   doi = {10.1007/s10664-019-09721-w},
   issn = {1573-7616},
   issue = {6},
   journal = {Empirical Software Engineering},
   pages = {3755-3789},
   title = {ESSMArT way to manage customer requests},
   volume = {24},
   url = {https://doi.org/10.1007/s10664-019-09721-w},
   year = {2019}
}
@article{Xu2020,
   abstract = {Nowadays, with the rapid growth of open source software (OSS), library reuse becomes more and more popular since a large amount of third- party libraries are available to download and reuse. A deeper understanding on why developers reuse a library (i.e., replacing self-implemented code with an external library) or re-implement a library (i.e., replacing an imported external library with self-implemented code) could help researchers better understand the factors that developers are concerned with when reusing code. This understanding can then be used to improve existing libraries and API recommendation tools for researchers and practitioners by using the developers concerns identified in this study as design criteria. In this work, we investigated the reasons behind library reuse and re-implementation. To achieve this goal, we first crawled data from two popular sources, F-Droid and GitHub. Then, potential instances of library reuse and re-implementation were found automatically based on certain heuristics. Next, for each instance, we further manually identified whether it is valid or not. For library re-implementation, we obtained 82 instances which are distributed in 75 repositories. We then conducted two types of surveys (i.e., individual survey to corresponding developers of the validated instances and another open survey) for library reuse and re-implementation. For library reuse individual survey, we received 36 responses out of 139 contacted developers. For re-implementation individual survey, we received 13 responses out of 71 contacted developers. In addition, we received 56 responses from the open survey. Finally, we perform qualitative and quantitative analysis on the survey responses and commit logs of the validated instances. The results suggest that library reuse occurs mainly because developers were initially unaware of the library or the library had not been introduced. Re-implementation occurs mainly because the used library method is only a small part of the library, the library dependencies are too complicated, or the library method is deprecated. Finally, based on all findings obtained from analyzing the surveys and commit messages, we provided a few suggestions to improve the current library recommendation systems: tailored recommendation according to users’ preferences, detection of external code that is similar to a part of the users’ code (to avoid duplication or re-implementation), grouping similar recommendations for developers to compare and select the one they prefer, and disrecommendation of poor-quality libraries.},
   author = {Bowen Xu and Le An and Ferdian Thung and Foutse Khomh and David Lo},
   doi = {10.1007/s10664-019-09771-0},
   issn = {1573-7616},
   issue = {1},
   journal = {Empirical Software Engineering},
   pages = {755-789},
   title = {Why reinventing the wheels? An empirical study on library reuse and re-implementation},
   volume = {25},
   url = {https://doi.org/10.1007/s10664-019-09771-0},
   year = {2020}
}
@article{Sulistya2020,
   abstract = {Software developers have benefited from various sources of knowledge such as forums, question-and-answer sites, and social media platforms to help them in various tasks. Extracting software-related knowledge from different platforms involves many challenges. In this paper, we propose an approach to improve the effectiveness of knowledge extraction tasks by performing cross-platform analysis. Our approach is based on transfer representation learning and word embedding, leveraging information extracted from a source platform which contains rich domain-related content. The information extracted is then used to solve tasks in another platform (considered as target platform) with less domain-related content. We first build a word embedding model as a representation learned from the source platform, and use the model to improve the performance of knowledge extraction tasks in the target platform. We experiment with Software Engineering Stack Exchange and Stack Overflow as source platforms, and two different target platforms, i.e., Twitter and YouTube. Our experiments show that our approach improves performance of existing work for the tasks of identifying software-related tweets and helpful YouTube comments.},
   author = {Agus Sulistya and Gede Artha Azriadi Prana and Abhishek Sharma and David Lo and Christoph Treude},
   doi = {10.1007/s10664-019-09775-w},
   issn = {1573-7616},
   issue = {1},
   journal = {Empirical Software Engineering},
   pages = {996-1030},
   title = {SIEVE: Helping developers sift wheat from chaff via cross-platform analysis},
   volume = {25},
   url = {https://doi.org/10.1007/s10664-019-09775-w},
   year = {2020}
}
@article{daSilva2020,
   abstract = {Developers often search for relevant code examples on the web for their programming tasks. Unfortunately, they face three major problems. First, they frequently need to read and analyse multiple results from the search engines to obtain a satisfactory solution. Second, the search is impaired due to a lexical gap between the query (task description) and the information associated with the solution (e.g., code example). Third, the retrieved solution may not be comprehensible, i.e., the code segment might miss a succinct explanation. To address these three problems, we propose CROKAGE (CrowdKnowledge Answer Generator), a tool that takes the description of a programming task (the query) as input and delivers a comprehensible solution for the task. Our solutions contain not only relevant code examples but also their succinct explanations written by human developers. The search for code examples is modeled as an Information Retrieval (IR) problem. We first leverage the crowd knowledge stored in Stack Overflow to retrieve the candidate answers against a programming task. For this, we use a fine-tuned IR technique, chosen after comparing 11 IR techniques in terms of performance. Then we use a multi-factor relevance mechanism to mitigate the lexical gap problem, and select the top quality answers related to the task. Finally, we perform natural language processing on the top quality answers and deliver the comprehensible solutions containing both code examples and code explanations unlike earlier studies. We evaluate and compare our approach against ten baselines, including the state-of-art. We show that CROKAGE outperforms the ten baselines in suggesting relevant solutions for 902 programming tasks (i.e., queries) of three popular programming languages: Java, Python and PHP. Furthermore, we use 24 programming tasks (queries) to evaluate our solutions with 29 developers and confirm that CROKAGE outperforms the state-of-art tool in terms of relevance of the suggested code examples, benefit of the code explanations and the overall solution quality (code + explanation).},
   author = {Rodrigo Fernandes Gomes da Silva and Chanchal K Roy and Mohammad Masudur Rahman and Kevin A Schneider and Klérisson Paixão and Carlos Eduardo de Carvalho Dantas and Marcelo de Almeida Maia},
   doi = {10.1007/s10664-020-09863-2},
   issn = {1573-7616},
   issue = {6},
   journal = {Empirical Software Engineering},
   pages = {4707-4758},
   title = {CROKAGE: effective solution recommendation for programming tasks by leveraging crowd knowledge},
   volume = {25},
   url = {https://doi.org/10.1007/s10664-020-09863-2},
   year = {2020}
}
@article{Choetkiertikul2021,
   abstract = {Today’s software development is typically driven by incremental changes made to software to implement a new functionality, fix a bug, or improve its performance and security. Each change request is often described as an issue. Recent studies suggest that a set of components (e.g., software modules) relevant to the resolution of an issue is one of the most important information provided with the issue that software engineers often rely on. However, assigning an issue to the correct component(s) is challenging, especially for large-scale projects which have up to hundreds of components. In this paper, we propose a predictive model which learns from historical issue reports and recommends the most relevant components for new issues. Our model uses Long Short-Term Memory, a deep learning technique, to automatically learn semantic features representing an issue report, and combines them with the traditional textual similarity features. An extensive evaluation on 142,025 issues from 11 large projects shows that our approach outperforms one common baseline, two state-of-the-art techniques, and six alternative techniques with an improvement of 16.70\%–66.31\% on average across all projects in predictive performance.},
   author = {Morakot Choetkiertikul and Hoa Khanh Dam and Truyen Tran and Trang Pham and Chaiyong Ragkhitwetsagul and Aditya Ghose},
   doi = {10.1007/s10664-020-09898-5},
   issn = {1573-7616},
   issue = {2},
   journal = {Empirical Software Engineering},
   pages = {14},
   title = {Automatically recommending components for issue reports using deep learning},
   volume = {26},
   url = {https://doi.org/10.1007/s10664-020-09898-5},
   year = {2021}
}
@article{Abid2021,
   abstract = {To save time, developers often search for code examples that implement their desired software features. Existing code search techniques typically focus on finding code snippets for a single given query, which means that developers need to perform a separate search for each desired functionality. In this paper, we propose FACER (F eature-driven A PI usage-based C ode E xamples R ecommender), a technique that avoids repeated searches through opportunistic reuse. Specifically, given the selected code snippet that matches the initial search query, FACER finds and suggests related code snippets that represent features that the developer may want to implement next. FACER first constructs a code fact repository by parsing the source code of open-source Java projects to obtain methods’ textual information, call graphs, and Application Programming Interface (API) usages. It then detects unique features by clustering methods based on similar API usages, where each cluster represents a feature or functionality. Finally, it detects frequently co-occurring features across projects using frequent pattern mining and recommends related methods from the mined patterns. To evaluate FACER, we run it on 120 Java Android apps from GitHub. We first manually validate that the detected method clusters represent methods with similar functionality. We then perform an automated evaluation to determine the best parameters (e.g., similarity threshold) for FACER. We recruit 10 professional developers along with 39 experienced students to judge FACER’s recommendation of related methods. Our results show that, on average, FACER’s recommendations are 80\% precise. We also survey a total of 20 professional Android and Java developers to understand their code search and reuse experiences, and also to obtain their feedback on the usability and usefulness of FACER. The survey results show that 95\% of our surveyed professional developers find the idea of related method recommendations useful during code reuse.},
   author = {Shamsa Abid and Shafay Shamail and Hamid Abdul Basit and Sarah Nadi},
   doi = {10.1007/s10664-021-10000-w},
   issn = {1573-7616},
   issue = {6},
   journal = {Empirical Software Engineering},
   pages = {110},
   title = {FACER: An API usage-based code-example recommender for opportunistic reuse},
   volume = {26},
   url = {https://doi.org/10.1007/s10664-021-10000-w},
   year = {2021}
}
@article{Li2021,
   abstract = {The requirements phase is the most critical phase of the software development life cycle. The quality of the requirements specification affects the overall quality of the subsequent phases and hence, the software product. An effective and efficient method to qualify the software requirements specification (SRS) is necessary to ensure the reliability and safety of software. In this paper, a requirements inspection method based on scenarios generated by model mutation (RIMSM) is proposed to detect defects in the functional requirements of a safety-critical system. The RIMSM method models software requirements using a High Level Extended Finite State Machine (HLEFSM). A method that executes the HLEFSM model is defined. The method uncovers the behaviors and generates the outputs of the system for a given scenario. To identify an adequate set of scenarios in which the model shall be executed, an analogue to mutation testing is defined which applies to the requirements phase. Twenty-one mutation operators are designed based on a taxonomy of defects defined for the requirements phase. Mutants of the HLEFSM model are generated using these operators. Further, an algorithm is developed to identify scenarios that can kill the mutants. The set of scenarios is considered to be adequate for detecting defects in the model when all mutants generated are killed. The HLEFSM model is then executed for the scenarios generated. The results of execution are used to detect defects in the model. A Requirements Inspection Tool based on Scenarios Generated by Model Mutation (RITSM) is developed to automate the application of the RIMSM method. The performance and usability of the RIMSM method are studied and demonstrated in an experiment by comparing the RIMSM method to the checklist-based reading method.},
   author = {Boyuan Li and Xiaoxu Diao and Wei Gao and Carol Smidts},
   doi = {10.1007/s10664-021-10001-9},
   issn = {1573-7616},
   issue = {5},
   journal = {Empirical Software Engineering},
   pages = {108},
   title = {A requirements inspection method based on scenarios generated by model mutation and the experimental validation},
   volume = {26},
   url = {https://doi.org/10.1007/s10664-021-10001-9},
   year = {2021}
}
@article{Mahadi2021,
   abstract = {Developer discussions range from in-person hallway chats to comment chains on bug reports. Being able to identify discussions that touch on software design would be helpful in documentation and refactoring software. Design mining is the application of machine learning techniques to correctly label a given discussion artifact, such as a pull request, as pertaining (or not) to design. In this paper we demonstrate a simple example of how design mining works. We then show how conclusion stability is poor on different artifact types and different projects. We show two techniques—augmentation and context specificity—that greatly improve the conclusion stability and cross-project relevance of design mining. Our new approach achieves AUC of 0.88 on within dataset classification and 0.80 on the cross-dataset classification task.},
   author = {Alvi Mahadi and Neil A Ernst and Karan Tongay},
   doi = {10.1007/s10664-021-10009-1},
   issn = {1573-7616},
   issue = {1},
   journal = {Empirical Software Engineering},
   pages = {9},
   title = {Conclusion stability for natural language based mining of design discussions},
   volume = {27},
   url = {https://doi.org/10.1007/s10664-021-10009-1},
   year = {2021}
}
@article{Li2021,
   abstract = {Developers often face difficulties in using different API methods during the software development process. Answering API related questions on API Q&A forums often costs API development teams a lot of time. To help save time for API development teams, we propose a deep learning-based approach, namely Rap4DQ, to identify relevant web API documentation for developer’s API related questions on API Q&A forums. Rap4DQ learns representation vectors for questions and API documentation separately using Gated Recurrent Unit (GRU) and adds different weights to reflect the various importance of varied API documents during training. Rap4DQ is designed to train on positive and negative samples with a loss function that minimizes the distances between questions and their relevant documentation, but maximizes the distances between questions and their irrelevant documentation. In the end, we construct a learning-to-rank layer to rank the API documentation based on learned representation vectors from GRUs. We have conducted several experiments to evaluate Rap4DQ on three popular and large API Q&A forums, Twitter, eBay, and AdWords. The results show that Rap4DQ can outperform all baselines by having a relative improvement up to 84.3\% in terms of AUC. Rap4DQ can obtain a high AUC of 0.84, 0.88, and 0.94 on identifying relevant API documentation on Twitter, eBay, and AdWords, respectively.},
   author = {Yi Li and Shaohua Wang and Wenbo Wang and Tien N Nguyen and Yan Wang and Xinyue Ye},
   doi = {10.1007/s10664-021-10067-5},
   issn = {1573-7616},
   issue = {1},
   journal = {Empirical Software Engineering},
   pages = {23},
   title = {Rap4DQ: Learning to recommend relevant API documentation for developer questions},
   volume = {27},
   url = {https://doi.org/10.1007/s10664-021-10067-5},
   year = {2021}
}
@article{Alves2022,
   abstract = {A Secondary Study (SS) is an important research method used in several areas. A crucial step in the Conduction phase of a SS is the search of studies. This step is time-consuming and error-prone, mainly due to the refinement of the search string. The objective of this study is to validate the effectiveness of an automatic formulation of search strings for SS. Our approach, termed Search String Generator (SeSG), takes as input a small set of studies (as a Quasi-Gold Standard) and processes them using text mining. After that, SeSG generates search strings that deliver a high F1-Score on the start set of a hybrid search strategy. To achieve this objective, we (1) generate a structured textual representation of the initial set of input studies as a bag-of-words using Term Frequency and Document Frequency; (2) perform automatic topic modeling using LDA (Latent Dirichlet Allocation) and enrichment of terms with a pre-trained dense language representation (embedding) called BERT (Bidirectional Encoder Representations from Transformers); (3) formulate and evaluate the search string using the obtained terms; and (4) use the developed search strings in a digital library. For the validation of our approach, we conduct an experiment—using some SS as objects—comparing the effectiveness of automatically formulated search strings by SeSG with manual search strings reported in these studies. SeSG generates search strings that achieve a better final F1-Score on the start set than the searches reported by these SS. Our study shows that SeSG can effectively supersede the formulation of search strings, in hybrid search strategies, since it dismisses the manual string refinements.},
   author = {Leonardo Fuchs Alves and Francisco J S Vasconcellos and Bruno Magalhães Nogueira},
   doi = {10.1007/s10664-021-10084-4},
   issn = {1573-7616},
   issue = {5},
   journal = {Empirical Software Engineering},
   pages = {105},
   title = {SeSG: a search string generator for Secondary Studies with hybrid search strategies using text mining},
   volume = {27},
   url = {https://doi.org/10.1007/s10664-021-10084-4},
   year = {2022}
}
@article{Rwemalika2022,
   abstract = {Test smells are known as bad development practices that reflect poor design and implementation choices in software tests. Over the last decade, there are few attempts to study test smells in the context of system tests that interact with the System Under Test through a Graphical User Interface. To fill the gap, we conduct an exploratory analysis of test smells occurring in System User Interactive Tests (SUIT). We thus, compose a catalog of 35 SUIT-specific smells, identified through a multi-vocal literature review, and show how they differ from smells encountered in unit tests. We also conduct an empirical analysis to assess the diffuseness and removal of these smells in 48 industrial repositories and 12 open-source projects. Our results show that the same type of smells tends to appear in both industrial and open-source projects, but they are not addressed in the same way. We also find that smells originating from a combination of multiple code locations appear more often than those that are localized on a single line. This happens because of the difficulty to observe non-local smells without tool support. Furthermore, we find that smell-removing actions are not frequent with less than 50\% of the affected tests ever undergoing a smell removal. Interestingly, while smell-removing actions are rare, some smells disappear while discarding tests, i.e., these smells do not appear in follow-up tests that replace the discarded ones.},
   author = {Renaud Rwemalika and Sarra Habchi and Mike Papadakis and Yves Le Traon and Marie-Claude Brasseur},
   doi = {10.1007/s10664-022-10251-1},
   issn = {1573-7616},
   issue = {1},
   journal = {Empirical Software Engineering},
   pages = {20},
   title = {Smells in system user interactive tests},
   volume = {28},
   url = {https://doi.org/10.1007/s10664-022-10251-1},
   year = {2022}
}
@article{Ciurumelea2023,
   abstract = {Source code comments are a cornerstone of software documentation facilitating feature development and maintenance. Well-defined documentation formats, like Javadoc, make it easy to include structural metadata used to, for example, generate documentation manuals. However, the actual usage of structural elements in source code comments has not been studied yet. We investigate to which extent these structural elements are used in practice and whether the added information can be leveraged to improve tools assisting developers when writing comments. Existing research on comment generation traditionally focuses on automatic generation of summaries. However, recent works have shown promising results when supporting comment authoring through a next-word prediction. In this paper, we present an in-depth analysis of commenting practice in more than 18K open-source projects written in Python and Java showing that many structural elements, particularly parameter and return value descriptions are indeed widely used. We discover that while a majority are rather short at about 6 to 9 words, many are several hundred words in length. We further find that Python comments tend to be significantly longer than Java comments, possibly due to the weakly-typed nature of the former. Following the empirical analysis, we extend an existing language model with support for structural information, substantially improving the Top-1 accuracy of predicted words (Python 9.6\%, Java 7.8\%).},
   author = {Adelina Ciurumelea and Carol V Alexandru and Harald C Gall and Sebastian Proksch},
   doi = {10.1007/s10664-022-10284-6},
   issn = {1573-7616},
   issue = {4},
   journal = {Empirical Software Engineering},
   pages = {86},
   title = {Completing Function Documentation Comments Using Structural Information},
   volume = {28},
   url = {https://doi.org/10.1007/s10664-022-10284-6},
   year = {2023}
}
@article{Chen2023,
   abstract = {The emerging service mesh architecture tries to simplify microservices by delegating crucial tasks to dedicated infrastructure. However, service mesh introduces new notions and enables complex capabilities such as sidecar proxies that inevitably bring major adoption concerns.},
   author = {Yihao Chen and Eduardo Fernandes and Bram Adams and Ahmed E Hassan},
   doi = {10.1007/s10664-023-10348-1},
   issn = {1573-7616},
   issue = {5},
   journal = {Empirical Software Engineering},
   pages = {113},
   title = {On practitioners’ concerns when adopting service mesh frameworks},
   volume = {28},
   url = {https://doi.org/10.1007/s10664-023-10348-1},
   year = {2023}
}
@article{Yang2023,
   abstract = {Due to the development of pre-trained language models, automated code generation techniques have shown great promise in recent years. However, the generated code will not always adhere to syntactic constraints of the target language, especially in the case of Turducken-style code, where declarative code snippets are embedded within imperative programs. In this study, we summarize three significant challenges in regards to syntactic constraints: (1) the efficient representation of syntactic constraints, (2) the effective integration of syntactic information, and (3) the scalable syntax-first decoding algorithm. To address these challenges, we propose a syntax-guided multi-task learning approach TurduckenGen. Specifically, we first explicitly append the type information to the code tokens to capture the representation of syntactic constraints. Then we formalize code generation with syntactic constraint representation as an auxiliary task to enable the model to learn the syntactic constraints of the code. Finally, the syntactically correct code is selected accurately from the multiple candidates with the help of the compiler feedback. Extensive experiments and comprehensive analysis demonstrate the effectiveness and general applicability of our approach after being compared with six state-of-the-art baselines on two Turducken-style code datasets. Finally, we conducted a human study and found the code quality generated by our approach is better than baselines in terms of code readability and semantic similarity.},
   author = {Guang Yang and Yu Zhou and Xiang Chen and Xiangyu Zhang and Yiran Xu and Tingting Han and Taolue Chen},
   doi = {10.1007/s10664-023-10372-1},
   issn = {1573-7616},
   issue = {6},
   journal = {Empirical Software Engineering},
   pages = {141},
   title = {A syntax-guided multi-task learning approach for Turducken-style code generation},
   volume = {28},
   url = {https://doi.org/10.1007/s10664-023-10372-1},
   year = {2023}
}
@article{Winkler2024,
   abstract = {The readability of source code is key for understanding and maintaining software systems and tests. Although several studies investigate the readability of source code, there is limited research specifically on the readability of test code and related influence factors.},
   author = {Dietmar Winkler and Pirmin Urbanke and Rudolf Ramler},
   doi = {10.1007/s10664-023-10390-z},
   issn = {1573-7616},
   issue = {2},
   journal = {Empirical Software Engineering},
   pages = {53},
   title = {Investigating the readability of test code},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-023-10390-z},
   year = {2024}
}
@article{Khalili2024,
   abstract = {Reusing test cases across apps that share similar functionalities reduces both the effort required to produce useful test cases and the time to offer reliable apps to the market. The main approaches to reuse test cases across apps combine different semantic matching and test generation algorithms to migrate test cases across Android apps. In this paper we define a general framework to evaluate the impact and effectiveness of different choices of semantic matching with Test Reuse approaches on migrating test cases across Android apps. We offer a thorough comparative evaluation of the many possible choices for the components of test migration processes. We propose an approach that combines the most effective choices for each component of the test migration process to obtain an effective approach. We report the results of an experimental evaluation on 8,099 GUI events from 337 test configurations. The results attest the prominent impact of semantic matching on test reuse. They indicate that sentence level perform better than word level embedding techniques. They surprisingly suggest a negligible impact of the corpus of documents used for building the word embedding model for the Semantic Matching Algorithm. They provide evidence that semantic matching of events of selected types perform better than semantic matching of events of all types. They show that the effectiveness of overall Test Reuse approach depends on the characteristics of the test suites and apps. The replication package that we make publicly available online (https://star.inf.usi.ch/#/software-data/11) allows researchers and practitioners to refine the results with additional experiments and evaluate other choices for test reuse components.},
   author = {Farideh Khalili and Leonardo Mariani and Ali Mohebbi and Mauro Pezzè and Valerio Terragni},
   doi = {10.1007/s10664-023-10406-8},
   issn = {1573-7616},
   issue = {3},
   journal = {Empirical Software Engineering},
   pages = {70},
   title = {Semantic matching in GUI test reuse},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-023-10406-8},
   year = {2024}
}
@article{Sas2023,
   abstract = {One of the most time-consuming tasks for developers is the comprehension of new code bases. An effective approach to aid this process is to label source code files with meaningful annotations, which can help developers understand the content and functionality of a code base quicker. However, most existing solutions for code annotation focus on project-level classification: manually labelling individual files is time-consuming, error-prone and hard to scale.},
   author = {Cezar Sas and Andrea Capiluppi},
   doi = {10.1007/s10664-023-10423-7},
   issn = {1573-7616},
   issue = {1},
   journal = {Empirical Software Engineering},
   pages = {12},
   title = {Multi-granular software annotation using file-level weak labelling},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-023-10423-7},
   year = {2023}
}
@article{Jabrayilzade2024,
   abstract = {Code comments play a vital role in source code comprehension and software maintainability. It is common for developers to write comments to explain a code snippet, and commenting code is generally considered a good practice in software engineering. However, low-quality comments can have a detrimental effect on software quality or be ineffective for code understanding. This study aims to create a taxonomy of inline code comment smells and determine how frequently each smell type occurs in software projects. We conducted a multivocal literature review to define the initial taxonomy of inline comment smells. Afterward, we manually labeled 2447 inline comments from eight open-source projects where half of them were Java, and another half were Python projects. We created a taxonomy of 11 inline code comment smell types and found out that the smells exist in both Java and Python projects with varying degrees. Moreover, we conducted an online survey with 41 software practitioners to learn their opinions on these smells and their impact on code comprehension and software maintainability. The survey respondents generally agreed with the taxonomy; however, they reported that some smell types might have a positive effect on code comprehension in certain scenarios. We also opened pull requests and issues fixing the comment smells in the sampled projects, where we got a 27\% acceptance rate. We share our manually labeled dataset online and provide implications for software engineering practitioners, researchers, and educators.},
   author = {Elgun Jabrayilzade and Ayda Yurtoğlu and Eray Tüzün},
   doi = {10.1007/s10664-023-10425-5},
   issn = {1573-7616},
   issue = {3},
   journal = {Empirical Software Engineering},
   pages = {58},
   title = {Taxonomy of inline code comment smells},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-023-10425-5},
   year = {2024}
}
@article{Yang2024-4,
   abstract = {When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. In addition, our ablation studies also confirm the effectiveness of component settings (such as bi-modal information, prompt learning, hybrid prompts, and multi-task learning) of SOTitle+. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.},
   author = {Shaoyu Yang and Xiang Chen and Ke Liu and Guang Yang and Chi Yu},
   doi = {10.1007/s10664-024-10466-4},
   issn = {1573-7616},
   issue = {3},
   journal = {Empirical Software Engineering},
   pages = {63},
   title = {Automatic bi-modal question title generation for Stack Overflow with prompt learning},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-024-10466-4},
   year = {2024}
}
@article{Zhao2024,
   abstract = {In machine learning (ML) applications, assets include not only the ML models themselves, but also the datasets, algorithms, and deployment tools that are essential in the development, training, and implementation of these models. Efficient management of ML assets is critical to ensure optimal resource utilization, consistent model performance, and a streamlined ML development lifecycle. This practice contributes to faster iterations, adaptability, reduced time from model development to deployment, and the delivery of reliable and timely outputs.},
   author = {Zhimin Zhao and Yihao Chen and Abdul Ali Bangash and Bram Adams and Ahmed E. Hassan},
   doi = {10.1007/s10664-024-10474-4},
   issn = {1573-7616},
   issue = {4},
   journal = {Empirical Software Engineering},
   pages = {98},
   title = {An empirical study of challenges in machine learning asset management},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-024-10474-4},
   year = {2024}
}
@article{Rani2024,
   abstract = {Researchers testing hypotheses related to factors leading to low-quality software often rely on historical data, specifically on details regarding when defects were introduced into a codebase of interest. The prevailing techniques to determine the introduction of defects revolve around variants of the SZZ algorithm. This algorithm leverages information on the lines modified during a bug-fixing commit and finds when these lines were last modified, thereby identifying bug-introducing commits.},
   author = {Pooja Rani and Fernando Petrulio and Alberto Bacchelli},
   doi = {10.1007/s10664-024-10511-2},
   issn = {1573-7616},
   issue = {5},
   journal = {Empirical Software Engineering},
   pages = {115},
   title = {On Refining the SZZ Algorithm with Bug Discussion Data},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-024-10511-2},
   year = {2024}
}
@article{Chen2024-10,
   abstract = {Title quality is important for different software engineering communities. For example, in Stack Overflow, posts with low-quality question titles often discourage potential answerers. In GitHub, issues with low-quality titles can make it difficult for developers to grasp the core idea of the problem. In previous studies, researchers mainly focused on generating titles from scratch by analyzing the body contents, such as the post body for Stack Overflow question title generation (SOTG) and the issue body for issue title generation (ISTG). However, the quality of the generated titles is still limited by the information available in the body contents. A more effective way is to provide accurate completion suggestions when developers compose titles. Inspired by this idea, we are the first to study the problem of automatic title completion for software engineering title generation tasks and propose the approach TC4SETG. Specifically, we first preprocess the gathered titles to form incomplete titles (i.e., tip information provided by developers) for simulating the title completion scene. Then we construct the input by concatenating the incomplete title with the body’s content. Finally, we fine-tune the pre-trained model CodeT5 to learn the title completion patterns effectively. To evaluate the effectiveness of TC4SETG, we selected 189,655 high-quality posts from Stack Overflow by covering eight popular programming languages for the SOTG task and 333,563 issues in the top-200 starred repositories on GitHub for the ISTG task. Our empirical results show that compared with the approaches of generating question titles from scratch, our proposed approach TC4SETG is more practical in automatic and human evaluation. Our experimental results demonstrate that TC4SETG outperforms corresponding state-of-the-art baselines in the SOTG task by a minimum of 25.82\% and in the ISTG task by at least 45.48\% in terms of ROUGE-L. Therefore, our study provides a new direction for studying automatic software engineering title generation and calls for more researchers to investigate this direction in the future.},
   author = {Xiang Chen and Wenlong Pei and Shaoyu Yang and Yanlin Zhou and Zichen Zhang and Jiahua Pei},
   doi = {10.1007/s10664-024-10513-0},
   issn = {1573-7616},
   issue = {5},
   journal = {Empirical Software Engineering},
   pages = {120},
   title = {Automatic title completion for Stack Overflow posts and GitHub issues},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-024-10513-0},
   year = {2024}
}
@article{Agh2024,
   abstract = {A Software Product Line (SPL) is a software development paradigm in which a family of software products shares a set of core assets. Testing has a vital role in both single-system development and SPL development in identifying potential faults by examining the behavior of a product or products, but it is especially challenging in SPL. There have been many research contributions in the SPL testing field; therefore, assessing the current state of research and practice is necessary to understand the progress in testing practices and to identify the gap between required techniques and existing approaches. This paper aims to survey existing research on SPL testing to provide researchers and practitioners with up-to-date evidence and issues that enable further development of the field. To this end, we conducted a Systematic Literature Review (SLR) with seven research questions in which we identified and analyzed 118 studies dating from 2003 to 2022. The results indicate that the literature proposes many techniques for specific aspects (e.g., controlling cost/effort in SPL testing); however, other elements (e.g., regression testing and non-functional testing) still need to be covered by existing research. Furthermore, most approaches are evaluated by only one empirical method, most of which are academic evaluations. This may jeopardize the adoption of approaches in industry. The results of this study can help identify gaps in SPL testing since specific points of SPL Engineering still need to be addressed entirely.},
   author = {Halimeh Agh and Aidin Azamnouri and Stefan Wagner},
   doi = {10.1007/s10664-024-10516-x},
   issn = {1573-7616},
   issue = {6},
   journal = {Empirical Software Engineering},
   pages = {146},
   title = {Software product line testing: a systematic literature review},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-024-10516-x},
   year = {2024}
}
@article{Hao2024,
   abstract = {ChatGPT has significantly impacted software development practices, providing substantial assistance to developers in various tasks, including coding, testing, and debugging. Despite its widespread adoption, the impact of ChatGPT as an assistant in collaborative coding remains largely unexplored. In this paper, we analyze a dataset of 210 and 370 developers’ shared conversations with ChatGPT in GitHub pull requests (PRs) and issues. We manually examined the content of the conversations and characterized the dynamics of the sharing behavior, i.e., understanding the rationale behind the sharing, identifying the locations where the conversations were shared, and determining the roles of the developers who shared them. Our main observations are: (1) Developers seek ChatGPT’s assistance across 16 types of software engineering inquiries. In both conversations shared in PRs and issues, the most frequently encountered inquiry categories include code generation, conceptual questions, how-to guides, issue resolution, and code review. (2) Developers frequently engage with ChatGPT via multi-turn conversations where each prompt can fulfill various roles, such as unveiling initial or new tasks, iterative follow-up, and prompt refinement. Multi-turn conversations account for 33.2\% of the conversations shared in PRs and 36.9\% in issues. (3) In collaborative coding, developers leverage shared conversations with ChatGPT to facilitate their role-specific contributions, whether as authors of PRs or issues, code reviewers, or collaborators on issues. Our work serves as the first step towards understanding the dynamics between developers and ChatGPT in collaborative software development and opens up new directions for future research on the topic.},
   author = {Huizi Hao and Kazi Amit Hasan and Hong Qin and Marcos Macedo and Yuan Tian and Steven H H Ding and Ahmed E Hassan},
   doi = {10.1007/s10664-024-10540-x},
   issn = {1573-7616},
   issue = {6},
   journal = {Empirical Software Engineering},
   pages = {150},
   title = {An empirical study on developers’ shared conversations with ChatGPT in GitHub pull requests and issues},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-024-10540-x},
   year = {2024}
}
@article{He2024-2,
   abstract = {Stack Overflow is one of the most influential Software Question and Answer (SQA) websites, hosting millions of programming-related questions and answers. Tags play a critical role in efficiently organizing the contents on Stack Overflow and are vital to support various site operations, such as querying relevant content. Poorly chosen tags often lead to issues such as tag ambiguity and tag explosion. Therefore, a precise and accurate automated tag recommendation technique is needed. Inspired by the recent success of pre-trained models (PTMs) in natural language processing (NLP), we present PTM4Tag+, a tag recommendation framework for Stack Overflow posts that utilize PTMs in language modeling. PTM4Tag+ is implemented with a triplet architecture, which considers three key components of a post, i.e., Title, Description, and Code, with independent PTMs. We utilize a number of popular pre-trained models, including BERT-based models (e.g., BERT, RoBERTa, CodeBERT, BERTOverflow, and ALBERT), and encoder-decoder models (e.g., PLBART, CoTexT, and CodeT5). Our results show that leveraging CodeT5 under the PTM4Tag+ framework achieves the best performance among the eight considered PTMs and outperforms the state-of-the-art Convolutional Neural Network-based approach by a substantial margin in terms of average Precision@k, Recall@k, and F1-score@k (k ranges from 1 to 5). Specifically, CodeT5 improves the performance of F1-score@1-5 by 8.8\%, 12.4\%, 15.3\%, 16.4\%, and 16.6\%, respectively. Moreover, to address the concern with inference latency, we experimented PTM4Tag+ using smaller PTM models (i.e., DistilBERT, DistilRoBERTa, CodeBERT-small, and CodeT5-small). We find that although smaller PTMs cannot outperform larger PTMs, they still maintain over 93.96\% of the performance on average while reducing the mean inference time by more than 47.2\%.},
   author = {Junda He and Bowen Xu and Zhou Yang and DongGyun Han and Chengran Yang and Jiakun Liu and Zhipeng Zhao and David Lo},
   doi = {10.1007/s10664-024-10576-z},
   issn = {1573-7616},
   issue = {1},
   journal = {Empirical Software Engineering},
   pages = {28},
   title = {PTM4Tag+: Tag recommendation of stack overflow posts with pre-trained models},
   volume = {30},
   url = {https://doi.org/10.1007/s10664-024-10576-z},
   year = {2024}
}
@article{Zhang2024,
   abstract = {Code summarization plays a pivotal role in the field of software engineering by offering developers a concise natural language comprehension of source code semantics. As software complexity continues to escalate, code summarization confronts various challenges, including discrepancies between source code and summarization, the absence of crucial or up-to-date information, and the inefficiency and resource demands of manual summarization. To address these challenges, Automatic Source Code Summarization (ASCS) has garnered widespread attention. This paper presents a comprehensive review and synthesis of ASCS research. It aims to provide an in-depth understanding of the core issues and challenges inherent in each phase of ASCS, illustrated with specific examples and application scenarios. Around of the core phases of ASCS including data collection, source code modeling, the generation of code summaries, and the assessment of their quality, the paper thoroughly compiles and assesses existing datasets, categorizes and examines prevalent source code modeling techniques, and delves into the methods for generating and evaluating the quality of code summaries. Concluding with an exploration of future research avenues and emerging trends, this paper serves as a guide for readers to grasp the cutting-edge developments in this field, enriched by the analysis of pivotal research contributions.},
   author = {Xuejun Zhang and Xia Hou and Xiuming Qiao and Wenfeng Song},
   doi = {10.1007/s10664-024-10553-6},
   issn = {1573-7616},
   issue = {6},
   journal = {Empirical Software Engineering},
   pages = {162},
   title = {A review of automatic source code summarization},
   volume = {29},
   url = {https://doi.org/10.1007/s10664-024-10553-6},
   year = {2024}
}
@article{Caddy2024,
   abstract = {Software development creates and relies on a large volume of information, yet the volume of this information can make it challenging for developers to maintain an overview of all goings-on that a team and external actors contribute to a project. We posit that unexpected or “surprising” events could serve as important signposts amidst this information overload. These unexpected events may indicate underlying anomalies or emergent situations that require immediate attention. To explore this premise, our study leverages the concept of ‘surprisal’ from information theory to identify and quantify these unusual occurrences from the issues and pull requests of popular open-source software repositories.},
   author = {James Caddy and Christoph Treude and Markus Wagner and Earl T Barr},
   doi = {10.1007/s10664-024-10587-w},
   issn = {1573-7616},
   issue = {1},
   journal = {Empirical Software Engineering},
   pages = {30},
   title = {The role of surprisal in issue trackers},
   volume = {30},
   url = {https://doi.org/10.1007/s10664-024-10587-w},
   year = {2024}
}
@article{Hemberg2024,
   abstract = {Algorithms that use Large Language Models (LLMs) to evolve code arrived on the Genetic Programming (GP) scene very recently. We present LLM_GP, a general LLM-based evolutionary algorithm designed to evolve code. Like GP, it uses evolutionary operators, but its designs and implementations of those operators significantly differ from GP’s because they enlist an LLM, using prompting and the LLM’s pre-trained pattern matching and sequence completion capability. We also present a demonstration-level variant of LLM_GP and share its code. By presentations that range from formal to hands-on, we cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming.},
   author = {Erik Hemberg and Stephen Moskal and Una-May O’Reilly},
   doi = {10.1007/s10710-024-09494-2},
   issn = {1573-7632},
   issue = {2},
   journal = {Genetic Programming and Evolvable Machines},
   pages = {21},
   title = {Evolving code with a large language model},
   volume = {25},
   url = {https://doi.org/10.1007/s10710-024-09494-2},
   year = {2024}
}
@article{Harel2024,
   abstract = {Multi-core shared memory architectures have become ubiquitous in computing hardware nowadays. As a result, there is a growing need to fully utilize these architectures by introducing appropriate parallelization schemes, such as OpenMP worksharing-loop constructs, to applications. However, most developers find introducing OpenMP directives to their code hard due to pervasive pitfalls in managing parallel shared memory. To assist developers in this process, many compilers, as well as source-to-source (S2S) translation tools, have been developed over the years, tasked with inserting OpenMP directives into code automatically. In addition to having limited robustness to their input format, these compilers still do not achieve satisfactory coverage and precision in locating parallelizable code and generating appropriate directives. Recently, many data-driven AI-based code completion (CC) tools, such as GitHub CoPilot, have been developed to ease and improve programming productivity. Leveraging the insights from existing AI-based programming-assistance tools, this work presents a novel AI model that can serve as a parallel-programming assistant. Specifically, our model, named PragFormer, is tasked with identifying for loops that can benefit from conversion to parallel worksharing-loop construct (OpenMP directive) and even predict the need for specific data-sharing attributes clauses on the fly. We created a unique database, named Open-OMP, specifically for this goal. Open-OMP contains over 32,000 unique code snippets from different domains, half of which contain OpenMP directives, while the other half do not. We experimented with different model design parameters for these tasks and showed that our best-performing model outperforms a statistically-trained baseline as well as a state-of-the-art S2S compiler. In fact, it even outperforms the popular generative AI model of ChatGPT. In the spirit of advancing research on this topic, we have already released source code for PragFormer as well as Open-OMP dataset to public. Moreover, an interactive demo of our tool, as well as a Hugging Face webpage to experiment with our tool, are already available.},
   author = {Re’em Harel and Tal Kadosh and Niranjan Hasabnis and Timothy Mattson and Yuval Pinter and Gal Oren},
   doi = {10.1007/s10766-024-00778-9},
   issn = {1573-7640},
   issue = {1},
   journal = {International Journal of Parallel Programming},
   pages = {2},
   title = {PragFormer: Data-Driven Parallel Source Code Classification with Transformers},
   volume = {53},
   url = {https://doi.org/10.1007/s10766-024-00778-9},
   year = {2024}
}
@article{Czech2020,
   abstract = {Model-driven software development comes in different styles. While standard-based approaches leverage existing language standards (e.g., UML), tooling, and development processes, domain-specific modeling (DSM) requires languages and tool support to be created prior to the actual software development. The design, implementation, and testing of languages and tool support require a wide spectrum of methods and techniques which introduce complexity and new challenges. To tackle these DSM-specific challenges, best practices have been collected from various application domains and published in literature to guide the development and application of DSM solutions. It is the goal of this paper to identify studies reporting best practices on domain-specific modeling. Moreover, a systematic and comprehensive compilation of best practices should act as a starting point to identify literature that facilitates industrial adoption of DSM. To search for literature and classify identified studies, we conduct a systematic mapping study (SMS). Furthermore, we perform an in-depth analysis of the identified studies to answer how practices overlap, complement, or contradict each other. The systematic search resulted in 21 studies reporting 321 best practices. From these 321 practices, we compiled 192 unique best practices. We found that the DSM community created a substantial corpus of best practices for DSM. The large majority of practices (75\%) are only reported once. The top best practices by number of reports contain only practices that are reported at least four times. However, the frequency of reports does not necessarily imply a high importance of practice, as the application of practices is always context specific.},
   author = {Gerald Czech and Michael Moser and Josef Pichler},
   doi = {10.1007/s11219-019-09466-1},
   issn = {1573-1367},
   issue = {2},
   journal = {Software Quality Journal},
   pages = {663-692},
   title = {A systematic mapping study on best practices for domain-specific modeling},
   volume = {28},
   url = {https://doi.org/10.1007/s11219-019-09466-1},
   year = {2020}
}
@article{Du2024,
   abstract = {Deep learning frameworks serve as the cornerstone for constructing robust deep learning systems. However, bugs within these frameworks can have severe consequences, negatively affecting various applications. Accurately classifying and understanding these bugs is essential to ensure framework reliability. By doing so, developers can proactively take appropriate measures to mitigate potential risks associated with specific bug types in both current and future software releases. Despite the significance of bug report classification, existing methods fall short in terms of performance, rendering them impractical for real-world applications. To address this limitation, we propose a bug report classification framework for deep learning frameworks, called LLM–BRC, leveraging OpenAI’s latest embedding model, text-embedding-ada-002. Our LLM–BRC framework achieves an impressive accuracy range of 92\% to 98.75\% in bug report classification for three deep learning frameworks: TensorFlow, MXNET, and PaddlePaddle. This represents a substantial improvement of 17.21\% to 69.15\% compared to existing methods. Furthermore, we conduct a comprehensive investigation into the impact of different bug report components and different models.},
   author = {Xiaoting Du and Zhihao Liu and Chenglong Li and Xiangyue Ma and Yingzhuo Li and Xinyu Wang},
   doi = {10.1007/s11219-024-09675-3},
   issn = {1573-1367},
   issue = {3},
   journal = {Software Quality Journal},
   pages = {985-1005},
   title = {LLM-BRC: A large language model-based bug report classification framework},
   volume = {32},
   url = {https://doi.org/10.1007/s11219-024-09675-3},
   year = {2024}
}
@article{Meng2024,
   abstract = {Bug reports play an important role in the software development and maintenance process. As the eye of a bug report, a concise and fluent title is always preferred and expected by developers as it could help them quickly seize the problem point and make better decisions in handling the bugs. However, in practice, not all titles filled by bug reporters are found to be of high quality; some may not carry essential bug-related information, and some may be hard to understand or contain extra noise. With the aim to reduce the burden of bug reporters and ease developers’ life in handling bugs, we propose a deep learning-based technique named KeyTitle, to automatically generate a title for a given bug report. KeyTitle formulates the title generation problem as a one-sentence summarization task. It could be viewed as a Seq2Seq generation model (which generally directly generates target text based on source text) that incorporates keywords planning. Specifically, within KeyTitle, a transformer-based encoder-decoder model is enforced to generate a chain of keywords first from the detailed textual problem description, and then generate the target title by considering both these keywords and description content. the titles generated by KeyTitle are also found to be better in terms of Relevance, Accuracy, Conciseness, Fluency in human evaluation. Besides generating titles from textual descriptions, KeyTitle is also found to have great potential in generating titles based on just a few keywords, a task that also has much value in bug reporting/handling practice.},
   author = {Qianshuang Meng and Weiqin Zou and Biyu Cai and Jingxuan Zhang},
   doi = {10.1007/s11219-024-09695-z},
   issn = {1573-1367},
   issue = {4},
   journal = {Software Quality Journal},
   pages = {1655-1682},
   title = {KeyTitle: towards better bug report title generation by keywords planning},
   volume = {32},
   url = {https://doi.org/10.1007/s11219-024-09695-z},
   year = {2024}
}
@article{Talib2021,
   abstract = {Artificial intelligence (AI) and machine learning (ML) tools play a significant role in the recent evolution of smart systems. AI solutions are pushing towards a significant shift in many fields such as healthcare, autonomous airplanes and vehicles, security, marketing customer profiling and other diverse areas. One of the main challenges hindering the AI potential is the demand for high-performance computation resources. Recently, hardware accelerators are developed in order to provide the needed computational power for the AI and ML tools. In the literature, hardware accelerators are built using FPGAs, GPUs and ASICs to accelerate computationally intensive tasks. These accelerators provide high-performance hardware while preserving the required accuracy. In this work, we present a systematic literature review that focuses on exploring the available hardware accelerators for the AI and ML tools. More than 169 different research papers published between the years 2009 and 2019 are studied and analysed.},
   author = {Manar Abu Talib and Sohaib Majzoub and Qassim Nasir and Dina Jamal},
   doi = {10.1007/s11227-020-03325-8},
   issn = {1573-0484},
   issue = {2},
   journal = {The Journal of Supercomputing},
   pages = {1897-1938},
   title = {A systematic literature review on hardware implementation of artificial intelligence algorithms},
   volume = {77},
   url = {https://doi.org/10.1007/s11227-020-03325-8},
   year = {2021}
}
@article{Carrin2024,
   abstract = {The metaverse is seen as a future generation of the Internet in which the virtual and the real merge into a common world. Technologies such as IoT, cloud computing, artificial intelligence or the reality–virtuality continuum underpin the metaverse and condition its evolution. This paper presents a bibliometric study on the WoS database (from 1995 to 2022) to obtain a comprehensive and non-subjective understanding of the metaverse. The study identifies the main subject areas and sources, the leading countries and authors. It also analyzes the evolution over time and the core and future research themes. Extended reality, blockchain, artificial intelligence and sensors are identified as core themes, while building information modeling, digital twins and governance emerge as future themes. Based on the bibliometric study, a general layered metaverse architecture is proposed that streamlines open challenges in the metaverse to assist researchers and companies in introducing innovative and disruptive improvements.},
   author = {Carmen Carrión},
   doi = {10.1007/s11227-023-05544-1},
   issn = {1573-0484},
   issue = {2},
   journal = {The Journal of Supercomputing},
   pages = {1598-1639},
   title = {Research streams and open challenges in the metaverse},
   volume = {80},
   url = {https://doi.org/10.1007/s11227-023-05544-1},
   year = {2024}
}
@article{Walha2024,
   abstract = {Data integration combines information from different sources to provide a comprehensive view for making informed business decisions. The ETL (Extract, Transform, and Load) process is essential in data integration. In the past two decades, modeling the ETL process has become a priority for effectively managing information. This paper aims to explore ETL approaches to help researchers and organizational stakeholders overcome challenges, especially in Big Data integration. It offers a comprehensive overview of ETL methods, from traditional to Big Data, and discusses their advantages, limitations, and the primary trends in Big Data integration. The study emphasizes that many technologies have been integrated into ETL steps for data collection, storage, processing, querying, and analysis without proper modeling. Therefore, more generic and customized design modeling of the ETL steps should be carried out to ensure reusability and flexibility. The paper summarizes the exploration of ETL modeling, focusing on Big Data scalability and processing trends. It also identifies critical dilemmas, such as ensuring compatibility across multiple sources and dealing with large volumes of Big Data. Furthermore, it suggests future directions in Big Data integration by leveraging advanced artificial intelligence processing and storage systems to ensure consistency, efficiency, and data integrity.},
   author = {Afef Walha and Faiza Ghozzi and Faiez Gargouri},
   doi = {10.1007/s11227-024-06413-1},
   issn = {1573-0484},
   issue = {19},
   journal = {The Journal of Supercomputing},
   pages = {26687-26725},
   title = {Data integration from traditional to big data: main features and comparisons of ETL approaches},
   volume = {80},
   url = {https://doi.org/10.1007/s11227-024-06413-1},
   year = {2024}
}
@article{Alraddadi2024,
   abstract = {Categorizing the reported software bugs into their types is a vital aspect of software development and maintenance. This procedure is initially handled manually by a bug triage. However, the classification approach should be automated to facilitate and improve the process. This research aims to enhance the predictive performance of machine learning models in classifying bug reports. The study proposes a novel framework for integrating chi-square for feature selection with stacked generalization ensemble-based models into the bug report classification process. The study involves an empirical investigation utilizing a set of seven base classifiers and three meta-classifiers (Logistic Regression (LoR), Naive Bayes (NB), and Multilayer Perceptron (MLP)) to construct the stacking ensemble. The models were trained on two open-source Java datasets using the textual data fields for the reported bug. Features were extracted using different variants of N-grams, including uni-grams, bi-grams, and tri-grams. The chi-square feature selection technique was applied to reduce the high dimensionality and select only the informative features. The experimental results were evaluated using the Matthews correlation coefficient and F1 metric and compared with state-of-the-art bug classification methods. The results show that the stacking models’ performance is comparatively higher than the standalone classifiers in almost all cases and for both datasets. Increasing the dataset size for all three stacked models improves the chances of achieving higher performance. The analytical comparison among the three stacking models and the statistical results using the Wilcoxon signed-rank test showed that MLP-Stacked and LoR-Stacked ensemble models were the best-performing classifiers among the other models.},
   author = {Rawan Alraddadi and Mohammad Alshayeb},
   doi = {10.1007/s11334-024-00584-z},
   issn = {1614-5054},
   journal = {Innovations in Systems and Software Engineering},
   title = {An empirical evaluation of stacked generalization models for binary bug report classification},
   url = {https://doi.org/10.1007/s11334-024-00584-z},
   year = {2024}
}
@article{Ding2023-1,
   abstract = {Identifying semantic types for attributes in relations, known as attribute semantic type (AST) identification, plays an important role in many data analysis tasks, such as data cleaning, schema matching, and keyword search in databases. However, due to a lack of unified naming standards across prevalent information systems (a.k.a. information islands), AST identification still remains as an open problem. To tackle this problem, we propose a context-aware method to figure out the ASTs for relations in this paper. We transform the AST identification into a multi-class classification problem and propose a schema context aware (SCA) model to learn the representation from a collection of relations associated with attribute values and schema context. Based on the learned representation, we predict the AST for a given attribute from an underlying relation, wherein the predicted AST is mapped to one of the labeled ASTs. To improve the performance for AST identification, especially for the case that the predicted semantic types of attributes are not included in the labeled ASTs, we then introduce knowledge base embeddings (a.k.a. KBVec) to enhance the above representation and construct a schema context aware model with knowledge base enhanced (SCA-KB) to get a stable and robust model. Extensive experiments based on real datasets demonstrate that our context-aware method outperforms the state-of-the-art approaches by a large margin, up to 6.14\% and 25.17\% in terms of macro average F1 score, and up to 0.28\% and 9.56\% in terms of weighted F1 score over high-quality and low-quality datasets respectively.},
   author = {Yue Ding and Yu-He Guo and Wei Lu and Hai-Xiang Li and Mei-Hui Zhang and Hui Li and An-Qun Pan and Xiao-Yong Du},
   doi = {10.1007/s11390-021-1048-y},
   issn = {1860-4749},
   issue = {4},
   journal = {Journal of Computer Science and Technology},
   pages = {927-946},
   title = {Context-Aware Semantic Type Identification for Relational Attributes},
   volume = {38},
   url = {https://doi.org/10.1007/s11390-021-1048-y},
   year = {2023}
}
@article{Xu2021,
   abstract = {Survey generation aims to generate a summary from a scientific topic based on related papers. The structure of papers deeply influences the generative process of survey, especially the relationships between sentence and sentence, paragraph and paragraph. In principle, the structure of paper can influence the quality of the summary. Therefore, we employ the structure of paper to leverage contextual information among sentences in paragraphs to generate a survey for documents. In particular, we present a neural document structure model for survey generation. We take paragraphs as units, and model sentences in paragraphs, we then employ a hierarchical model to learn structure among sentences, which can be used to select important and informative sentences to generate survey. We evaluate our model on scientific document data set. The experimental results show that our model is effective, and the generated survey is informative and readable.},
   author = {Huiyan Xu and Zhongqing Wang and Yifei Zhang and Xiaolan Weng and Zhijian Wang and Guodong Zhou},
   doi = {10.1007/s11704-020-9366-8},
   issn = {2095-2236},
   issue = {4},
   journal = {Frontiers of Computer Science},
   pages = {154325},
   title = {Document structure model for survey generation using neural network},
   volume = {15},
   url = {https://doi.org/10.1007/s11704-020-9366-8},
   year = {2021}
}

@article{Zhang2024,
   abstract = {Post recommendations refer to finding solutions related to a user’s problem on QA websites to help them solve their problems. However, finding the most relevant post from a large number of posts related to a problem is a challenging task. This paper proposes a novel recommendation model called FuEPRe, which based on a multi-headed self-attention network integrates semantic information, structural information of code and description information. It accurately recommends relevant Stack Overflow posts based on users’ queries, thereby helping them solve problems quickly and solving the problem of inaccurate post recommendations in the past. Each pair of codes and descriptions is represented as two vectors, and then, the three different types of information are fused into these two vectors through an attention mechanism. At this point, each vector contains the above three types of information and then recommends posts by comparing the similarity between the vectors. The proposed approach is evaluated on the Stack Overflow Posts dataset, and the results demonstrate that it outperforms some state-of-the-art methods in the post recommendation task. Specifically, the approach improves the recall, MRR, and NDCG of recommendations, enabling programmers to solve problems faster.},
   author = {Xinbo Zhang and Guohua Shen and Zhiqiu Huang and Yaoshen Yu and Kang Wang},
   doi = {10.1007/s11761-024-00386-y},
   issn = {1863-2394},
   issue = {1},
   journal = {Service Oriented Computing and Applications},
   pages = {67-79},
   title = {FuEPRe: a fusing embedding method with attention for post recommendation},
   volume = {18},
   url = {https://doi.org/10.1007/s11761-024-00386-y},
   year = {2024}
}
@article{Zulqarnain2024,
   abstract = {Sentiment analysis is a particularly common task for determining user thoughts and has been widely used in Natural Language Processing (NLP) applications. Gated Recurrent Unit (GRU) was already effectively integrated into the NLP process with comparatively excellent results. GRU networks outperform traditional recurrent neural networks in sequential learning tasks and solve gradient vanishing and explosion limitations of RNNs. This paper introduces a new method called Normalize Auto-Encoded GRU (NAE-GRU) to address data dimensionality reduction using an Auto-Encoder and to improve performance through batch normalization. Empirically, we demonstrate that with slight adjustments to hyperparameters and optimization of statistic vectors, the proposed model achieves excellent results in sentiment classification on benchmark datasets. The developed NAE-GRU approach outperformed other various traditional approaches in terms of accuracy and convergence rate. The experimental results showed that the developed NAE-GRU approach accomplished better sentiment analysis accuracy of 91.32\%, 82.27\%, 87.43\%, and 84.49\% on IMDB, SSTb, Amazon review, and Yelp review datasets respectively. Furthermore, experimental results have shown that the developed approach is proficient in reducing the loss function and capturing long-term relationships with an effective design that achieved excellent results compared to state-of-the-art methods.},
   author = {Muhammad Zulqarnain and Ahmed Khalaf Zager Alsaedi and Rubab Sheikh and Irfan Javid and Maqsood Ahmad and Ubaid Ullah},
   doi = {10.1007/s41870-023-01600-4},
   issn = {2511-2112},
   issue = {1},
   journal = {International Journal of Information Technology},
   pages = {587-599},
   title = {An improved gated recurrent unit based on auto encoder for sentiment analysis},
   volume = {16},
   url = {https://doi.org/10.1007/s41870-023-01600-4},
   year = {2024}
}
@article{Mandava2024,
   abstract = {Multiword term extraction refers to identifying and extracting meaningful phrases or expressions of multiple words from a given text or a corpus. These multiword terms often represent specific concepts, entities, or domain-specific terminology. Multiword terms often include compound words, combining multiple words to form a single term. Recognizing and extracting compound terms accurately can be challenging, especially when dealing with languages that have flexible word composition rules. To overcome the problems in multiword term extraction, propose a novel deep learning and attention-based technique. There are four steps to evaluate the process. First, collect the data from the datasets, clean the data, and then perform lowercasing, tokenization, stop word removal, stemming, and lemmatization. After that, perform n-grams extraction and part-of-speech tags in the feature extraction step. Then, extract the multiword terms using the attention layer of the pre-trained BERT method. Finally, optimize the hyper-parameters in the pre-trained BERT method using the Enhanced Northern Goshawk Optimization (ENGO) algorithm. The term extraction frameworks used in this study can be presented via the attention-based models. Results indicate that compared to the baseline pre-trained BERT word extraction model, the pre-trained BERT with attention layer can reach a high term extraction accuracy of 98.57\%. This study also shows that attention-based models can find pertinent terms with strong associations to the categories found in clinical progress notes.},
   author = {Mamatha Mandava and Surendra Reddy Vinta},
   doi = {10.1007/s41870-024-01855-5},
   issn = {2511-2112},
   journal = {International Journal of Information Technology},
   title = {Optimized BERT: an effective attention layer based deep learning technique utilizing for multiword term extraction},
   url = {https://doi.org/10.1007/s41870-024-01855-5},
   year = {2024}
}
@article{Tharaniyasairaj2024,
   abstract = {The key to automatic question generation (AQG) is the selection of the tail entity (named entity in latter part of the question), which is vital concern for generating coherent and relevant questions. To address this requirement, advanced techniques such as positional encoding and dependency parsing are widely adopted. Positional encoding captures the sequential representation of words by analyzing context and structure, while dependency parsing identifies grammatical relationships to ensure linguistic coherence. Dependency parsing techniques such as transition-based and graph-based methods are widely employed to model source text sequences and generate accurate dependency relationships. However, the applicability of the existing dependency models to capture complex semantic relationships in the source text with subject or domain specific nouns is quite challenging. The evolution of AQG is recently approached by the application of Transformer models and end-to-end neural network architectures. To address this challenge, a hybrid model is proposed with TreeRNN and T5 Transformer to improve the AQG process. This model focuses on two main objectives: (1) training TreeRNN for dependency-aware tail entity extraction and (2) fine-tuning T5 Transformer with multi-shot tail entity prompting. Experimental results show that the proposed TreeRNN significantly enhances performance metrics such as Labelled Attachment Score and Unlabelled Attachment Score. Moreover, performance analysis of the proposed model for domain-specific question generation highlights the model's effectiveness, yielding substantial auto-generated questions.},
   author = {R Tharaniya sairaj and S R Balasundaram},
   doi = {10.1007/s41870-024-02205-1},
   issn = {2511-2112},
   issue = {8},
   journal = {International Journal of Information Technology},
   pages = {5407-5419},
   title = {Reducing tail entity hallucinations with dependency edge prediction in text to text transfer transformer based auto-generated questions},
   volume = {16},
   url = {https://doi.org/10.1007/s41870-024-02205-1},
   year = {2024}
}
@article{Uddin2024,
   abstract = {Bug triage is a critical aspect of software development that involves classifying and prioritizing reported bugs. Effective bug triage ensures that resources are allocated and bug patterns are efficiently identified to address underlying issues. However, inappropriate bug distributions among different types of developers cause delays, errors, incapability, and lower job satisfaction. Many works were proposed to recommend developers for fixing bugs. However, previous works did not consider the proper workload distributions among developers. The objective of the work is to introduce a task allocation and load-balancing model called Developer Scheduler (DevSched) that properly distributes unassigned bugs among different types of developers more efficiently. DevSched creates multiple developer profiles from existing bug reports. Different new bug reports are converted bug into vectors, and developer profiles are transformed into a corpus of words. Then, different bugs are assigned into developers by comparing the highest cosine similarity between vectors of bugs and developers’ corpus. DevSched dynamically updates the developer’s workload, and their ratings are adjusted based on the performance. In this work, Eclipse, Mozilla, and NetBeans bug reports are utilized to investigate individual developers’ performance in bug-triaging process. DevSched assigns and balances bugs among different kinds of developers more efficiently. After applying the proposed load-balancing model, the standard deviations are rapidly decreased than normal bug distributions for different datasets. This process is repeated for each bug to ensure the efficient allocation of resources and the resolution of critical issues. Therefore, the lowest standard deviation is found to be 0.48 for the Eclipse dataset. By assigning bugs based on defined thresholds, skills, and workloads, DevSched improves its efficiency, reduces delays, and enhances job satisfaction among developers.},
   author = {K M Aslam Uddin and Md. Shahriare Satu and Md. Mahmudul Hasan Riyad and Kazi Sakib},
   doi = {10.1007/s42044-023-00153-w},
   issn = {2520-8446},
   issue = {1},
   journal = {Iran Journal of Computer Science},
   pages = {1-11},
   title = {DevSched: an efficient bug-triaging model for allocating and balancing developer tasks},
   volume = {7},
   url = {https://doi.org/10.1007/s42044-023-00153-w},
   year = {2024}
}
@article{Alharthi2020,
   abstract = {We propose an automatic, low-cost, large-scale, nonintrusive human need recognition framework that utilized a multi-layered psychological-based reference model and designed with different modules including data collection, preprocessing, feature extraction and contextualization module. The reference model comprises several classification and regression models to identify human psychological needs, measure their satisfaction levels, evaluate their surrounding environment around different life aspects during any subjective event or towards emerging topics at any time, and in any location, using their publicly available social media content. We evaluate the predictive powers of various textual, psychological, semantic, lexicon-based and Twitter-specific features. To provide benchmark results, we compare and evaluate the performance of diverse machine learning algorithms. Our results confirm the effectiveness of the developed reference model. The framework is used to recognize citizen needs in response to the New Zealand terror attacks which occurred on March 15th, 2019.},
   author = {Rajwa Alharthi and Abdulmotaleb El Saddik},
   doi = {10.1007/s42979-020-00271-3},
   issn = {2661-8907},
   issue = {5},
   journal = {SN Computer Science},
   pages = {291},
   title = {A Multi-layered Psychological-Based Reference Model for Citizen Need Assessment Using AI-Powered Models},
   volume = {1},
   url = {https://doi.org/10.1007/s42979-020-00271-3},
   year = {2020}
}
@article{Vivaldi2021,
   abstract = {Even though many NLP resources and tools claim to be domain independent, their application to specific tasks is restricted to some specific domain, otherwise their performance degrade notably. As the accuracy of NLP resources drops heavily when applied in environments different from which they were built a tuning to the new environment is needed. This paper proposes a method for automatically compile terminologies from potentially any domain. The proposed method takes as reference the set of domains defined by Magnini, the Multilingual Central Repository (a resource based on WordNet 3.0) together with DBpedia, an open knowledge source that had proven to be reliable for restricted domains. Using the method described in this article, we have produced a big set of reliable terminologies for 164 domains and 2 languages totalling 635,527 terms. The proposed method has been applied to English and Spanish languages but it is potentially applicable to any language that has its own a DBpedia evolved enough. The obtained results have been intensively evaluated in several ways.},
   author = {Jorge Vivaldi and Horacio Rodríguez},
   doi = {10.1007/s42979-021-00952-7},
   issn = {2661-8907},
   issue = {1},
   journal = {SN Computer Science},
   pages = {76},
   title = {Automatically Producing Semantically Tagged Bilingual Terminologies},
   volume = {3},
   url = {https://doi.org/10.1007/s42979-021-00952-7},
   year = {2021}
}
@article{Zhang2022,
   abstract = {The Linked Open Data practice has led to a significant growth of structured data on the Web. While this has created an unprecedented opportunity for research in the field of Natural Language Processing, there is a lack of systematic studies on how such data can be used to support downstream NLP tasks. This work focuses on the e-commerce domain and explores how we can use such structured data to create language resources for product data mining tasks. To do so, we process billions of structured data points in the form of RDF n-quads, to create multi-million words of product-related corpora that are later used in three different ways for creating language resources: training word-embedding models, continued pre-training of BERT-like language models, and training machine translation models that are used as a proxy to generate product-related keywords. These language resources are then evaluated in three downstream tasks, product classification, linking, and fake review detection using an extensive set of benchmarks. Our results show word embeddings to be the most reliable and consistent method to improve the accuracy on all tasks (with up to 6.9\% points in macro-average F1 on some datasets). Contrary to some earlier studies that suggest a rather simple but effective approach such as building domain-specific language models by pre-training using in-domain corpora, our work serves a lesson that adapting these methods to new domains may not be as easy as it seems. We further analyse our datasets and reflect on how our findings can inform future research and practice.},
   author = {Ziqi Zhang and Xingyi Song},
   doi = {10.1007/s42979-022-01415-3},
   issn = {2661-8907},
   issue = {1},
   journal = {SN Computer Science},
   pages = {15},
   title = {An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining},
   volume = {4},
   url = {https://doi.org/10.1007/s42979-022-01415-3},
   year = {2022}
}
@article{Sahu2023,
   abstract = {This paper explores and evaluates the effect of different stopword removal and stemming techniques in Urdu IR. The issues are examined from four viewpoints. Is there any performance difference between non-corpus-based and corpus-based stopword removal in Urdu IR? Can corpus-based stopword lists improve performance in Urdu IR? Among the different corpus-based stopword lists, which stopword list gives the best performance in the IR domain? Does language-independent stemmer improve the performance of Urdu IR? Which is the best stemmer for Urdu IR? Whether to use clustering-based (YASS) or fast corpus-based (FCB) or co-occurrence-based (SNS) or graph-based (GRAS) or Trunc-n-based indexing? It was observed that the shorter length of a corpus-based stopword list outperforms a larger length of a non-corpus-based stopword list in the IR domain. Among the different corpus-based stopword lists, Zipf’s law-based stopword list (nidf approach) provides the best performance and improves a mean average precision (MAP) score of 4.9\% compared to baseline approaches. During stemming evaluation, we observed that the language-independent stemming techniques improve retrieval performance in Urdu IR. Among the different stemming techniques, the FCB V-1-based stemmer performs best and improves a MAP score of 1.41\% compared to the no-stemming approach. The trunc-n-based indexing strategy provides comparable performance to the language-independent stemming approach. In both the stopword removal and stemming strategies, the BB2 retrieval model outperforms other models in the IR domain.},
   author = {Siba Sankar Sahu and Debrup Dutta and Sukomal Pal and Imran Rasheed},
   doi = {10.1007/s42979-023-01953-4},
   issn = {2661-8907},
   issue = {5},
   journal = {SN Computer Science},
   pages = {547},
   title = {Effect of Stopwords and Stemming Techniques in Urdu IR},
   volume = {4},
   url = {https://doi.org/10.1007/s42979-023-01953-4},
   year = {2023}
}
@article{Unger2023,
   abstract = {Despite the increase in scientific publications in the field of integrative medicine over the past decades, a valid overview of published evidence remains challenging to get. The online literature database CAMbase (available at https://cambase.de) is one of the established databases designed to provide such an overview. In 2020, the database was migrated from a 32-bit to a 64-bit operating system, which resulted in unexpected, technical issues and forced the replacement of the semantic search algorithm with Solr, an open-source platform that uses a score ranking algorithm. Although semantic search was replaced, the goal was to create a literature database that is essentially no different from the legacy system. Therefore, a before-after analysis was conducted to compare first the number of retrieved documents and then their titles, while the titles were syntactically compared using two Sentence-Bidirectional Encoder Representations from Transformers (SBERT) models. Analysis with a paired t-test revealed no significant overall differences between the legacy system and the final system in the number of documents (t =− 1.41, df = 35, p = 0.17), but an increase in performance (t = 4.13, df = 35, p < 0.01). Analysis with a t-test for independent samples of the values from the models also revealed a high degree of consistency between the retrieved documents. The results show that an equivalent search can be provided by using Solr, while improving the performance, making this technical report a viable blueprint for projects with similar contexts.},
   author = {Sebastian Unger and Christa K Raak and Thomas Ostermann},
   doi = {10.1007/s42979-023-02146-9},
   issn = {2661-8907},
   issue = {5},
   journal = {SN Computer Science},
   pages = {691},
   title = {Reliability and Performance of the Online Literature Database CAMbase after Changing from a Semantic Search to a Score Ranking Algorithm},
   volume = {4},
   url = {https://doi.org/10.1007/s42979-023-02146-9},
   year = {2023}
}
@article{Bozas2024,
   abstract = {With the constant growth of social media in our daily lives, a huge amount of information is generated online by multiple social networks. However, what can we actually extract with the science of social media sensing? It is a very challenging task to mine meaningful data out of this vast crowdsourcing volume, which also rapidly changes or ends up being misleading. The scope of this paper is to present different approaches that overcome these challenges and utilize social media information from various sources. This work illustrates applications that: improve the performance of architectural design; preserve the cultural heritage; enhance citizen security; provide early detection for disasters; and discover creeping crisis events. A large variety of analyses are presented, including, among other, disaster or crime event detection, user identity linkage, relevance classification, and community detection techniques. The evaluation of the presented methods is also given in this article, proving that they can be practical and valuable in many applications.},
   author = {Aris Bozas and Stelios Andreadis and Despoina Chatzakou and Spyridon Symeonidis and Ourania Theodosiadou and Pantelis Kyriakidis and Alexandros Kokkalas and Evangelos A Stathopoulos and Sotiris Diplaris and Theodora Tsikrika and Ilias Gialampoukidis and Stefanos Vrochidis and Ioannis Kompatsiaris},
   doi = {10.1007/s42979-024-02712-9},
   issn = {2661-8907},
   issue = {5},
   journal = {SN Computer Science},
   pages = {484},
   title = {From Research to Applications: What Can We Extract with Social Media Sensing?},
   volume = {5},
   url = {https://doi.org/10.1007/s42979-024-02712-9},
   year = {2024}
}
@article{Fraga2024,
   abstract = {The field of Personal Knowledge Management (PKM) has seen a surge in popularity in recent years. Interestingly, Natural Language Processing (NLP) and Large Language Models are also becoming mainstream, but PKM has not seen much integration with NLP. With this motivation, this article first introduces a methodology to automatically interconnect isolated text collections using NLP techniques combined with Knowledge Graphs. The text connections are generated by exploring the semantic relatedness of the texts and the concepts they share. The article proceeds to describe PKM Assistants that incorporate the methodology to assist users in understanding and exploring the knowledge contained in text collections using a Knowledge Management tool called Tana. The article continues with an assessment of the methodology using a text collection composed of several books and book passages collected for each book. Finally, the article concludes with a discussion of the proposed methodology, with special attention to the potential use cases.},
   author = {Felipe Poggi A Fraga and Marcus Poggi and Marco A Casanova and Luiz André P Paes Leme},
   doi = {10.1007/s42979-024-02876-4},
   issn = {2661-8907},
   issue = {5},
   journal = {SN Computer Science},
   pages = {525},
   title = {Creating Automatic Connections for Personal Knowledge Management},
   volume = {5},
   url = {https://doi.org/10.1007/s42979-024-02876-4},
   year = {2024}
}
@article{Karakikes2024,
   abstract = {Bias identification and mitigation in the social media ecosystem has been lately researched towards achieving a more efficient utilization of social media platforms for different stakeholders and purposes. Among these stakeholders, intelligence services worldwide, collectively called the Intelligence Community (IC), tend to use social media, supplementarily to their pre-extant disciplines, for monitoring areas of interest and identifying emerging social, political and security trends/threats. Over time, the IC has identified bias as the major impediment in information analysis, thus it has developed scientific and empirical methods for bias mitigation, in parallel to those developed by the information and communication technology (ICT) and artificial intelligence (AI) community. As it becomes apparent, it is to both communities’ interest to accurately trace bias and ideally eradicate or moderate its effects. This paper is an extension of a previously presented academic work, in which we drew systemic parallels between Intelligence Analysis (IA) and Twitter Analytics (TA), comparatively examined existing bias mitigating methodologies to identify similarities/dissimilarities, and subsequently investigated the viability of adopting and attuning methodologies from the first field to the latter. Furthermore, we proposed a novel framework for AI-augmented bias mitigation in the IC and simultaneously recommended on a theoretical level, methods and tools, already adapted by the ICT community, for efficiently supporting bias mitigation in each phase of the aforementioned framework. In the current paper, we extend our previous work by implementing the collection phase of the proposed framework on a real-world use case utilizing Telegram as a collection platform. We contribute new insights resulted from our experimentation with a tri-modal source selection approach in which human agents and Large Language Models (LLMs) are involved. The experiments were performed with data collected using one of the correspondingly suggested tools, engineering an equally represented, balanced dataset for the working case.},
   author = {Alexandros Karakikes and Panagiotis Alexiadis and Konstantinos Kotis},
   doi = {10.1007/s42979-024-02935-w},
   issn = {2661-8907},
   issue = {5},
   journal = {SN Computer Science},
   pages = {574},
   title = {Bias in X (Twitter) and Telegram Based Intelligence Analysis: Exploring Challenges and Potential Mitigating Roles of AI},
   volume = {5},
   url = {https://doi.org/10.1007/s42979-024-02935-w},
   year = {2024}
}
@article{Fahad2024,
   abstract = {Question generation (QG) from a given context paragraph is a demanding task in natural language processing for its practical applications and prospects in various fields. Several studies have been conducted on QG in high-resource languages like English, however, very few have been done on resource-poor languages like Arabic and Bangla. In this work, we propose a finetuning method for QG that uses pre-trained transformer-based language models to generate questions from a given context paragraph in Bangla. Our approach is based on the idea that a transformer-based language model can be used to learn the relationships between words and phrases in a context paragraph which allows the models to generate questions that are both relevant and grammatically correct. We finetuned three different transformer models: (1) BanglaT5, (2) mT5-base, (3) BanglaGPT2, and demonstrated their capabilities using two different data formatting techniques: (1) AQL—All Question Per Line, (2) OQL—One Question Per Line, making it a total of six different variations of QG models. For each of these variants, six different decoding algorithms: (1) Greedy search, (2) Beam search, (3) Random Sampling, (4) Top K sampling, (5) Top- p Sampling, 6) a combination of Top K and Top-p Sampling were used to generate questions from the test dataset. For evaluation of the quality of questions generated using different models and decoding techniques, we also fine-tuned another transformer model BanglaBert on two custom datasets of our own and created two question classifier (QC) models that check the relevancy and Grammatical correctness of the questions generated by our QG models. The QC models showed test accuracy of 88.54\% and 95.76\% in the case of correctness and relevancy checks, respectively. Our results show that among all the variants of the QG, the mT5 OQL approach and beam decoding algorithm outperformed all the other ones in terms of relevancy (77\%) and correctness (96\%) with 36.60 Bleu_4, 48.98 METEOR, and 63.38 ROUGE-L scores.},
   author = {Abdur Rahman Fahad and Nazme Al Nahian and Md Ahanaf Islam and Rashedur M Rahman},
   doi = {10.1007/s44227-023-00018-5},
   issn = {2211-7946},
   issue = {1},
   journal = {International Journal of Networked and Distributed Computing},
   pages = {82-107},
   title = {Answer Agnostic Question Generation in Bangla Language},
   volume = {12},
   url = {https://doi.org/10.1007/s44227-023-00018-5},
   year = {2024}
}
@article{He2022,
   abstract = {The increasingly developed online platform generates a large amount of online reviews every moment, e.g., Yelp and Amazon. Consumers gradually develop the habit of reading previous reviews before making a decision of buying or choosing various products. Online reviews play an vital part in determining consumers’ purchase choices in e-commerce, yet many online reviews are intentionally created to confuse or mislead potential consumers. Moreover, driven by product reputations and merchants’ profits, more and more spam reviews were inserted into online platform. This kind of reviews can be positive, negative or neutral, but they had common features: misleading consumers or damaging reputations. In the past decade, many people conducted research on detecting spam reviews using statistical or deep learning method with various datasets. In view of that, this article first introduces the task of spam online reviews detection and makes a common definition of spam reviews. Then, we comprehensively conclude the existing method and available datasets. Third, we summarize the existing network-based approaches in dealing with this task and propose some direction for future research.},
   author = {Li He and Xianzhi Wang and Hongxu Chen and Guandong Xu},
   doi = {10.1007/s44230-022-00001-3},
   issn = {2667-1336},
   issue = {1},
   journal = {Human-Centric Intelligent Systems},
   pages = {14-30},
   title = {Online Spam Review Detection: A Survey of Literature},
   volume = {2},
   url = {https://doi.org/10.1007/s44230-022-00001-3},
   year = {2022}
}
@article{Moguel-Snchez2023,
   abstract = {Modern Software Engineering thrives with innovative tools that aid developers in creating better software grounded on quality standards. Software bots are an emerging and exciting trend in this regard, supporting numerous software development activities. As an emerging trend, few studies describe and analyze different bots in software development. This research presents a systematic literature review covering the state of the art of applied and proposed bots for software development. Our study spans literature from 2003 to 2022, with 82 different bots applied in software development activities, covering 83 primary studies. We found four bot archetypes: chatbots which focus on direct communication with developers to aid them, analysis bots that display helpful information in different tasks, repair bots for resolving software defects, and development bots that combine aspects of other bot technologies to provide a service to the developer. The primary benefits of using bots are increasing software quality, providing useful information to developers, and saving time through the partial or total automation of development activities. However, drawbacks are reported, including limited effectiveness in task completion, high coupling to third-party technologies, and some prejudice from developers toward bots and their contributions. We discovered that including Bots in software development is a promising field of research in software engineering that has yet to be fully explored.},
   author = {R Moguel-Sánchez and C S Sergio Martínez-Palacios and J O Ocharán-Hernández and X Limón and A J Sánchez-García},
   doi = {10.1134/S0361768823080145},
   issn = {1608-3261},
   issue = {8},
   journal = {Programming and Computer Software},
   pages = {712-734},
   title = {Bots in Software Development: A Systematic Literature Review and Thematic Analysis},
   volume = {49},
   url = {https://doi.org/10.1134/S0361768823080145},
   year = {2023}
}
@article{Bijari2022,
   abstract = {The amount of unstructured text produced daily in scholarly journals is enormous. Systematically identifying, sorting, and structuring information from such a volume of data is increasingly challenging for researchers even in delimited domains. Named entity recognition is a fundamental natural language processing tool that can be trained to annotate, structure, and extract information from scientific articles. Here, we harness state-of-the-art machine learning techniques and develop a smart neuroscience metadata suggestion system accessible by both humans through a user-friendly graphical interface and machines via Application Programming Interface. We demonstrate a practical application to the public repository of neural reconstructions, NeuroMorpho.Org, thus expanding the existing web-based metadata management system currently in use. Quantitative analysis indicates that the suggestion system reduces personnel labor by at least 50\%. Moreover, our results show that larger training datasets with the same software architecture are unlikely to further improve performance without ad-hoc heuristics due to intrinsic ambiguities in neuroscience nomenclature. All components of this project are released open source for community enhancement and extensions to additional applications.},
   author = {Kayvan Bijari and Yasmeen Zoubi and Giorgio A Ascoli},
   doi = {10.1186/s40708-022-00174-4},
   issn = {2198-4026},
   issue = {1},
   journal = {Brain Informatics},
   pages = {26},
   title = {Assisted neuroscience knowledge extraction via machine learning applied to neural reconstruction metadata on NeuroMorpho.Org},
   volume = {9},
   url = {https://doi.org/10.1186/s40708-022-00174-4},
   year = {2022}
}
@article{Tang2019,
   abstract = {Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.},
   author = {Yong-chuan Tang and Jiang-jie Huang and Meng-ting Yao and Jia Wei and Wei Li and Yong-xing He and Ze-jian Li},
   doi = {10.1631/FITEE.1900398},
   issn = {2095-9230},
   issue = {12},
   journal = {Frontiers of Information Technology and Electronic Engineering},
   pages = {1595-1617},
   title = {A review of design intelligence: progress, problems, and challenges},
   volume = {20},
   url = {https://doi.org/10.1631/FITEE.1900398},
   year = {2019}
}
@article{Jia2021,
   abstract = {Tagging is a defining characteristic of Web 2.0. It allows users of social computing systems (e.g., question and answering (Q&A) sites) to use free terms to annotate content. However, is tagging really a free action? Existing work has shown that users can develop implicit consensus about what tags best describe the content in an online community. However, there has been no work studying the regularities in how users order tags during tagging. In this paper, we focus on the natural ordering of tags in domain-specific Q&A sites. We study tag sequences of millions of questions in four Q&A sites, i.e., CodeProject, SegmentFault, Biostars, and CareerCup. Our results show that users of these Q&A sites can develop implicit consensus about in which order they should assign tags to questions. We study the relationships between tags that can explain the emergence of natural ordering of tags. Our study opens the path to improve existing tag recommendation and Q&A site navigation by leveraging the natural ordering of tags.},
   author = {Junfang Jia and Guoqiang Li},
   doi = {10.1631/FITEE.1900645},
   issn = {2095-9230},
   issue = {2},
   journal = {Frontiers of Information Technology and Electronic Engineering},
   pages = {170-184},
   title = {Learning natural ordering of tags in domain-specific Q&A sites},
   volume = {22},
   url = {https://doi.org/10.1631/FITEE.1900645},
   year = {2021}
}
@article{Jia2021,
   abstract = {A sheer number of techniques and web resources are available for software engineering practice and this number continues to grow. Discovering semantically similar or related technical terms and web resources offers the opportunity to design appealing services to facilitate information retrieval and information discovery. In this study, we extract technical terms and web resources from a community of question and answer (Q&A) discussions and propose an approach based on a neural language model to learn the semantic representations of technical terms and web resources in a joint low-dimensional vector space. Our approach maps technical terms and web resources to a semantic vector space based only on the surrounding technical terms and web resources of a technical term (or web resource) in a discussion thread, without the need for mining the text content of the discussion. We apply our approach to Stack Overflow data dump of March 2018. Through both quantitative and qualitative analyses in the clustering, search, and semantic reasoning tasks, we show that the learnt technical-term and web-resource vector representations can capture the semantic relatedness of technical terms and web resources, and they can be exploited to support various search and semantic reasoning tasks, by means of simple K-nearest neighbor search and simple algebraic operations on the learnt vector representations in the embedding space.},
   author = {Junfang Jia and Valeriia Tumanian and Guoqiang Li},
   doi = {10.1631/FITEE.2000186},
   issn = {2095-9230},
   issue = {7},
   journal = {Frontiers of Information Technology and Electronic Engineering},
   pages = {969-985},
   title = {Discovering semantically related technical terms and web resources in Q&A discussions},
   volume = {22},
   url = {https://doi.org/10.1631/FITEE.2000186},
   year = {2021}
}
@article{Li2024-4,
   abstract = {Text generation is an essential research area in artificial intelligence (AI) technology and natural language processing and provides key technical support for the rapid development of AI-generated content (AIGC). It is based on technologies such as natural language processing, machine learning, and deep learning, which enable learning language rules through training models to automatically generate text that meets grammatical and semantic requirements. In this paper, we sort and systematically summarize the main research progress in text generation and review recent text generation papers, focusing on presenting a detailed understanding of the technical models. In addition, several typical text generation application systems are presented. Finally, we address some challenges and future directions in AI text generation. We conclude that improving the quality, quantity, interactivity, and adaptability of generated text can help fundamentally advance AI text generation development.},
   author = {Bing Li and Peng Yang and Yuankang Sun and Zhongjian Hu and Meng Yi},
   doi = {10.1631/FITEE.2300410},
   issn = {2095-9230},
   issue = {1},
   journal = {Frontiers of Information Technology and Electronic Engineering},
   pages = {64-83},
   title = {Advances and challenges in artificial intelligence text generation},
   volume = {25},
   url = {https://doi.org/10.1631/FITEE.2300410},
   year = {2024}
}
@inbook{Haleem2022,
   abstract = {Summary Internet of Things (IoT) is the global trend, which has been used everywhere and anywhere. When used in sensitive places, privacy becomes the concern. IoT security has to be taken seriously. When it comes to implementation, we have to think of restrictions in processing power and energy consumption which are the major obstacles that can give huge chances to intruder and third party access, compared with the normal system. Here we are discussing about considering resource scarcity and vulnerabilities for the IoT smart home, concerning the node to node throughout the wireless network communication and transmission of data via providing the uniquely labeled key generation model, double hashed unique labeled keybased validation, which was proposed for efficient and simplest authentication mechanism to the IoT smart home.},
   author = {Sulaima Lebbe Abdul Haleem},
   doi = {https://doi.org/10.1002/9781119865605.ch14},
   isbn = {9781119865605},
   booktitle = {Cognitive Computing Models in Communication Systems},
   keywords = {I                    o                    T,S                    mart home,access control,double hashed unique labeled key,node authentication},
   pages = {199-219},
   publisher = {John Wiley \& Sons, Ltd},
   title = {A Secured Node Authentication and Access Control Model for IoT Smart Home Using Double-Hashed Unique Labeled Key-Based Validation},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119865605.ch14},
   year = {2022}
}
@article{Zolfaghari2022,
   abstract = {Abstract Cloud systems have become an essential part of our daily lives owing to various Internet-based services. Consequently, their energy utilization has also become a necessary concern in cloud computing systems increasingly. Live migration, including several virtual machines (VMs) packed on in minimal physical machines (PMs) as virtual machines consolidation (VMC) technique, is an approach to optimize power consumption. In this article, we have proposed an energy-aware method for the VMC problem, which is called energy-aware virtual machines consolidation (EVMC), to optimize the energy consumption regarding the quality of service guarantee, which comprises: (1) the support vector machine classification method based on the utilization rate of all resource of PMs that is used for PM detection in terms of the amount' load; (2) the modified minimization of migration approach which is used for VM selection; (3) the modified particle swarm optimization which is implemented for VM placement. Also, the evaluation of the functional requirements of the method is presented by the formal method and the non-functional requirements by simulation. Finally, in contrast to the standard greedy algorithms such as modified best fit decreasing, the EVMC decreases the active PMs and migration of VMs, respectively, 30\%, 50\% on average. Also, it is more efficient for the energy 30\% on average, resources and the balance degree 15\% on average in the cloud.},
   author = {Rahmat Zolfaghari and Amir Sahafi and Amir Masoud Rahmani and Reza Rezaei},
   doi = {https://doi.org/10.1002/spe.3010},
   issue = {1},
   journal = {Software: Practice and Experience},
   keywords = {cloud computing systems (CCSs),data center,energy consumption,formal verification,virtual machines consolidation (VMC)},
   pages = {194-235},
   title = {An energy-aware virtual machines consolidation method for cloud computing: Simulation and verification},
   volume = {52},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3010},
   year = {2022}
}
@article{Niyogi2024,
   abstract = {Abstract Team formation in an environment where some relevant parameters are not known in advance is a challenging problem. Communicating automata and distributed algorithms have been used to describe protocols for team formation. A high-level specification provides a mathematical description of a protocol or a program. TLA is a formal specification language designed to provide high-level specifications of concurrent and distributed systems. The associated model checker known as TLC is capable of model checking the TLA specifications. Recently, formal specification of a team formation protocol is given using TLA when there is a single initiator (an agent or a robot) that initiates the team formation. Using TLA, we examine the formal specification for the multiple initiator situation and demonstrate that a composition technique can yield a single monolithic specification for the multiple initiator situation from the single initiator situation specification. We have used models of varying sizes, and the TLC model checker has confirmed that the protocol's specifications meet certain desired characteristics in each case.},
   author = {Rajdeep Niyogi and Amar Nath},
   doi = {https://doi.org/10.1002/spe.3307},
   issue = {6},
   journal = {Software: Practice and Experience},
   keywords = {TLA+ ^+,formal specification,multi-agent system,team formation,verification},
   pages = {961-984},
   title = {Formal specification and verification of a team formation protocol using TLA+},
   volume = {54},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3307},
   year = {2024}
}
@article{Schoofs2022,
   abstract = {Abstract Test amplification aims to automatically improve a test suite. One technique generates new test methods through transformations of the original tests. These test amplification tools heavily rely on analysis techniques that benefit a lot from type declarations present in the source code of projects written in statically typed languages. In dynamically typed languages, such type declarations are not available, and therefore, research regarding test amplification for those languages is sparse. Recent work has brought test amplification to the dynamically typed language Pharo Smalltalk by introducing the concept of dynamic type profiling. The technique is dependent on Pharo-specific frameworks and has not yet been generalized to other languages. Another significant downside in test amplification tools based on the mutation score of a test suite is their high time cost. In this paper, we present AmPyfier, a tool that brings test amplification and type profiling to the dynamically typed language Python. AmPyfier introduces multi-metric selection in order to increase the time efficiency of test amplification. We evaluated AmPyfier on 11 open-source projects and found that AmPyfier could strengthen 37 out of 54 test classes. Multi-metric selection decreased the time cost ranging from 17\% to 98\% as opposed to selection based on the full mutation score.},
   author = {Ebert Schoofs and Mehrdad Abdi and Serge Demeyer},
   doi = {https://doi.org/10.1002/smr.2490},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Python,mutation testing,test amplification,unit testing},
   pages = {e2490},
   title = {AmPyfier: Test amplification in Python},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2490},
   year = {2022}
}
@article{Reyes-Anastacio2024,
   abstract = {Abstract This article presents the design and implementation of a reliable computing virtual container-based model with integrity verification for data processing strategies named the reliability and integrity verification (RIV) scheme. It has been integrated into a system construction model as well as existing workflow engines (e.g., Kulla and Makeflow) for composing in-memory systems. In the RIV scheme, the reliability (R) component is in charge of providing an implicit fault tolerance mechanism for the processes of data acquisition and storage that take place in a data processing system. The integrity verification (IV) component is in charge of ensuring that data transmitted/received between two processing stages are correct and are not modified during the transmission process. To show the feasibility of using the RIV scheme, real-world applications were created by using different distributed and parallel systems to solve use cases of satellite and medical imagery processing. This evaluation revealed encouraging results as some solutions that assumed the cost (overhead) of using the RIV scheme, for example, Kulla (the Kulla-RIV solution), achieve better response times than others without the RIV scheme (e.g., Makeflow) that remain exposed to the risks caused by to the lack of RIV strategies.},
   author = {Hugo G Reyes-Anastacio and Jose L Gonzalez-Compeán and Victor J Sosa-Sosa and Ricardo Marcelín-Jiménez and Miguel Morales-Sandoval},
   doi = {https://doi.org/10.1002/spe.3328},
   issue = {8},
   journal = {Software: Practice and Experience},
   keywords = {cloud computing,design pattern,reliability and integrity verification,task and data parallelism,virtual containers,workflow},
   pages = {1516-1542},
   title = {Kulla-RIV: A composing model with integrity verification for efficient and reliable data processing services},
   volume = {54},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3328},
   year = {2024}
}
@article{Liu2022-1,
   abstract = {Summary Function block diagram (FBD) is a standard programming language for programmable logic controllers (PLCs). PLCs have been widely used to develop safety-critical systems such as nuclear reactor protection systems. It is crucial to test FBD programs for such systems effectively. This paper presents an automated test sequence generation approach using mutation testing techniques for FBD programs and the developed tool, MuFBDTester. Given an FBD program, MuFBDTester analyses the program and generates mutated programs based on mutation operators. MuFBDTester translates the given program and mutants into the input language of a satisfiability modulo theories (SMT) solver to derive a set of test sequences. The primary objective is to find the test data that can distinguish between the results of the given program and mutants. We conducted experiments with several examples including real industrial cases to evaluate the effectiveness and efficiency of our approach. With the control of test size, the results indicated that the mutation-based test suites were statistically more effective at revealing artificial faults than structural coverage-based test suites. Furthermore, the mutation-based test suites detected more reproduced faults, found in industrial programs, than structural coverage-based test suites. Compared to structural coverage-based test generation time, the time required by MuFBDTester to generate one test sequence from industrial programs is approximately 1.3 times longer; however, it is considered to be worth paying the price for high effectiveness. Using MuFBDTester, the manual effort of creating test suites was significantly reduced from days to minutes due to automated test generation. MuFBDTester can provide highly effective test suites for FBD engineers.},
   author = {Lingjun Liu and Eunkyoung Jee and Doo-Hwan Bae},
   doi = {https://doi.org/10.1002/stvr.1815},
   issue = {8},
   journal = {Software Testing, Verification and Reliability},
   keywords = {SMT solver,automated test generation,function block diagram,mutation adequacy,mutation testing},
   pages = {e1815},
   title = {MuFBDTester: A mutation-based test sequence generator for FBD programs implementing nuclear power plant software},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1815},
   year = {2022}
}
@article{Wang2020,
   abstract = {Summary Software-defined networking (SDN) decouples the control and data planes to simplify network management and function deployment. SDN provides a solution for managing large-scale virtual networks in the cloud environment. However, in the process of SDN network update, various attacks can lead to network state inconsistency. In this paper, a comprehensive and efficient verification scheme is proposed to defend the security threats and guarantee the network state consistency in the cloud environment. The scheme verifies the consistency of network update from two stages of network update request and response. Firstly, the flow path model and the security space are abstracted to quickly verify whether the network request is allowed. Then, a novel forwarding path probing and verification method is designed to validate the actual forwarding path and locate the abnormal path in real time. With the two-stage verification, the scheme can prevent the spread of illegal flow rules and ensure the correct delivery and execution of flow rules. Finally, we carry out a series of experiments in OpenStack. The results show that the proposed scheme can detect security threats and label the abnormal forwarding path in real time to ensure the network state consistency, while introducing negligible performance overhead.},
   author = {Xiaoyan Wang and Xingshu Chen and Yitong Wang and Long Ge},
   doi = {https://doi.org/10.1002/cpe.5440},
   issue = {2},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {OpenFlow,abnormal forwarding,cloud computing,consistency verification,software-defined networking (SDN)},
   note = {e5440 cpe.5440},
   pages = {e5440},
   title = {An efficient scheme for SDN state consistency verification in cloud computing environment},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5440},
   year = {2020}
}
@article{Amara2021,
   abstract = {Software metrics which are language-dependent are proposed as quantitative measures to assess internal quality factors for both method and class levels like cohesion and complexity. The external quality factors like reliability and maintainability are in general predicted using different metrics of internal attributes. Literature review shows a lack of software metrics which are proposed for reliability measurement and prediction. In this context, a suite of four semantic language-independent metrics was proposed by Mili et al. (2014) to assess program redundancy using Shannon entropy measure. The main objective of these metrics is to monitor program reliability. Despite their important purpose, they are manually computed and only theoretically validated. Therefore, this paper aims to assess the redundancy metrics and empirically validate them as significant reliability indicators. As software reliability is an external attribute that cannot be directly evaluated, we employ other measurable quality factors that represent direct reflections of this attribute. Among these factors, defect density is widely used to measure and predict software reliability based on software metrics. Therefore, a linear regression technique is used to show the usefulness of these metrics as significant indicators of software defect density. A quantitative model is then proposed to predict software defect density based on redundancy metrics in order to monitor software reliability.},
   author = {Dalila Amara and Ezzeddine Fatnassi and Latifa Ben Arfa Rabai},
   doi = {https://doi.org/10.1155/2021/8325417},
   issue = {1},
   journal = {Scientific Programming},
   pages = {8325417},
   title = {An Empirical Assessment and Validation of Redundancy Metrics Using Defect Density as Reliability Indicator},
   volume = {2021},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/8325417},
   year = {2021}
}
@article{Chawla2019,
   abstract = {Summary This paper presents the framework of cloud-based software test data generation service (CSTS) that caters to cost-effective test data generation service in a cloud environment. In contrast to existing conventional or cloud-based testing frameworks, CSTS has a number of unique benefits. First, CSTS is designed to facilitate test data generation in minimum time and cost. Second, unlike existing frameworks which mandates clients to opt for resources to test their jobs, CSTS guides customer for selecting best cluster configuration in order to minimize the cost. While the existing models do not provide any solution for trust establishment in cloud computing services, CSTS delivers it by implementing security mechanism with the provision of role based access control. The security mechanism proposed in this paper ensures the protection of data and code of different users. Third, CSTS provides a mathematical pricing model to fulfill the expectations of customers and also to maximize the net profit of service providers. Cloud service request model has also been designed that postulates service level agreements between customers and service providers. We have evaluated, compared, and analyzed our framework and have found that it outperforms other existing cloud-based frameworks.},
   author = {Priyanka Chawla and Inderveer Chana and Ajay Rana},
   doi = {https://doi.org/10.1002/spe.2708},
   issue = {8},
   journal = {Software: Practice and Experience},
   keywords = {cloud testing,cloud-based testing service,prediction and profiling,pricing model,security model,software testing,testing service framework,trust},
   note = { spe.2708},
   pages = {1307-1328},
   title = {Framework for cloud-based software test data generation service},
   volume = {49},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2708},
   year = {2019}
}
@article{Minhas2020,
   abstract = {Abstract Model-based test case generation techniques provide a mechanism to derive tests systematically. This study provides a systematic mapping of test case generation techniques based on UML interaction diagrams. The study compares the test case generation techniques regarding their capabilities and limitations, and it also assesses the reporting quality of the primary studies. We can conclude that the studies presenting test case generation techniques using UML interaction diagrams were not following the guidelines for research methods (eg, case studies or experiments). Solutions were not empirically evaluated in industrial contexts. Our study revealed that better tool support is needed to introduce the UML interaction diagram–based test case generation techniques in the industry.},
   author = {Nasir Mehmood Minhas and Sohaib Masood and Kai Petersen and Aamer Nadeem},
   doi = {https://doi.org/10.1002/smr.2235},
   issue = {6},
   journal = {Journal of Software: Evolution and Process},
   keywords = {interaction diagrams,model-based testing,software testing,systematic mapping,test case generation},
   note = {e2235 smr.2235},
   pages = {e2235},
   title = {A systematic mapping of test case generation techniques using UML interaction diagrams},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2235},
   year = {2020}
}
@article{Amasya2024,
   abstract = {Abstract This retrospective study is aimed at developing a web-based artificial intelligence (AI) software (DiagnoCat) for periodontal bone loss detection on panoramic radiographs and evaluating the model's performance by comparing it with clinicians' results. Separate models are trained for tooth and periodontal bone loss detection. The first model's objective was to detect teeth, segmenting their masks, and to define their numbering and developed with Mask R-CNN using pretrained ResNet-101 as a backbone. The second model was based on Cascade R-CNN architecture and used for bone loss prediction. Around 100 radiographs are evaluated by three clinicians regarding tooth identification and periodontal bone loss, separately. Ground truth is determined by the consensus and model's performance is evaluated with kappa, precision, recall, and F-score statistics. For tooth conditions, the overall F-score, accuracy, and Cohen's kappa coefficients were found to be 0.948, 0.977, and 0.933 for the binary, and 0.992, 0.988, and 0.961 for the multiclass results. For bone loss detection, the overall F-score, accuracy, and Cohen's kappa coefficients were found to be 0.985, 0.980, and 0.956 for the binary, and 0.996, 0.993, and 0.974 for the multiclass results. The results of this study suggest that the use of a web-based AI software (DiagnoCat) can be beneficial in detecting periodontal bone loss on panoramic radiographs.},
   author = {Hakan Amasya and Prashant Prakash Jaju and Matvey Ezhov and Maxim Gusarev and Cemal Atakan and Alex Sanders and David Manulius and Maria Golitskya and Kriti Shrivastava and Ajita Singh and Anuja Gupta and Merve Önder and Kaan Orhan},
   doi = {https://doi.org/10.1002/ima.22973},
   issue = {1},
   journal = {International Journal of Imaging Systems and Technology},
   keywords = {alveolar bone,dentistry,periodontal diagnostics,periodontal disease,radiography},
   pages = {e22973},
   title = {Development and validation of an artificial intelligence software for periodontal bone loss in panoramic imaging},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ima.22973},
   year = {2024}
}
@article{Leveau2022,
   abstract = {Summary Exploratory testing (ET) is a software testing approach that complements automated testing by leveraging business expertise. It has gained momentum over the last decades as it appeals testers to exploit their business knowledge to stress the system under test (SUT). Exploratory tests, unlike automated tests, are defined and executed on-the-fly by testers. However, testers who perform exploratory tests may be biased by their experience and, incidentally, miss anomalies or unusual interactions proposed by the SUT. This is even more complex in the context of web applications, which typically expose a huge number of interaction paths to their users. As testers of these applications cannot remember all the sequences of interactions they performed, they may fail to deeply explore the application scope. This article, therefore, introduces a new approach to assist testers in widely exploring any web application. In particular, our approach monitors the online interactions performed by the testers to suggest in real-time the probabilities of performing next interactions. Looking at these probabilities, we claim that the testers who favour interactions that have a low probability (because they were rarely performed), will increase the diversity of their explorations. Our approach defines a prediction model, based on n-grams, that encodes the history of past interactions and that supports the estimation of the probabilities. Integrated within a web browser extension, it automatically and transparently injects feedback within the application itself. We conduct a controlled experiment and a qualitative study to assess our approach. Results show that it prevents testers to be trapped in already tested loops, and succeeds to assist them in performing deeper explorations of the SUT.},
   author = {Julien Leveau and Xavier Blanc and Laurent Réveillère and Jean-Rémy Falleri and Romain Rouvoy},
   doi = {https://doi.org/10.1002/stvr.1827},
   issue = {5},
   journal = {Software Testing, Verification and Reliability},
   keywords = {exploratory test,n-gram,software testing,web applications},
   pages = {e1827},
   title = {Fostering the diversity of exploratory testing in web applications},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1827},
   year = {2022}
}
@article{Li2024-5,
   abstract = {Summary Continuous integration (CI) is a widely applied development practice to allow frequent integration of software changes, detecting early faults. However, extremely frequent builds consume amounts of time and resources in such a scenario. It is quite challenging for existing test case prioritization (TCP) to address this issue due to the time-consuming information collection (e.g. test coverage) or inaccurately modelling code semantics to result in the unsatisfied prioritization. In this paper, we propose a semantic-aware two-phase TCP framework, named SatTCP, which combines the coarse-grained filtering and fine-grained prioritization to perform the precise TCP with low time costs for CI. It consists of three parts: (1) code representation, parsing the programme changes and test cases to obtain the code change and test case representations; (2) coarse-grained filtering, conducting the preliminary ranking and filtering of test cases based on information retrieval; and (3) fine-grained prioritization, training a pretrained Siamese language model based on the filtered test set to further sort the test cases via semantic similarity. We evaluate SatTCP on a large-scale, real-world dataset with cross-project validation from fault detection efficiency and time costs and compare it with five baselines. The results show that SatTCP outperforms all baselines by 6.3\%–45.6\% for mean average percentage of fault detected per cost (APFDc), representing an obvious upward trend as the project scale increases. Meanwhile, SatTCP can reduce the real CI testing by 71.4\%, outperforming the best baseline by 17.2\% for time costs on average. Furthermore, we discuss the impact of different configurations, flaky tests and hybrid techniques on the performance of SatTCP, respectively.},
   author = {Yingling Li and Ziao Wang and Junjie Wang and Jie Chen and Rui Mou and Guibing Li},
   doi = {https://doi.org/10.1002/stvr.1864},
   issue = {1},
   journal = {Software Testing, Verification and Reliability},
   keywords = {continuous integration,information retrieval,pretrained language models,test case prioritization},
   pages = {e1864},
   title = {Semantic-aware two-phase test case prioritization for continuous integration},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1864},
   year = {2024}
}
@article{Xiao2021,
   abstract = {Summary Software Defined Networking (SDN) with multiple controllers draws more attention for the increasing scale of the network. Multi-controller architectures can handle what SDN with single controller is not able to address. In order to understand what these architectures can accomplish and face precisely, we analyze them with formal methods. In this paper, we apply Communicating Sequential Processes (CSP) to model the routing service of SDN under multi-controller architectures, in particular the HyperFlow architecture and the Kandoo architecture based on OpenFlow protocol. By using model checker Process Analysis Toolkit (PAT), we verify that the models satisfy three properties, namely deadlock freeness, consistency, and fault tolerance. In addition, for studying the security of those models, some extension is added. We find that the extended models are capable of coping with Denial of Service and may suffer from Information Disclosure. Moreover, a fake path and a tampered message could be present in SDN.},
   author = {Lili Xiao and Huibiao Zhu and Shuangqing Xiang and Phan Cong Vinh},
   doi = {https://doi.org/10.1002/cpe.5334},
   issue = {2},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {CSP,Software Defined Networking (SDN),modeling,multiple controllers,verification},
   note = {e5334 cpe.5334},
   pages = {e5334},
   title = {Modeling and verifying SDN under Multi-controller architectures using CSP},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5334},
   year = {2021}
}
@article{Leotta2024-2,
   abstract = {Abstract A new advancement in test automation is the use of natural language processing (NLP) to generate test cases (or test scripts) from natural language text. NLP is innovative in this context and promises of reducing test cases creation time and simplifying understanding for “non-developer” software testers as well. Recently, many vendors have launched on the market many proposals of NLP-based tools and testing frameworks but their superiority has never been empirically validated. This paper investigates the adoption of NLP-based test automation in the web context with a series of case studies conducted to compare the costs of the NLP testing approach—measured in terms of test cases development and test cases evolution—with respect to more consolidated approaches, that is, programmable (or script-based) testing and capture&replay testing. The results of our study show that NLP-based test automation appears to be competitive for small- to medium-sized test suites such as those considered in our empirical study. It minimizes the total cumulative cost (development and evolution) and does not require software testers with programming skills.},
   author = {Maurizio Leotta and Filippo Ricca and Alessandro Marchetto and Dario Olianas},
   doi = {https://doi.org/10.1002/smr.2606},
   issue = {5},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Selenium,empirical study,natural language processing,page object pattern,test automation,web testing},
   pages = {e2606},
   title = {An empirical study to compare three web test automation approaches: NLP-based, programmable, and capture&replay},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2606},
   year = {2024}
}
@article{Camilli2020,
   abstract = {Summary Nowadays, there exists an increasing demand for reliable software systems able to fulfill their requirements in different operational environments and to cope with uncertainty that can be introduced both at design-time and at runtime because of the lack of control over third-party system components and complex interactions among software, hardware infrastructures and physical phenomena. This article addresses the problem of the discrepancy between measured data at runtime and the design-time formal specification by using an inverse uncertainty quantification approach. Namely, we introduce a methodology called METRIC and its supporting toolchain to quantify and mitigate software system uncertainty during testing by combining (on-the-fly) model-based testing and Bayesian inference. Our approach connects probabilistic input/output conformance theory with statistical hypothesis testing in order to assess if the behaviour of the system under test corresponds to its probabilistic formal specification provided in terms of a Markov decision process. An uncertainty-aware model-based test case generation strategy is used as a means to collect evidence from software components affected by sources of uncertainty. Test results serve as input to a Bayesian inference process that updates beliefs on model parameters encoding uncertain quality attributes of the system under test. This article describes our approach from both theoretical and practical perspectives. An extensive empirical evaluation activity has been conducted in order to assess the cost-effectiveness of our approach. We show that, under same effort constraints, our uncertainty-aware testing strategy increases the accuracy of the uncertainty quantification process up to 50 times with respect to traditional model-based testing methods.},
   author = {Matteo Camilli and Angelo Gargantini and Patrizia Scandurra},
   doi = {https://doi.org/10.1002/stvr.1730},
   issue = {2},
   journal = {Software Testing, Verification and Reliability},
   keywords = {bayesian inference,formal methods,model-based testing,probabilistic systems,uncertainty quantification},
   note = {e1730 stvr.1730},
   pages = {e1730},
   title = {Model-based hypothesis testing of uncertain software systems},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1730},
   year = {2020}
}
@article{Huang2019,
   abstract = {Test-case prioritisation (TCP) attempts to schedule the order of test-case execution such that faults can be detected as quickly as possible. TCP has been widely applied in many testing scenarios such as regression testing and fault localisation. Abstract test cases (ATCs) are derived from models of the system under test and have been applied to many testing environments such as model-based testing and combinatorial interaction testing. Although various empirical and analytical comparisons for some ATC prioritisation (ATCP) techniques have been conducted, to the best of the authors’ knowledge, no comparative study focusing on the most current techniques has yet been reported. In this study, they investigated 18 ATCP techniques, categorised into four classes. They conducted a comprehensive empirical study to compare 16 of the 18 ATCP techniques in terms of their testing effectiveness and efficiency. They found that different ATCP techniques could be cost-effective in different testing scenarios, allowing us to present recommendations and guidelines for which techniques to use under what conditions.},
   author = {Rubing Huang and Weiwen Zong and Tsong Yueh Chen and Dave Towey and Yunan Zhou and Jinfu Chen},
   doi = {https://doi.org/10.1049/iet-sen.2018.5199},
   issue = {4},
   journal = {IET Software},
   keywords = {ATC prioritisation techniques,ATCP techniques,TCP,combinatorial interaction testing,comprehensive empirical study,fault localisation,model-based testing,program testing,regression testing,software tools,test-case execution,test-case prioritisation,testing effectiveness},
   pages = {313-326},
   title = {Prioritising abstract test cases: an empirical study},
   volume = {13},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5199},
   year = {2019}
}
@article{Hu2021,
   abstract = {Summary This paper proposes an algebraic system, verification algebra (VA), for reducing the number of component combinations to be verified in multi-tenant architecture (MTA). MTA is a design architecture used in SaaS (Software-as-a-Service) where a tenant can customize its applications by integrating services already stored in the SaaS databases or newly supplied services. Similar to SaaS, VaaS (Verification-as-a-Service) is a verification service in a cloud that leverages the computing power offered by a cloud environment with automated provisioning, scalability and service composition. In VaaS architecture, however, there is a challenging problem called ‘combinatorial explosion’ that it is difficult to verify a large number of compositions constructed by both quantities of components and various combination structures even with computing resources in cloud. This paper proposes rules to emerge combinations status for future verification, on the basis of the existing results. Both composition patterns and properties are considered and analysed in VA rules.},
   author = {Kai Hu and Ji Wan and Kan Luo and Yuzhuang Xu and Zijing Cheng and Wei-Tek Tsai},
   doi = {https://doi.org/10.1002/stvr.1763},
   issue = {6},
   journal = {Software Testing, Verification and Reliability},
   keywords = {MTA,SaaS,combinatorial explosion,verification,workflow patterns},
   note = {e1763 stvr.1763},
   pages = {e1763},
   title = {Verification algebra for multi-tenant applications in VaaS architecture},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1763},
   year = {2021}
}
@article{Jawaddi2022,
   abstract = {Abstract The process of scaling microservices is a challenging task, especially in maintaining optimum resource provisioning while respecting QoS constraints and SLA. Many research works have proposed autoscaling approaches for microservices, however, less likely concerned with the correctness guarantee of the proposed algorithms. Hence, it is significant to gather and summarize these approaches to foster future innovation. Meanwhile, a few reviews have been published concerning microservices from different aspects. Therefore, our review complements the existing by focusing on autoscaling with verification perspectives. This study highlights the recent contributions in three inter-related main topics that were published within the year 2017 to 2022, namely, microservice, verification, and autoscaling. Due to limited resources on verification for microservice autoscaling, we widen the perspective by considering the verification for autoscaling in cloud-based systems. Based on our findings, we found that the formal method is not a new thing in verifying the autoscaling policies in cloud-based systems, and one recent study that implements the formal method in the microservices area has been identified. Apart from the autoscaling techniques, we have also determined several factors that have been a concern in scaling the microservices as well as the relatable metrics. Meanwhile, from a verification perspective, we identified that probabilistic model checking is the common formal verification technique used to verify microservices and cloud autoscaling. Finally, we recommend open challenges from two perspectives which highlight the verification for existing microservice autoscaling and verification for ML-based microservice autoscaling.},
   author = {Siti Nuraishah Agos Jawaddi and Muhammad Hamizan Johari and Azlan Ismail},
   doi = {https://doi.org/10.1002/spe.3135},
   issue = {11},
   journal = {Software: Practice and Experience},
   keywords = {autoscaling,formal verification,microservice,probabilistic model checking,scalability},
   pages = {2476-2495},
   title = {A review of microservices autoscaling with formal verification perspective},
   volume = {52},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3135},
   year = {2022}
}

@article{Terragni2021,
   abstract = {Summary Concurrency testing is an important activity to expose concurrency faults in thread-safe classes. A concurrent test for a thread-safe class is a set of method call sequences that exercise the public interface of the class from multiple threads. Automatically generating fault-revealing concurrent tests within an affordable time budget is difficult due to the huge search space of possible concurrent tests. In this paper, we present DepCon+, a novel approach that reduces the search space of concurrent tests by leveraging statically computed dependencies among public methods. DepCon+ exploits the intuition that concurrent tests can expose thread-safety violations that manifest exceptions or deadlocks, only if they exercise some specific method dependencies. DepCon+ provides an efficient way to identify such dependencies by statically analysing the code and relies on the computed dependencies to steer the test generation towards those concurrent tests that exhibit the computed dependencies. We developed a prototype DepCon+ implementation for Java and evaluated the approach on 19 known concurrency faults of thread-safe classes that lead to thread-safety violations of either exception or deadlock type. The results presented in this paper show that DepCon+ is more effective than state-of-the-art approaches in exposing the concurrency faults. The search space pruning of DepCon+ dramatically reduces the search space of possible concurrent tests, without missing any thread-safety violations.},
   author = {Valerio Terragni and Mauro Pezzè},
   doi = {https://doi.org/10.1002/stvr.1774},
   issue = {4},
   journal = {Software Testing, Verification and Reliability},
   keywords = {concurrency,deadlocks,non-determinism,race conditions,test generation,thread safety},
   note = {e1774 stvr.1774},
   pages = {e1774},
   title = {Statically driven generation of concurrent tests for thread-safe classes},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1774},
   year = {2021}
}
@article{Hashim2022,
   abstract = {Nowadays, the verification of handwritten signatures has become an effective research field in computer vision as well as machine learning. Signature verification is naturally formulated as a machine-learning task. This task is performed by determining if the signature is genuine or forged. Therefore, it is considered a two-class classification issue. Since handwritten signatures are widely used in legal documents and financial transactions, it is important for researchers to select an efficient machine-learning technique for verifying these signatures and to avoid forgeries that may cause many losses to customers. So far, great outcomes have been obtained when using machine learning techniques in terms of equal error rates and calculations. This paper presents a comprehensive review of the latest studies and results in the last 10 years in the field of online and offline handwritten signature verification. More than 20 research papers were used to make a comparison between datasets, feature extraction, and classification techniques used in each system, taking into consideration the problems that occur in each. In addition, the general limitations and advantages of machine-learning techniques that are used to classify or extract signature features were summarized in the form of a table. We also present the general steps of the verification system and a list of the most considerable datasets available in online and offline fields.},
   author = {Zainab Hashim and Hanaa M Ahmed and Ahmed Hussein Alkhayyat},
   doi = {https://doi.org/10.1155/2022/8170424},
   issue = {1},
   journal = {Scientific Programming},
   pages = {8170424},
   title = {A Comparative Study among Handwritten Signature Verification Methods Using Machine Learning Techniques},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/8170424},
   year = {2022}
}
@inbook{Choudhary2023,
   abstract = {Summary Multimodal biometric verification in the secured automated certificate creation system is crucial to its security and accuracy. The system can verify the user's identity with high accuracy and reliability by utilizing multiple biometric characteristics, such as facial, fingerprint, and voice recognition. Each biometric element provides a unique identifier that is difficult to replicate or fake, reducing the risk of fraud or impersonation in the certificate creation process. One of the main steps in this system is the collection of candidate face images and fingerprints in a secure format, such as tif, jpg, or png. The collected images are then used as a base to create unique certificates for each candidate using software that retains the underlying characteristics of the image. Data are stored in xls and xlsx formats to ensure the accuracy and security of the certificate creation process. MS Excel is used as it offers mathematical and statistical tools that can be utilized for internal computation and analysis of data within the file. This approach helps automate the certificate creation process, ensuring high accuracy, security, and data integrity. Overall, this system provides a more efficient and reliable certificate creation solution that reduces administrators’ workload and minimizes the potential for errors.},
   author = {Shilpa Choudhary and Sandeep Kumar and Monali Gulhane and Munish Kumar},
   doi = {https://doi.org/10.1002/9781119785491.ch13},
   isbn = {9781119785491},
   booktitle = {Multimodal Biometric and Machine Learning Technologies},
   keywords = {Face recognition,Haar features,chaff features,excel,finger recognition},
   pages = {269-281},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Secured Automated Certificate Creation Based on Multimodal Biometric Verification},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119785491.ch13},
   year = {2023}
}
@article{Ponugoti2023,
   abstract = {Abstract Transient execution attacks such as Spectre and Meltdown exploit speculative execution in modern microprocessors to leak information via cache side-channels. Software solutions to defend against many transient execution attacks employ the lfence serialising instruction, which does not allow instructions that come after the lfence to execute out-of-order with respect to instructions that come before the lfence. However, errors and Trojans in the hardware implementation of lfence can be exploited to compromise the software mitigations that use lfence. The aforementioned security gap has not been identified and addressed previously. The authors provide a formal method solution that addresses the verification of lfence hardware implementation. The authors also show how hardware Trojans can be designed to circumvent lfence and demonstrate that their verification approach will flag such Trojans as well. The authors have demonstrated the efficacy of our approach using RSD, which is an open source RISC-V based superscalar out-of-order processor.},
   author = {Kushal K Ponugoti and Sudarshan K Srinivasan and Nimish Mathure},
   doi = {https://doi.org/10.1049/cdt2.12058},
   issue = {3-4},
   journal = {IET Computers and Digital Techniques},
   keywords = {computer architecture,electronic design automation,formal verification,integrated circuit design,logic design,microcomputers,microprocessor chips},
   pages = {127-140},
   title = {Verification of serialising instructions for security against transient execution attacks},
   volume = {17},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cdt2.12058},
   year = {2023}
}
@article{Dvila2020,
   abstract = {Nowadays, information technology services management (ITSM) has become strongly needed for every kind of organisations providing IT services for customers or for themselves. However, existing models (as CMMI-SVC, ITIL or ISO/IEC 20000) are strongly difficult to implement on very small organisations. The aim of this study is to validate an ITSM model specifically proposed for small organisations, the PCPS4SVC model. This model was developed considering the needs and constraints of small organisations and was verified against reference models using process mapping. In this study, the model was validated in three small enterprises, where it was tailored and implemented. As a result, the enterprises improved their IT services operation. Finally, it was determined than PCPS4SVC is a model easy-to-use and adequate for small enterprises.},
   author = {Abraham Dávila and Rosanna Janampa and Paula Angeleri and Karin Melendez},
   doi = {https://doi.org/10.1049/iet-sen.2019.0034},
   issue = {2},
   journal = {IET Software},
   keywords = {CMMI-SVC,Capability Maturity Model,DP industry,IEC standards,ISO standards,ISO/IEC 2000,IT services operation,ITIL,ITSM model,PCPS4SVC model,empirical validation,information technology,information technology services management,process mapping,quality management,reference models,small enterprises,very small organisation},
   pages = {138-144},
   title = {ITSM model for very small organisation: an empirical validation},
   volume = {14},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2019.0034},
   year = {2020}
}
@article{FerreiraVilela2023,
   abstract = {Summary Concurrent programming is increasingly present in modern applications. Although it provides higher performance and better use of available resources, the mechanisms of interaction between processes/threads result in a greater challenge for software testing activity. The nondeterminism present in those applications is one of the main issues during the test activity since the same test input can produce different possible execution paths, which may or not contain defects. The test data automatic generation can alleviate this problem, ensuring higher speed and reliability in software testing activity. This paper explores the automatic test data generation for concurrent programs through Genetic Algorithm, a bioinspired optimization technique, and proposes a test data generation approach for concurrent programs, called BioConcST, and a new operator for the selection of test subjects, called FuzzyST, which uses fuzzy logic. The approaches were evaluated in an experimental study towards their validation. The results showed that BioConcST is more promising than the other approaches at all analyzed levels. FuzzyST, together with Elitism and Tournament operators, provided the best results; however, it proved more suitable for concurrent programs of higher complexity.},
   author = {Ricardo Ferreira Vilela and João Choma Neto and Victor Hugo Santiago Costa Pinto and Paulo Sérgio de Souza and Simone do Rocio Senger de Souza},
   doi = {https://doi.org/10.1002/cpe.7489},
   issue = {2},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {concurrent software testing,search-based software testing,test data generation},
   pages = {e7489},
   title = {Bio-inspired optimization to support the test data generation of concurrent software},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7489},
   year = {2023}
}
@article{George2021,
   abstract = {Abstract Although in industry and academia cloud computing provides several services, cloud storage is the most unavoidable one. Cloud storage allows remote data validation of the deployed user's data without keeping a local copy. A third-party auditor is delegated by the users in order to examine the remote data integrity thereby reducing the burden of the users. Data integrity checking protects the user's data from tinkering and illicit access especially when they store data in public clouds such as AWS. Various schemes were proposed by several scholars which possess computational overhead and communication overhead during the integrity checking. Most of the schemes proposed were possessing computational overhead in the cloud server side which increases the financial overwhelming of the clients. In this article an improved multiparty computation architecture is proposed to minimize the computational overhead in the cloud server side while generating the proof of the data blocks in public clouds. The proposed scheme can be implemented on semi-trusted cloud environments and allows a set of server nodes called ingestion nodes to compute on private inputs thus preserving the privacy and soundness of the computing data. The improved protocol also uses an indistinguishability obfuscation program with a message authentication code tag to minimize the verification overhead of the third-party auditor as well as preserves the privacy and security of the stored data.},
   author = {Anju Susan George and A Shajin Nargunam},
   doi = {https://doi.org/10.1002/cpe.6427},
   issue = {21},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {cloud computing,cloud storage,data integrity,indistinguishability obfuscation,multiparty computation},
   pages = {e6427},
   title = {Improved multi-party verification protocol with reduced computational overhead in cloud storage system},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6427},
   year = {2021}
}
@article{Bonfanti2020,
   abstract = {Abstract According to best practices of model-driven engineering, the implementation of a system should be obtained from its model through a systematic model-to-code transformation. We present in this paper a methodology supported by the Asm2C++ tool, which allows the users to generate C++ code from abstract state machine models. Thanks to Asm2C++, the implementation is generated in a seamless manner with an assurance of potential bug freeness of the generated code. Following the same approach, model-based testing suggests deriving also (unit) tests from abstract models. We extend the Asm2C++ tool such that it can automatically produce unit tests for the generated code. Abstract test sequences, either generated randomly or through model checking, are translated to concrete C++ unit tests using the Boost library. In a similar manner, also, scenarios are generated in a behavior-driven development (BDD) approach. To guarantee the correctness of the transformation process, we define a mechanism to test the correctness of the model-to-code transformation with respect to two main criteria: syntactical correctness and semantic correctness, which is based on the definition of conformance between the specification and the code. Using this approach, we have devised a process able to test the generated code by reusing unit tests. The process has been used to validate our model-to-code transformations.},
   author = {Silvia Bonfanti and Angelo Gargantini and Atif Mashkoor},
   doi = {https://doi.org/10.1002/smr.2205},
   issue = {2},
   journal = {Journal of Software: Evolution and Process},
   keywords = {C++,abstract state machine,automatic code generation,model-driven engineering,transformation validation,unit tests generation},
   note = {e2205 smr.2205},
   pages = {e2205},
   title = {Design and validation of a C++ code generator from Abstract State Machines specifications},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2205},
   year = {2020}
}
@article{DiMartino2023,
   abstract = {Abstract The adoption of the business process model notation for the description of internal workflows and procedures by both public administrations and private organizations is steadily growing, thanks to the simplicity of the standard and its consistent expressivity. However, the lack of semantic support from BPMN can pose important limitations to its efficient use, as ambiguities in task definitions and communications can lead to misinterpretations. Furthermore, there is the need to validate the BPMNs, to check their adherence to regulations, especially in public administration, and to verify their conformance to security and privacy constraints. In this work, we present SemPrAnn, a semantic annotation tool for BPMN that, exploiting domain ontologies and logical rules, provides the possibility to unambiguously identify concepts in workflows and to run inferential engines against them to enforce the rules. The manuscript presents the methodology applied for the implementation of the tool, the tool itself with its exposed functionalities, and a case study demonstrating its current capabilities.},
   author = {Beniamino Di Martino and Luigi Colucci Cante and Antonio Esposito and Mariangela Graziano},
   doi = {https://doi.org/10.1002/spe.3184},
   issue = {5},
   journal = {Software: Practice and Experience},
   keywords = {Semantic Web,business process model notation,expert system,process validation},
   pages = {1174-1195},
   title = {A tool for the semantic annotation, validation and optimization of business process models},
   volume = {53},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3184},
   year = {2023}
}
@article{Chen2024-1,
   abstract = {Abstract Blockchain smart contracts are codes that can execute and enforce rules for blockchain digital transactions. However, smart contracts may contain numerous subtle vulnerabilities, among which Ponzi vulnerabilities are notable. Existing Ponzi scheme contract detection approaches often rely on machine learning models trained on manually extracted features to achieve satisfactory classification results. Nonetheless, the code of a smart contract potentially harbours elusive semantics and characteristics, which compromises the precision and accuracy of vulnerability detection. Therefore, this paper proposes a method of converting operation codes into sequences to process data to avoid losing unnecessary important information, and uses a one-dimensional convolutional neural network combined with formal verification. This method is named PZ-C1DZ3(Ponzi-Conv1D-Z3) and is used for Ponzi scheme detection. Four types of machine learning models, namely Conv1D, Conv1D-LSTM, Conv1D-MLP, and Conv1D-transformer, are employed for improvement and comparative validation experiments. Additionally, formal verification tool Z3 solver is utilized to conduct formal security verification on the final model, ensuring its safety. Experimental results demonstrate that the improved Conv1D model outperforms other existing models in terms of detection efficiency and accuracy while also meeting the requirements of formal security verification.},
   author = {Shibao Chen and Fei Li},
   doi = {https://doi.org/10.1049/blc2.12056},
   issue = {2},
   journal = {IET Blockchain},
   keywords = {artificial intelligence,blockchains,contracts},
   pages = {185-196},
   title = {Ponzi scheme detection in smart contracts using the integration of deep learning and formal verification},
   volume = {4},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/blc2.12056},
   year = {2024}
}
@article{Costello2020,
   abstract = {Abstract People respond to illness in a range of ways, and take different approaches to engaging with health information throughout the course of their illness. This study describes and explains the variety of approaches to health information interactions made by patients on hemodialysis. Ethnographic observations (156 hours) were conducted in three hemodialysis clinics, and semistructured interviews about health information were held with 28 patients. Demographic data were collected. Data were analyzed qualitatively. We found a spectrum of five approaches to health information: avoiders, who close themselves off from health information; receivers, who encounter information in the dialysis clinic but do not seek it out; askers, who only pose questions about health to their healthcare providers but otherwise do not seek; seekers, who actively look for health information both in and out of the clinic; and verifiers, who seek information and triangulate it among multiple sources. Trust in healthcare providers and coping sociality differed across approaches. The findings indicate that health information should be provided to patients using strategies tailored to their preferences and existing approaches to information interaction.},
   author = {Kaitlin L Costello and Tiffany C Veinot},
   doi = {https://doi.org/10.1002/asi.24310},
   issue = {8},
   journal = {Journal of the Association for Information Science and Technology},
   pages = {871-886},
   title = {A spectrum of approaches to health information interaction: From avoidance to verification},
   volume = {71},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24310},
   year = {2020}
}

@article{Verma2024,
   abstract = {Abstract This study presents a significant advancement in the realm of information science by exploring and validating a semiotic scale tailored for library software interfaces. Employing a design-based approach, the investigation identifies 14 crucial semiotic elements specific to library software interfaces, forming the cornerstone of its theoretical framework. The process of developing the semiotic scale involves creating user personas, conducting a comprehensive semiotic analysis, and engaging in productive collaboration with experts. Through meticulous evaluation and refinement, achieved by expert assessments and user testing, the scale offers actionable insights for guiding interface enhancement. The study's findings encompass a breadth of revelations, including user needs, user personas, outcomes of semiotic analysis, criteria development, and evaluation results across a diverse range of library software interfaces. Serving as a valuable tool for interface designers, the semiotic scale facilitates the alignment of design choices with user preferences and requirements. Its inherent adaptability ensures applicability across a spectrum of library software interfaces. The study's paramount contribution lies in bridging the gap between theoretical semiotics and practical design considerations. The comprehensive model offered by the development and validation of the semiotic scale empowers the evaluation of semiotic elements' impact on user perceptions and interface usability.},
   author = {Manoj Kumar Verma and Vinit Kumar and Mayank Yuvaraj},
   doi = {https://doi.org/10.1002/asi.24875},
   issue = {6},
   journal = {Journal of the Association for Information Science and Technology},
   pages = {704-716},
   title = {Semiotic scale for library software interfaces: Development and validation},
   volume = {75},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24875},
   year = {2024}
}
@article{Cerioli2023,
   abstract = {Abstract Tests, when not correctly implemented, can pass on incorrect system implementations rather than fail. In this case, they are named silent horrors or false-negative tests. They make releasing low-quality (buggy) versions of the software system more probable. Furthermore, faithfully implementing test specifications is crucial when they play the role of documentation, like when documenting components or services or driving legacy systems' re-engineering. This paper presents TestWizard, a novel approach and tool for automatically assessing individual tests' quality from the point of view of their coherence to specifications. TestWizard automatically assesses the quality of each individual test case w.r.t. its specification, providing detailed reports on why a single test is a false negative, hence helping testers fix them. Thus, TestWizard can help to automate the test code review process, which is still mainly manual today. The analysis of 1012 test implementations, developed by 123 students in three experiments, shows that TestWizard is (1) by far more accurate than code review performed by multiple students, (2) slightly better than code review performed by three senior experts, and (3) always able to detect a significant percentage of false-negative test methods (up to 21.22\%).},
   author = {Maura Cerioli and Giovanni Lagorio and Maurizio Leotta and Filippo Ricca},
   doi = {https://doi.org/10.1002/smr.2396},
   issue = {4},
   journal = {Journal of Software: Evolution and Process},
   keywords = {automated testing,code review,false-negative unit tests,test code,test quality,test specifications},
   pages = {e2396},
   title = {Fight silent horror unit test methods by consulting a TestWizard},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2396},
   year = {2023}
}
@inbook{Singh2023,
   abstract = {Summary Software Testing consumes very significant amount of time in the life cycle of software. Test case prioritization is a way to provide priorities to test cases, to meet various testing goals. This study reports a systematic literature review of prioritization techniques. The survey adheres to the guidelines of eminent researchers in the field of software engineering. This survey is based on a review of 312 articles selected from leading research journals and proceedings of premier conferences. The survey provides a deep insight into the area of prioritization and highlights 22 different techniques that have been emerged in the past. The study not only throws light on different possible techniques for prioritization but also presents a set of applicable tools and subject systems in this domain. The survey also has a specific focus on prioritization in model-based testing and object-oriented testing since these two paradigms have become popular among the researchers. The study concludes that the field of prioritization has considerably been explored and many prioritization techniques have evolved. But still, there are possibilities of improvements, especially in implementation and analysis. The study also highlights the current status of prioritization and provides comparative analysis with similar works. Results presented in this survey would benefit the researchers to gain knowledge of the field of prioritization in general, and object-oriented prioritization, in particular. This review follows the guidelines of eminent researchers like Kitchenham et al . [1, 2] and Budgen et al . [3].},
   author = {Ajmer Singh and Anita Singhrova and Rajesh Bhatia and Dhavleesh Rattan},
   doi = {https://doi.org/10.1002/9781119896838.ch7},
   isbn = {9781119896838},
   booktitle = {Agile Software Development},
   keywords = {Test case prioritization,software testing,systematic review,test case prioritization techniques},
   pages = {101-159},
   publisher = {John Wiley \& Sons, Ltd},
   title = {A Systematic Literature Review on Test Case Prioritization Techniques},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119896838.ch7},
   year = {2023}
}
@article{Nagy2023,
   abstract = {Abstract Background Item response times in computerized assessments are frequently used to identify rapid guessing behaviour as a manifestation of response disengagement. However, non-rapid responses (i.e., with longer response times) are not necessarily engaged, which means that response-time-based procedures could overlook disengaged responses. Therefore, the identification of disengaged responses could be improved by considering additional indicators of disengagement. We investigated the extent to which decreases in individuals' item solution probabilities over the course of a test reflect disengaged response behaviour. Objectives To disentangle different types of possibly disengaged responses and better understand non-effortful test-taking behaviour, we augmented responses-time-based procedures for identifying rapid guessing with strategies for detecting disengaged responses on the basis of performance declines in non-rapid responses. Methods We combined item response theory (IRT) models for rapid guessing and test-taking persistence to examine the capability of response times and item positions to capture response disengagement. We used a computerized assessment in which science items were randomly distributed across positions for each student. This allowed us to estimate individual differences in test-taking persistence (i.e., the duration for which the initial level of performance is maintained) while accounting for rapid responses. Results and Conclusions Response times did not fully explain disengagement; item responses reflected test-taking persistence even when rapid responses were accounted for. This interpretation was supported by a strong correlation of test-taking persistence with decreases in self-reported test-taking effort. Furthermore, our results suggest that IRT models for test-taking persistence can effectively account for the undesirable impact of low test-taking effort even when response times are unavailable.},
   author = {Gabriel Nagy and Esther Ulitzsch and Marlit Annalena Lindner},
   doi = {https://doi.org/10.1111/jcal.12719},
   issue = {3},
   journal = {Journal of Computer Assisted Learning},
   keywords = {item response theory,process data,rapid guessing behaviour,response time,test-taking engagement,test-taking persistence},
   pages = {751-766},
   title = {The role of rapid guessing and test-taking persistence in modelling test-taking engagement},
   volume = {39},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12719},
   year = {2023}
}
@article{Mrtensson2023,
   abstract = {Abstract The characteristics of the test environment are of vital importance to its ability to support the organizations testing objectives. This paper seeks to address the need for a structured and reliable approach, which can be used by companies and other organizations to optimize their test environments in each individual case. The reported study included a series of interviews with 30 individuals, a series of focus groups with in total 31 individuals and a cross-company workshop with 30 participants from five large-scale companies, operating in different industry segments. The study resulted in a list of success factors, including not only characteristics and capabilities existing within a test environment (intrinsic success factors) but also properties not inherent to the test environment, but still vital for a successfully implemented test environment (extrinsic success factors). This distinction is important, as the root causes differ and as addressing them requires distinct approaches—not only of technology but also of organization, communication and collaboration. We find that successful implementations of test environments for large-scale software systems depend primarily on how they support the company's business strategy, test organization and product testability (extrinsic success factors). Based on this, test environments can then be optimized to improve test environment capabilities, usability and stability (intrinsic success factors). The list of intrinsic and extrinsic success factors was well received by all five companies included in the study, supporting that the intrinsic and extrinsic success factors for test environments can be applied to a large segment of the software industry.},
   author = {Torvald Mårtensson and Göran Ancher and Daniel Ståhl},
   doi = {https://doi.org/10.1002/stvr.1839},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {continuous delivery,continuous integration,large-scale systems,software testing,test environment},
   pages = {e1839},
   title = {Test environments for large-scale software systems—An industrial study of intrinsic and extrinsic success factors},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1839},
   year = {2023}
}
@article{ZieglerHaselein2022,
   abstract = {Abstract Electric motors are highly customizable products that have to meticulously comply with the client's needs, commercial and legal requirements of each market. In this context, the configure-to-order (CTO) approach allows the user to define the product configuration at the time of the order and the supplier to develop a product that meets the customer's needs. This paper presents an object-oriented knowledge-based system (OOKBS) that integrates rule- and case-based reasoning to verify the compliance of a variant-rich and complex product—electric motors—in a multinational-level company. The system adopts a modular structure to evaluate distinct compliance aspects, such as technical constraints and commercial requirements, and to improve the compliance assessment based on the designs of motors previously sold. The work targeted a product line with significant market share in North America, and is the result of a collaboration among the following teams: product compliance, international sales, engineering systems, and product engineering. In total, the system development involved nine experts from these areas. This study has the originality of presenting a product configuration system (PCS) that integrates rule-based and case-based approaches to verify the compliance of a modular product. The results indicate a reduction of 73\% of internal technical queries within the prototype scope. Moreover, the system usability tests highlight the completeness of outcomes, quick access to information, and easy integration of automatic product compliance verification into the company's design flow. Furthermore, we discussed how the system was effectively implemented in a design routine based on both concurrent engineering and CTO scenarios.},
   author = {Bruno Ziegler Haselein and Jonny Carlos da Silva},
   doi = {https://doi.org/10.1111/exsy.12979},
   issue = {7},
   journal = {Expert Systems},
   keywords = {compliance,electric motor,knowledge-based system,product configuration system},
   pages = {e12979},
   title = {A knowledge-based system for electric motors compliance verification in a multinational-level company},
   volume = {39},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12979},
   year = {2022}
}
@article{Koroglu2021,
   abstract = {Summary With the ever-growing Android graphical user interface (GUI) application market, there have been many studies on automated test generation for Android GUI applications. These studies successfully demonstrate how to detect fatal exceptions and achieve high coverage with fully automated test generation engines. However, it is unclear how many GUI functions these engines manage to test. The current best practice for the functional testing of Android GUI applications is to design user interface (UI) test scenarios with a non-technical and human-readable language such as Gherkin and implement Java/Kotlin methods for every statement of all the UI test scenarios. Writing tests for UI test scenarios is hard, especially when some scenario statements are high-level and declarative, so it is not clear what actions should the generated test perform. We propose the Fully Automated Reinforcement LEArning-Driven specification-based test generator for Android (FARLEAD-Android). FARLEAD-Android first translates the UI test scenario to a GUI-level formal specification as a linear-time temporal logic (LTL) formula. The LTL formula guides the test generation and acts as a specified test oracle. By dynamically executing the application under test (AUT), and monitoring the LTL formula, FARLEAD-Android learns how to produce a witness for the UI test scenario, using reinforcement learning (RL). Our evaluation shows that FARLEAD-Android is more effective and achieves higher performance in generating tests for UI test scenarios than three known engines: Random, Monkey and QBEa. To the best of our knowledge, FARLEAD-Android is the first fully automated mobile GUI testing engine that uses formal specifications.},
   author = {Yavuz Koroglu and Alper Sen},
   doi = {https://doi.org/10.1002/stvr.1752},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {mobile applications,reinforcement learning,software testing,temporal logic,test oracles,test scenarios},
   note = {e1752 stvr.1752},
   pages = {e1752},
   title = {Functional test generation from UI test scenarios using reinforcement learning for android applications},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1752},
   year = {2021}
}
@article{Corradini2022,
   abstract = {Abstract RESTful APIs (or REST APIs for short) represent a mainstream approach to design and develop web APIs using the REpresentational State Transfer architectural style. Black-box testing, which assumes only the access to the system under test with a specific interface, is the only viable option when white-box testing is impracticable. This is the case for REST APIs: their source code is usually not (or just partially) available, or a white-box analysis across many dynamically allocated distributed components (typical of a micro-services architecture) is computationally challenging. This paper presents RestTestGen, a novel black-box approach to automatically generate test cases for REST APIs, based on their interface definition (an OpenAPI specification). Input values and requests are generated for each operation of the API under test with the twofold objective of testing nominal execution scenarios and error scenarios. Two distinct oracles are deployed to detect when test cases reveal implementation defects. While this approach is mainly targeting the research community, it is also of interest to developers because, as a black-box approach, it is universally applicable across different programming languages, or in the case external (compiled only) libraries are used in a REST API. The validation of our approach has been performed on more than 100 of real-world REST APIs, highlighting the effectiveness of the approach in revealing actual faults in already deployed services.},
   author = {Davide Corradini and Amedeo Zampieri and Michele Pasqua and Emanuele Viglianisi and Michael Dallago and Mariano Ceccato},
   doi = {https://doi.org/10.1002/stvr.1808},
   issue = {5},
   journal = {Software Testing, Verification and Reliability},
   keywords = {REST APIs,automatic test case generation,black-box testing,test oracle},
   pages = {e1808},
   title = {Automated black-box testing of nominal and error scenarios in RESTful APIs},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1808},
   year = {2022}
}
@article{El-Fakih2022,
   abstract = {Summary Tests can be derived from extended finite state machine (EFSM) specifications considering the coverage of single-transfer faults, all transitions using a transition tour, all-uses, edge-pair, and prime path with side trip. We provide novel empirical assessments of the effectiveness of these test suites. The first assessment determines for each pair of test suites if there is a difference between the pair in covering EFSM faults of six EFSM specifications. If the difference is found significant, we determine which test suite outperforms the other. The second assessment is similar to the first; yet, it is carried out against code faults of 12 Java implementations of the specifications. Besides, two assessments are provided to determine whether test suites have better coverage of certain classes of EFSM (or code) faults than others. The evaluation uses proper data transformation of mutation scores and p-value adjustments for controlling Type I error due to multiple tests. Furthermore, we show that subsuming mutants have an impact on mutation scores of both EFSM and code faults; and accordingly, we use a score that removes them in order not to invalidate the obtained results. The assessments show that all-uses tests were outperformed by all other tests; transition tours outperformed both edge-pair and prime path with side trips; and single-transfer fault tests outperformed all other test suites. Similar results are obtained over the considered EFSM and code fault domains, and there were no significant differences between the test suites coverage of different classes of EFSM and code faults.},
   author = {K El-Fakih and Ayman Alzaatreh and Uraz Cengiz Türker},
   doi = {https://doi.org/10.1002/stvr.1789},
   issue = {7},
   journal = {Software Testing, Verification and Reliability},
   keywords = {empirical assessment,extended finite state machines,model-based testing,mutation scores,mutation testing},
   pages = {e1789},
   title = {Assessing test suites of extended finite state machines against model- and code-based faults},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1789},
   year = {2022}
}
@article{Kulczynski2023,
   abstract = {Abstract The increased interest in string solving in the recent years has made it very hard to identify the right tool to address a particular user's purpose. Firstly, there is a multitude of string solvers, each addressing essentially some subset of the general problem. Generally, the addressed fragments are relevant and well motivated, but the lack of comparisons between the existing tools on an equal set of benchmarks cannot go unnoticed, especially as a common framework to compare solvers seems to be missing. In this paper, we gather a set of relevant benchmarks and introduce our new benchmarking framework to address this purpose.},
   author = {Mitja Kulczynski and Florin Manea and Dirk Nowotka and Danny Bøgsted Poulsen},
   doi = {https://doi.org/10.1002/smr.2400},
   issue = {4},
   journal = {Journal of Software: Evolution and Process},
   keywords = {analysis of string solvers,string solving benchmarks,test framework for string solvers},
   pages = {e2400},
   title = {ZaligVinder: A generic test framework for string solvers},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2400},
   year = {2023}
}
@article{Chavoshi2023,
   abstract = {Abstract Some types of software systems, like event-based and non-deterministic ones, are usually specified as rules so that we can analyse the system behaviour by drawing inferences from firing the rules. However, when the fuzzy rules are used for the specification of non-deterministic behaviour and they contain a large number of variables, they constitute a complex form that is difficult to understand and infer. A solution is to visualise the system specification with the capability of automatic rule inference. In this study, by representing a high-level system specification, the authors visualise rule representation and firing using fuzzy coloured Petri-nets. Already, several fuzzy Petri-nets-based methods have been presented, but they either do not support a large number of rules and variables or do not consider significant cases like (a) the weight of the premise's propositions in the occurrence of the rule conclusion, (b) the weight of conclusion's proposition, (c) threshold values for premise and conclusion's propositions of the rule, and (d) the certainty factor (CF) for the rule or the conclusion's proposition. By considering cases (a)–(d), a wider variety of fuzzy rules are supported. The authors applied their model to the analysis of attacks against a part of a real secure water treatment system. In another real experiment, the authors applied the model to the two scenarios from their previous work and analysed the results.},
   author = {Mina Chavoshi and Seyed Morteza Babamir},
   doi = {https://doi.org/10.1049/cit2.12251},
   issue = {3},
   journal = {CAAI Transactions on Intelligence Technology},
   keywords = {fuzzy logic,software engineering,verification},
   pages = {863-879},
   title = {Fuzzy coloured petri nets-based method to analyse and verify the functionality of software},
   volume = {8},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12251},
   year = {2023}
}
@article{Bombarda2023,
   abstract = {Abstract In this paper, we present an approach to conformance testing based on abstract state machines (ASMs) that combines model refinement and test execution (RATE) and its application to three case studies. The RATE approach consists in generating test sequences from ASMs and checking the conformance between code and models in multiple iterations. The process follows these steps: (1) model the system as an abstract state machine; (2) validate and verify the model; (3) generate test sequences automatically from the ASM model; (4) execute the tests over the implementation and compute the code coverage; (5) if the coverage is below the desired threshold, then refine the abstract state machine model to add the uncovered functionalities and return to step 2. We have applied the proposed approach in three case studies: a traffic light control system (TLCS), the IEEE 11073-20601 personal health device (PHD) protocol, and the mechanical ventilator Milano (MVM). By applying RATE, at each refinement level, we have increased code coverage and identified some faults or conformance errors for all the case studies. The fault detection capability of RATE has also been confirmed by mutation analysis, in which we have highlighted that, many mutants can be killed even by the most abstract models.},
   author = {Andrea Bombarda and Silvia Bonfanti and Angelo Gargantini and Yu Lei and Feng Duan},
   doi = {https://doi.org/10.1002/stvr.1835},
   issue = {2},
   journal = {Software Testing, Verification and Reliability},
   keywords = {Asmeta,IEEE 11073 PHD protocol,abstract state machine,formal Method,mechanical ventilator,model-based testing,refinement,test execution,testing implementation},
   pages = {e1835},
   title = {RATE: A model-based testing approach that combines model refinement and test execution},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1835},
   year = {2023}
}
@article{Lonetti2022,
   abstract = {Abstract In the early stages of a system of systems (SoS) conception, several constituent systems could be available that provide similar functionalities. An SoS design methodology should provide adequate means to model variability in order to support the opportunistic selection of the most desirable SoS configuration. We propose the VANTESS approach that (i) supports SoS modeling taking into account the variation points implied by the considered constituent systems; (ii) includes a heuristics to weight benefits and costs of potential architectural choices (called as SoS variants) for the selection of the constituent systems; and finally (iii) also helps test planning for the selected SoS variant by deriving a simulation model on which test objectives and scenarios can be devised. We illustrate an application example of VANTESS to the “educational” SoS and discuss its pros and cons within a focus group.},
   author = {Francesca Lonetti and Vânia de Oliveira Neves and Antonia Bertolino},
   doi = {https://doi.org/10.1002/smr.2427},
   issue = {10},
   journal = {Journal of Software: Evolution and Process},
   keywords = {design,software product line,system of systems,test case generation,testing,variability model},
   pages = {e2427},
   title = {Designing and testing systems of systems: From variability models to test cases passing through desirability assessment},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2427},
   year = {2022}
}
@article{Wu2022,
   abstract = {This study constructs a cloud computing-based college English multimedia test question modeling and application through an in-depth study of cloud computing and college English multimedia test questions. The emergence of cloud computing technology undoubtedly provides a new and ideal method to solve test data and paper management problems. This study analyzes the advantages of the Hadoop computing platform and MapReduce computing model and builds a distributed computing platform based on Hadoop using universities’ existing hardware and software resources. The study analyzes the advantages of the Hadoop computing platform and the MapReduce computing model. The UML model of the system is given, the system is implemented, the system is tested functionally, and the results of the analysis are given. Multimedia is the critical link to realizing the optimization of English test questions. The proper use of multimedia test questions will undoubtedly become an inevitable trend in the development of English test questions in the future, which requires every worker on the education front to continuously analyze and study the problems arising from multimedia teaching, summarize the experience of multimedia teaching, and explore new methods of multimedia teaching, so that multimedia teaching can better promote the optimization of English test questions in colleges and universities and better serve the education teaching.},
   author = {Yanping Wu and Changlong Zheng and Lele Xie and Meihui Hao},
   doi = {https://doi.org/10.1155/2022/4563491},
   issue = {1},
   journal = {Computational Intelligence and Neuroscience},
   pages = {4563491},
   title = {[Retracted] Cloud-Based English Multimedia for Universities Test Questions Modeling and Applications},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/4563491},
   year = {2022}
}
@article{Simons2020,
   abstract = {Summary The Stream X-Machine (SXM) testing method provides strong and repeatable guarantees of functional correctness, up to a specification. These qualities make the method attractive for software certification, especially in the domain of brokered cloud services, where arbitrage seeks to substitute functionally equivalent services from alternative providers. However, practical obstacles include the difficulty in providing a correct specification, the translation of abstract paths into feasible concrete tests and the large size of generated test suites. We describe a novel SXM verification and testing method, which automatically checks specifications for completeness and determinism, prior to generating complete test suites with full grounding information. Three optimization steps achieve up to a 10-fold reduction in the size of the test suite, removing infeasible and redundant tests. The method is backed by a set of tools to validate and verify the SXM specification, generate technology-agnostic test suites and ground these in SOAP, REST or rich-client service implementations. The method was initially validated using seven specifications, three cloud platforms and five grounding strategies.},
   author = {Anthony J H Simons and Raluca Lefticaru},
   doi = {https://doi.org/10.1002/stvr.1729},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {X-machines,cloud computing,cloud service broker,functional testing,service certification,specification,state-based testing,test grounding,verification},
   note = {e1729 stvr.1729},
   pages = {e1729},
   title = {A verified and optimized Stream X-Machine testing method, with application to cloud service certification},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1729},
   year = {2020}
}
@article{Suryasarman2021,
   abstract = {Abstract Software-based self-test (SBST) method is one of the widely used test techniques in processors. SBST scheme provides high fault coverage but incurs long detection latencies in case of intermittent faults (IFs) in online testing mode, due to large size and longer execution time of the test codes. A study of fragmented SBST testing approaches is conducted to select the most efficient fragmented testing strategy. For the selected fragmented SBST method, a reliable set of SBST code fragments with minimal fault detection latency is determined. However, it incurs inconsiderable overall fault coverage drop, compared to the coverage of the complete SBST test code. From experimental results on MIPS Processor, a set of 20 fragments of test tasks with 80\% individual fault coverage was observed to have the highest reliability of all sets of fragments. A larger test task (i.e. complete SBST test code) with 96.3\% coverage and a test period of 8 ms was replaced by these 20 fragments, which provided an overall coverage of 96\% with an individual test period of 0.4 ms, to detect the same set of IFs.},
   author = {Vasudevan Matampu Suryasarman and Santosh Biswas and Aryabartta Sahu},
   doi = {https://doi.org/10.1049/cdt2.12003},
   issue = {1},
   journal = {IET Computers and Digital Techniques},
   keywords = {built-in self test,fault diagnosis,integrated circuit reliability,integrated circuit testing,logic testing,microprocessor chips},
   pages = {56-76},
   title = {Fragmented software-based self-test technique for online intermittent fault detection in processors},
   volume = {15},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cdt2.12003},
   year = {2021}
}
@article{Yang2021,
   abstract = {Abstract Neural network architectures are achieving superhuman performance on an expanding range of tasks. To effectively and safely deploy these systems, their decision-making must be understandable to a wide range of stakeholders. Methods to explain artificial intelligence (AI) have been proposed to answer this challenge, but a lack of theory impedes the development of systematic abstractions, which are necessary for cumulative knowledge gains. We propose Bayesian Teaching as a framework for unifying explainable AI (XAI) by integrating machine learning and human learning. Bayesian Teaching formalizes explanation as a communication act of an explainer to shift the beliefs of an explainee. This formalization decomposes a wide range of XAI methods into four components: (a) the target inference, (b) the explanation, (c) the explainee model, and (d) the explainer model. The abstraction afforded by Bayesian Teaching to decompose XAI methods elucidates the invariances among them. The decomposition of XAI systems enables modular validation, as each of the first three components listed can be tested semi-independently. This decomposition also promotes generalization through recombination of components from different XAI systems, which facilitates the generation of novel variants. These new variants need not be evaluated one by one provided that each component has been validated, leading to an exponential decrease in development time. Finally, by making the goal of explanation explicit, Bayesian Teaching helps developers to assess how suitable an XAI system is for its intended real-world use case. Thus, Bayesian Teaching provides a theoretical framework that encourages systematic, scientific investigation of XAI.},
   author = {Scott Cheng-Hsin Yang and Tomas Folke and Patrick Shafto},
   doi = {https://doi.org/10.1002/ail2.37},
   issue = {4},
   journal = {Applied AI Letters},
   keywords = {Bayesian Teaching,cognitive science,design patterns,explainable AI,human computer interaction},
   pages = {e37},
   title = {Abstraction, validation, and generalization for explainable artificial intelligence},
   volume = {2},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.37},
   year = {2021}
}
@article{Ouellet2024,
   abstract = {Abstract Many object-oriented metrics have been proposed in the literature to measure various structural properties of object-oriented software. Furthermore, many centrality measures have been introduced to identify central nodes in large networks. However, few studies have used them to measure dependencies in software systems. In fact, centrality measures, as opposed to most traditional object-oriented metrics that mainly focus on intrinsic properties of classes, can be used to better model the control flow and to identify the most important classes in a software system. This paper aims (1) to investigate the relationships between object-oriented metrics and centrality measures and (2) to explore the ability of their combination to support fault-proneness prediction from different perspectives (fault-prone classes, fault severity, and number of faults). Many studies in the literature have addressed the prediction of fault-prone classes, from different perspectives, using object-oriented metrics. The main motivation here is in fact to investigate if the information captured by centrality measures is related to fault proneness and complementary to the information captured by object-oriented metrics and to investigate if the combination of object-oriented metrics and centrality measures improves the performance of fault-proneness prediction significantly. We used size, complexity, and coupling object-oriented metrics in addition to various centrality measures. We collected data from 20 different versions of five open-source Java software systems. We first studied the relationships between selected metrics and their relationships to fault proneness. Then, we built different models to predict fault-prone classes using several machine learning algorithms. In addition, we built models to predict if a class contains a high severity fault, and the number of faults in a class. Results indicate that using centrality measures in combination with object-oriented metrics improves the prediction of fault-prone classes as well as the prediction of the number of faults in a class. However, the combination has no significant impact, according to the data we collected, on the quality of the prediction of fault severity. Moreover, using centrality measures in combination with object-oriented metrics also improves the prediction performance of fault proneness and the number of faults in both cross-version and cross-system validation.},
   author = {Alexandre Ouellet and Mourad Badri},
   doi = {https://doi.org/10.1002/smr.2548},
   issue = {4},
   journal = {Journal of Software: Evolution and Process},
   keywords = {centrality measures,cross-version and cross-system validation,empirical analysis,fault-proneness prediction,machine learning algorithms,object-oriented metrics},
   pages = {e2548},
   title = {Combining object-oriented metrics and centrality measures to predict faults in object-oriented software: An empirical validation},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2548},
   year = {2024}
}
@article{Sobrinho2023,
   abstract = {Summary Model-based testing (MBT) relies on models of the system's behaviour to generate abstract tests. Testers can reuse formal models using MBT to increase confidence in critical systems (e.g., medical and avionic systems). In this article, we investigate the current abstract test generation approaches for CPN to provide insights for testers who need to select a suitable one when applying the MBT using CPN. We conduct a systematic literature review to investigate the existing abstract test generation approaches designed for CPN. Subsequently, focusing on specific implementations and advantages/disadvantages, we experiment with formal models of medical systems during our empirical analysis to improve the discussion on existing abstract test generation approaches for CPN. Our study shows that CPN provides reliable tests quickly, depending on the abstract test generation approach applied.},
   author = {Álvaro Sobrinho and Ially Almeida and Leandro Dias da Silva and Lenardo e Silva and Adriano Araújo and Tássio Fernandes Costa and Angelo Perkusich},
   doi = {https://doi.org/10.1002/stvr.1837},
   issue = {2},
   journal = {Software Testing, Verification and Reliability},
   keywords = {coloured Petri nets,formal models,model-based testing,test case generation},
   pages = {e1837},
   title = {Coloured Petri nets for abstract test generation in software engineering},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1837},
   year = {2023}
}
@article{Qasaimeh2022,
   abstract = {Abstract As the number of cyber-attacks on financial institutions has increased over the past few years, an advanced system that is capable of predicting the target of an attack is essential. Such a system needs to be integrated into the existing detection systems of financial institutions as it provides them with proactive controls with which to halt an attack by predicting patterns. Advanced prediction systems also enhance the software design and security testing of new advanced cyber-security measures by providing new testing scenarios supported by attack forecasting. This present study developed a model that forecasts future network-based cyber-attacks on financial institutions using a deep neural network. The dataset that was used to train and test the model consisted of some of the biggest cyber-attacks on banking institutions over the past three years. This provided insight into new patterns that may end with a cyber-crime. These new attacks were also evaluated to determine behavioral similarities with the nearest known attack or a combination of several existing attacks. The performance of the forecasting model was then evaluated in a real banking environment and provided a forecasting accuracy of 90.36\%. As such, financial institutions can use the proposed forecasting model to improve their security testing measures.},
   author = {Malik Qasaimeh and Rand Abu Hammour and Muneer Bani Yassein and Raad S Al-Qassas and Juan Alfonso Lara Torralbo and David Lizcano},
   doi = {https://doi.org/10.1002/smr.2489},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   keywords = {cyber security,forecasting model,machine learning,security software testing},
   pages = {e2489},
   title = {Advanced security testing using a cyber-attack forecasting model: A case study of financial institutions},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2489},
   year = {2022}
}
@article{Benyagoub2020,
   abstract = {Abstract Contemporary interaction-based complex systems are often built by reusing existing distributed peers, which have to coordinate with each other to fulfill the client, system, and environment requirements. In this paper, we address the design of distributed systems composed of peers (state-transitions systems) communicating through message exchanges. We consider choreographies as the formal model, allowing a developer to describe and specify peers coordination as a set of conversations; ie, all sequences of messages exchanged between the communicating peers. Proceeding this way requires building neither the individual peers nor their composition as they may be obtained by the choreography projection. The correctness of the preservation of such messages exchanges by each peer obtained after projection is a key issue, known as the realizability problem. Checking choreography realizability is mandatory to build third-party applications with no coordination error, eg, absence of deadlocks, missing messages, and erroneous messaging order. In our previous work, we have proposed a set of composition operators, allowing designers to build realizable choreographies that are represented by conversation protocols (CPs). In this work, realizability is guaranteed by construction. We rely on the correct-by-construction Event-B method to prove that each CP constructed using our operators is realizable. In this paper, we show how our approach applies and scales to a set of use cases borrowed from the literature and used by the research community. We also show that our approach allows to detect failures and failure recovery in case realizability does not hold.},
   author = {Sarah Benyagoub and Yamine Aït-Ameur and Meriem Ouederni and Atif Mashkoor and Ahmed Medeghri},
   doi = {https://doi.org/10.1002/smr.2209},
   issue = {2},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Event-B,choreography realizability,conversation protocols,correct by construction,distributed systems,proof and refinement-based methods},
   note = {e2209 smr.2209},
   pages = {e2209},
   title = {Formal design of scalable conversation protocols using Event-B: Validation, experiments, and benchmarks},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2209},
   year = {2020}
}
@article{Wang2022,
   abstract = {Abstract Mature test automation is key for achieving software quality at speed. In this paper, we present a multivocal literature review with the objective to survey and synthesize the guidelines given in the literature for improving test automation maturity. We selected and reviewed 81 primary studies, consisting of 26 academic literature and 55 grey literature sources. From primary studies, we extracted 26 test automation best practices (e.g., Define an effective test automation strategy, Set up good test environments, and Develop high-quality test scripts) and collected many pieces of advice (e.g., in forms of implementation/improvement approaches, technical techniques, concepts, and experience-based heuristics) on how to conduct these best practices. We made main observations: (1) There are only six best practices whose positive effect on maturity improvement have been evaluated by academic studies using formal empirical methods; (2) several technical related best practices in this MLR were not presented in test maturity models; (3) some best practices can be linked to success factors and maturity impediments proposed by other scholars; (4) most pieces of advice on how to conduct proposed best practices were identified from experience studies and their effectiveness need to be further evaluated with cross-site empirical evidence using formal empirical methods; (5) in the literature, some advice on how to conduct certain best practices are conflicting, and some advice on how to conduct certain best practices still need further qualitative analysis.},
   author = {Yuqing Wang and Mika V Mäntylä and Zihao Liu and Jouni Markkula and Päivi Raulamo-jurvanen},
   doi = {https://doi.org/10.1002/stvr.1804},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {improvement,maturity,practice,software,systematic literature review,test automation},
   pages = {e1804},
   title = {Improving test automation maturity: A multivocal literature review},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1804},
   year = {2022}
}
@article{delaBarrera2023,
   abstract = {Abstract Quantum computing is expected to exponentially outperform classic computing on a broad set of problems, including encryption, machine learning, and simulations. It has an impact yet to explore on all software lifecycle's processes and techniques. Testing quantum software raises a significant number of challenges due to the unique properties of quantum physics—such as superposition and entanglementand the stochastic behavior of quantum systems. It is, therefore, an open research issue. In this work, we offer a systematic mapping study of quantum software testing engineering, presenting a comprehensive view of the current state of the art. The main identified trends in testing techniques are (1) the statistic approaches based on repeated measurements and (2) the use of Hoare-like logics to reason about software correctness. Another relevant line of research is reversible circuit testing, which is partially applicable to quantum software unitary testing. Finally, we have observed a flourishing of secondary studies and frameworks supporting testing processes from 2018 onwards.},
   author = {Antonio de la Barrera and Ignacio de Guzmán and Macario Polo and Mario Piattini},
   doi = {https://doi.org/10.1002/smr.2419},
   issue = {4},
   journal = {Journal of Software: Evolution and Process},
   pages = {e2419},
   title = {Quantum software testing: State of the art},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2419},
   year = {2023}
}
@article{Usman2020,
   abstract = {Abstract Mobile devices have limited resources, including memory and processing speed. The performance of mobile applications is an important concern. There are a large number of mobile platforms available with varying operating systems and hardware. Native applications are usually developed and maintained separately for these platforms. The overall performance of native applications may significantly vary across platforms. The current industrial practice is to manually test the performance for each variant, which is not a scalable or efficient approach. We tackled the problem of generating native application variants in our previous work. This paper proposes an automated model-based approach for performance test generation for native application variants at unit level. We propose a performance profile that allows modeling of domain-specific performance parameters on UML models, which are used for automated performance test generation for each native variant. The results of applying the approach on two real-world applications show that the approach evaluates the performance of application variants for two different versions of Android successfully and have potential to reduce the effort and time. A questionnaire-based study is conducted to evaluate the usefulness of the approach.},
   author = {Muhammad Usman and Muhammad Zohaib Iqbal and Muhammad Uzair Khan},
   doi = {https://doi.org/10.1002/smr.2215},
   issue = {1},
   journal = {Journal of Software: Evolution and Process},
   keywords = {aspect,mobile application,model-based,performance profile,performance testing,state machine},
   note = {e2215 JSME-18-0176.R2},
   pages = {e2215},
   title = {An automated model-based approach for unit-level performance test generation of mobile applications},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2215},
   year = {2020}
}
@article{BenMesmia2021,
   abstract = {Abstract In this paper, we provide a non-Markovian Stochastic Petri Net (SPN) model for DevOps workflow specification, and we determine how business processes are carried out. After describing our model semantics, we show how general properties related to liveness and safety can be checked. After that, we provide several extensions on SPNs (SPN) the notation and expressivity to check some specific properties related to actors' (Developers and Operators) availability, interactions between the actors, and execution failures detection linked to the DevOps steps. Next, we validate the proposed model relevance with MATLAB simulation through a specific DevOps case study. Finally, we propose a truncated density function to anticipate the delays related to the DevOps business process overall steps.},
   author = {Walid Ben Mesmia and Mohamed Escheikh and Kamel Barkaoui},
   doi = {https://doi.org/10.1002/smr.2329},
   issue = {3},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,NM-WSPN,prediction,specification,verification,workflow},
   note = {e2329 smr.2329},
   pages = {e2329},
   title = {DevOps workflow verification and duration prediction using non-Markovian stochastic Petri nets},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2329},
   year = {2021}
}
@article{Bakc2023,
   abstract = {Abstract Activity classification and biometric authentication have become synonymous with wearable technologies such as smartwatches and trackers. Although great efforts have been made to develop electrocardiogram (ECG)-based biometric verification and identification modalities using data from these devices, in this paper, we explore the use of adaptive techniques based on prior activity classification in an attempt to enhance biometric performance. In doing so, we also compare two waveform similarity distances to provide features for classification. Two public datasets which were collected from medical and wearable devices provide a cross-device comparison. Our results show that our method is able to be used for both wearable and medical devices in activity classification and biometric verification cases. This study is the first study which uses only ECG signals for both activity classification and biometric verification purposes.},
   author = {Hazal Su Bıçakcı and Marco Santopietro and Richard Guest},
   doi = {https://doi.org/10.1049/bme2.12105},
   issue = {1},
   journal = {IET Biometrics},
   keywords = {ECG biometrics,activity classification,behavioural biometrics,biometrics (access control),emotion recognition,wearable devices},
   pages = {38-51},
   title = {Activity-based electrocardiogram biometric verification using wearable devices},
   volume = {12},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/bme2.12105},
   year = {2023}
}
@article{Fei2023,
   abstract = {Abstract Named Data Networking (NDN) is a new promising architecture of information-centric networking, which supports multicast of data and adopts the publish/subscribe model in the network. The features of NDN, including name-based data and in-network caching, make it a promising Internet architecture for cyber-physical systems (CPSs). Named-Data Link State Routing (NLSR) protocol is the routing protocol for NDN, which is designed to disseminate link state advertisements (LSAs) to both build a network topology and distribute all the name prefixes to every node in the network. In this paper, we make the very first attempt to formally model and verify some fundamental properties of the NLSR protocol using model checker UPPAAL. We validate the NLSR protocol modeled into timed automata with simulator in UPPAAL. We verify our model with four fundamental properties (termination, reachability of Sync Interest, reachability of Sync Data, and digest synchronization). The first synchronization problem is found in a scenario with two nodes topology. We give the improved model that owns a valid result in digest synchronization verification. To capture more problems, we make the model to support the simulation of temporary network crash. The second synchronization problem is also exposed in two comparative scenarios. We also propose a mechanism implemented in our model, which has a valid result in digest synchronization verification. We hope that our study and preliminary results would help enhancing the adaptability and robustness of NLSR protocol.},
   author = {Yuan Fei and Huibiao Zhu and Jiaqi Yin},
   doi = {https://doi.org/10.1002/smr.2384},
   issue = {7},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Named Data Networking (NDN),Named-Data Link State Routing (NLSR) protocol,modeling,verification},
   pages = {e2384},
   title = {Modeling and verifying NLSR protocol of NDN for CPS using UPPAAL},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2384},
   year = {2023}
}
@article{Sah2023,
   abstract = {Abstract SARS-Coronavirus was first detected in December 2019, later named COVID-19, and declared a pandemic by the World Health Organization (WHO). As prediction models assist policymakers in making decisions based on expected outcomes. Existing models were only used to anticipate a smaller range of data resulting in irrelevant predictions. Our research focuses on predicting COVID-19 confirmed, recovered, and deceased Indian cases for 20 days ahead. Tuning of hyperparameters is performed with a grid search cross-validation approach. The dataset is collected from the Kaggle. Our forecast indicates that the count of confirmed and deceased cases is higher whereas, recovered cases prediction shows a decreasing trend. The R2 Score achieved is 0.5112 and root-mean-square error (RMSE) is 1251 using optimized SARIMAX. Finally, Monte Carlo simulation has also been performed to justify the prediction accuracy as compared to other models such as linear, polynomial, prophet, and SARIMAX without grid search cross validation.},
   author = {Sweeti Sah and Balasubramanian Surendiran and Ramasamy Dhanalakshmi and Mohammed Yamin},
   doi = {https://doi.org/10.1111/exsy.13086},
   issue = {5},
   journal = {Expert Systems},
   keywords = {Hyperparameter tuning,SARIMAX,SARS-CoV2,cases,grid search,optimization},
   pages = {e13086},
   title = {Covid-19 cases prediction using SARIMAX Model by tuning hyperparameter through grid search cross-validation approach},
   volume = {40},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13086},
   year = {2023}
}
@article{Sun2022,
   abstract = {Networked education represents a development direction of educational reform, has become a feature of modern education, and has formed a new impetus to the development of education. The change from paper-and-pencil examinations to computer network-based machine examinations is not only a reform of the content and form of the University English Level 4 and 6 examinations, but also a reflection of the reform of the comprehensive assessment system and teaching philosophy of University English teaching. Promoting the development and construction of online college English examination systems in universities, applying machine examinations to various types of tests and formal examinations for college English teaching, and updating the college English question bank through continuous accumulation of professional teachers will have very high research value and practical value in improving the quality of examinations and promoting the standardization of college English examinations. In order to show the individualized chemistry report simultaneously, it is necessary to further ask relevant subject experts, educational measurement experts, and front-line teachers to expand the question attributes; that is, the question attributes extend to module specific knowledge points, cognitive levels, and information literacy ability levels, while most of the pages are combined with Ajax technology to reduce hardware resource dependency and optimize software performance. The paper analyzes the requirements analysis and overall design of the online college English examination system, including the design objectives, design principles, functional design, core algorithm design, architecture design, database design, and security design. The main functions of the online college English examination system, such as personal information management, question bank information management, test paper grouping management, online examination management, paper marking management, and system setting management, are discussed in detail. The main functions of the system, such as personal information management, question bank information management, test paper grouping management, online test management, mark and approve management, and system setting management, are discussed in detail, and the performance test and some function test of the system are also briefly discussed. The whole research process involves the identification of test questions and their attributes in the construction of the question bank, the construction of the adaptive test system, the comprehensive analysis of the mock test results, and the thinking about the application of the high school adaptive academic level test.},
   author = {Wenjuan Sun and Ping Han and Shaohua Pei},
   doi = {https://doi.org/10.1155/2022/4769951},
   issue = {1},
   journal = {Scientific Programming},
   pages = {4769951},
   title = {Mathematical Model Construction of Network Computer Test Automation System for College English Teaching},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/4769951},
   year = {2022}
}
@article{Alam2024,
   abstract = {Abstract Security is an essential attribute of high-quality software. However, effectively incorporating security practices into different phases of the software development life cycle (SDLC) remains challenging. Owing to less mature secure testing processes, organizations are prone to ineffective testing practices for defect detection, including severe security-related failures. Thus, in this study, we present a maturity model for secure software testing (MMSST) to assist software development organizations in improving the secure testing of software applications. We conducted a multivocal literature review and identified 68 primary studies from the formal and gray literature. Then, based on the available evidence, 27 process areas were identified to develop the proposed MMSST. The MMSST includes five main categories: governance, contrive and design, execution, deployment and configuration, and mature. The MMSST was subsequently evaluated using case studies related to practical environments. Results demonstrate that the proposed MMSST is useful for estimating the maturity level of an organization with respect to the secure testing phase of the SDLC. The participants of the case studies also agreed that the proposed MMSST is useful in terms of structure, user satisfaction, and ease of use. We believe that the proposed MMSST can help organizations evaluate and improve software security testing practices. In addition, the proposed MMSST is expected to provide researchers and industry practitioners with an effective foundation for developing new secure testing approaches and tools.},
   author = {Gulzar Alam and Sajjad Mahmood and Mohammad Alshayeb and Mahmood Niazi and Saad Zafar},
   doi = {https://doi.org/10.1002/smr.2593},
   issue = {5},
   journal = {Journal of Software: Evolution and Process},
   keywords = {empirical study,maturity model,multivocal literature review,secure software,secure software testing},
   pages = {e2593},
   title = {Maturity model for secure software testing},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2593},
   year = {2024}
}
@article{Gyimesi2021,
   abstract = {Summary JavaScript is a popular programming language that is also error-prone due to its asynchronous, dynamic, and loosely typed nature. In recent years, numerous techniques have been proposed for analyzing and testing JavaScript applications. However, our survey of the literature in this area revealed that the proposed techniques are often evaluated on different datasets of programs and bugs. The lack of a commonly used benchmark limits the ability to perform fair and unbiased comparisons for assessing the efficacy of new techniques. To fill this gap, we propose BugsJS, a benchmark of 453 real, manually validated JavaScript bugs from 10 popular JavaScript server-side programs, comprising 444k lines of code (LOC) in total. Each bug is accompanied by its bug report, the test cases that expose it, as well as the patch that fixes it. We extended BugsJS with a rich web interface for visualizing and dissecting the bugs' information, as well as a programmable API to access the faulty and fixed versions of the programs and to execute the corresponding test cases, which facilitates conducting highly reproducible empirical studies and comparisons of JavaScript analysis and testing tools. Moreover, following a rigorous procedure, we performed a classification of the bugs according to their nature. Our internal validation shows that our taxonomy is adequate for characterizing the bugs in BugsJS. We discuss several ways in which the resulting taxonomy and the benchmark can help direct researchers interested in automated testing of JavaScript applications. © 2021 The Authors. Software Testing, Verification and Reliability published by John Wiley \& Sons, Ltd.},
   author = {Péter Gyimesi and Béla Vancsics and Andrea Stocco and Davood Mazinanian and Árpád Beszédes and Rudolf Ferenc and Ali Mesbah},
   doi = {https://doi.org/10.1002/stvr.1751},
   issue = {4},
   journal = {Software Testing, Verification and Reliability},
   keywords = {BugsJS,JavaScript,benchmark,bug database,bug taxonomy,reproducibility},
   note = {e1751 stvr.1751},
   pages = {e1751},
   title = {BUGSJS: a benchmark and taxonomy of JavaScript bugs},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1751},
   year = {2021}
}
@article{Gartziandia2022,
   abstract = {Abstract The software of systems of elevators needs constant maintenance to deal with new functionality, bug fixes, or legislation changes. To automatically validate the software of these systems, a typical approach in industry is to use regression oracles, which execute test inputs both in the software version under test and in a previous software version. However, these practices require a long test execution time and cannot be re-used at different test phases. To deal with these issues, we propose Dispatching AlgoRIthm Oracle (DARIO), a test oracle that relies on regression machine-learning algorithms to detect both functional and non-functional problems of the system. The machine-learning algorithms of this oracle are trained by using data from previously tested versions to predict reference functional and non-functional performance values of the new versions. An empirical evaluation with an industrial case study demonstrates the feasibility of using our approach. A total of five regression learning algorithms were validated by using mutation testing techniques. For the context of functional bugs, the accuracy when predicting verdicts by DARIO ranged between 95\% and 98\%, across the different scenarios proposed. For the context of non-functional bugs, were competitive too, having an accuracy when predicting verdicts by DARIO ranged between 83\% and 87\%.},
   author = {Aitor Gartziandia and Aitor Arrieta and Jon Ayerdi and Miren Illarramendi and Aitor Agirre and Goiuria Sagardui and Maite Arratibel},
   doi = {https://doi.org/10.1002/smr.2465},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   keywords = {machine learning,performance testing,regression testing,test oracle},
   pages = {e2465},
   title = {Machine learning-based test oracles for performance testing of cyber-physical systems: An industrial case study on elevators dispatching algorithms},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2465},
   year = {2022}
}
@article{Chen2022,
   abstract = {Abstract As a crucial component of intelligent transportation system, Internet of Vehicles (IoV) plays an important role in the smart and intelligent cities. However, current Internet architectures cannot guarantee efficient data delivery and adequate data security for IoV. Therefore, Named Data Networking (NDN), a leading architecture of Information-Centric Networking (ICN), is introduced into IoV. Although problems about data distribution can be resolved effectively, the combination of NDN and IoV causes some new security issues. In this paper, we apply Communicating Sequential Processes (CSP) to formalize NDN-based IoV. We mainly focus on its data access mechanism and model this mechanism in detail. By feeding the formalized model into the model checker Process Analysis Toolkit (PAT), we verify four vital properties, namely, deadlock freedom, data reliability, PIT deletion faking, and CS caching pollution. According to verification results, the model cannot ensure the security of data with the appearance of intruders. To solve these problems, we construct a blockchain-based mechanism by creating a blockchain-based distribution trusted platform on top of NDN-based IoV. Through the analysis of the improved model, the blockchain-based mechanism can truly guarantee the security of NDN-based IoV.},
   author = {Ningning Chen and Huibiao Zhu and Jiaqi Yin and Yuan Fei and Lili Xiao and Minghua Zhu},
   doi = {https://doi.org/10.1002/smr.2371},
   issue = {10},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Internet of Vehicles (IoV),Named Data Networking (NDN),blockchain,modeling and verification,process algebra CSP},
   pages = {e2371},
   title = {Modeling and verifying NDN-based IoV using CSP},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2371},
   year = {2022}
}
@article{Mahajan2021,
   abstract = {Summary Companies often employ (i18n) frameworks to provide translated text and localized media content on their websites in order to effectively communicate with a global audience. However, the varying lengths of text from different languages can cause undesired distortions in the layout of a web page. Such distortions, called Internationalization Presentation Failures (IPFs), can negatively affect the aesthetics or usability of the website. Most of the existing automated techniques developed for assisting repair of IPFs either produce fixes that are likely to significantly reduce the legibility and attractiveness of the pages or are limited to only detecting IPFs, with the actual repair itself remaining a labour intensive manual task. To address this problem, we propose a search-based technique for automatically repairing IPFs in web applications, while ensuring a legible and attractive page. The empirical evaluation of our approach reported that our approach was able to successfully resolve 94\% of the detected IPFs for 46 real-world web pages. In a user study, participants rated the visual quality of our fixes significantly higher than the unfixed versions and also considered the repairs generated by our approach to be notably more legible and visually appealing than the repairs generated by existing techniques.},
   author = {Sonal Mahajan and Abdulmajeed Alameer and Phil McMinn and William G J Halfond},
   doi = {https://doi.org/10.1002/stvr.1746},
   issue = {1-2},
   journal = {Software Testing, Verification and Reliability},
   keywords = {automated repair,internationalization,layout issues,presentation failures,search-based software engineering,web applications},
   note = {e1746 stvr.1746},
   pages = {e1746},
   title = {Effective automated repair of internationalization presentation failures in web applications using style similarity clustering and search-based techniques},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1746},
   year = {2021}
}
@article{Altiero2024,
   abstract = {Abstract Regression test prioritization (RTP) is an active research field, aiming at re-ordering the tests in a test suite to maximize the rate at which faults are detected. A number of RTP strategies have been proposed, leveraging different factors to reorder tests. Some techniques include an analysis of changed source code, to assign higher priority to tests stressing modified parts of the codebase. Still, most of these change-based solutions focus on simple text-level comparisons among versions. We believe that measuring source code changes in a more refined way, capable of discriminating between mere textual changes (e.g., renaming of a local variable) and more structural changes (e.g., changes in the control flow), could lead to significant benefits in RTP, under the assumption that major structural changes are also more likely to introduce faults. To this end, we propose two novel RTP techniques that leverage tree kernels (TK), a class of similarity functions largely used in Natural Language Processing on tree-structured data. In particular, we apply TKs to abstract syntax trees of source code, to more precisely quantify the extent of structural changes in the source code, and prioritize tests accordingly. We assessed the effectiveness of the proposals by conducting an empirical study on five real-world Java projects, also used in a number of RTP-related papers. We automatically generated, for each considered pair of software versions (i.e., old version, new version) in the evolution of the involved projects, 100 variations with artificially injected faults, leading to over 5k different software evolution scenarios overall. We compared the proposed prioritization approaches against well-known prioritization techniques, evaluating both their effectiveness and their execution times. Our findings show that leveraging more refined code change analysis techniques to quantify the extent of changes in source code can lead to relevant improvements in prioritization effectiveness, while typically introducing negligible overheads due to their execution.},
   author = {Francesco Altiero and Anna Corazza and Sergio Di Martino and Adriano Peron and Luigi Libero Lucio Starace},
   doi = {https://doi.org/10.1002/smr.2653},
   issue = {8},
   journal = {Journal of Software: Evolution and Process},
   keywords = {regression testing,source code changes,test prioritization,tree kernels},
   pages = {e2653},
   title = {Regression test prioritization leveraging source code similarity with tree kernels},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2653},
   year = {2024}
}
@article{Muniandi2021,
   abstract = {Abstract Rail operators around the world are adopting advanced control systems for railway signalling and train protection to improve their safety performance. In these re-signalling projects, the information about the already installed signals, point machines, track circuits etc. along the trackside infrastructure is essential for the configuration of advanced signalling equipment. The conventional information collection method which is based on unmanned aerial vehicles, helicopters etc. has the access limitations in terrains such as tunnels, hilly regions, river bridges, dense forests etc. To overcome these challenges, this paper proposes the railway vehicular crowdsensing method in which one group of trains participates in this information collection and another group of trains confirms the validity of the collected information. For information exchange between the trains and centralized server, this study chooses the permissioned blockchain-based information transaction mechanism to ensure the trustworthiness. Furthermore, the railway markup language-based blockchain databases are included for information immutability, crowd information integration, and the automatic execution of data preparation signalling rules. Finally, the case studies are carried out to analyse the adequacy of the proposed combination of crowdsensing based secure trackside infrastructure information collection and validation, permissioned blockchain-enabled information transaction and blockchain databases in the proposed signalling data preparation process.},
   author = {Ganesan Muniandi},
   doi = {https://doi.org/10.1049/blc2.12002},
   issue = {1},
   journal = {IET Blockchain},
   keywords = {Computing in other engineering fields,Data security,Mobile,ubiquitous and pervasive computing},
   pages = {16-32},
   title = {Blockchain-enabled secure crowdsensing for trackside infrastructure information collection and validation in railway signalling data preparation},
   volume = {1},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/blc2.12002},
   year = {2021}
}
@article{AftabJilani2022,
   abstract = {Abstract Data science (DS) applications not only suffer from traditional software faults but may also suffer from data-specific and model-related faults. Fault models play an important role in evaluating and designing tests for testing DS applications. The existing fault models do not consider DS specific faults. In this study, we built a fault model DS applications. We investigate the faults by using diverse approaches: (i) a multi-vocal literature survey of published literature, (ii) semi-structured interviews of industry experts. The Multi-vocal study allows us to synthesize the existing knowledge from researchers and practitioners. Qualitative data from semi-structured interviews provide us with insights into the nature of faults encountered by practitioners. We combine the results of (i) and (ii) to derive a detailed fault model. The developed fault model is further validated through a quantitative survey of industry practitioners, and the respondents were asked to identify the faults from our proposed fault model that they have experienced and classify those faults based on their severity as perceived by practitioners and its frequency. The results show that practitioners consider prediction bias and model decay as the most severe faults while data sampling and splitting faults along with feature engineering faults are the most frequent.},
   author = {Atif Aftab Jilani and Salman Sherin and Sidra Ijaz and Muhammad Zohaib Iqbal and Muhammad Uzair Khan},
   doi = {https://doi.org/10.1002/smr.2449},
   issue = {5},
   journal = {Journal of Software: Evolution and Process},
   keywords = {data science,fault model,software testing,testing machine learning applications},
   pages = {e2449},
   title = {Deriving and evaluating a fault model for testing data science applications},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2449},
   year = {2022}
}
@article{Barclay2023,
   abstract = {Abstract Adopting shared data resources requires scientists to place trust in the originators of the data. When shared data is later used in the development of artificial intelligence (AI) systems or machine learning (ML) models, the trust lineage extends to the users of the system, typically practitioners in fields such as healthcare and finance. Practitioners rely on AI developers to have used relevant, trustworthy data, but may have limited insight and recourse. This article introduces a software architecture and implementation of a system based on design patterns from the field of self-sovereign identity. Scientists can issue signed credentials attesting to qualities of their data resources. Data contributions to ML models are recorded in a bill of materials (BOM), which is stored with the model as a verifiable credential. The BOM provides a traceable record of the supply chain for an AI system, which facilitates on-going scrutiny of the qualities of the contributing components. The verified BOM, and its linkage to certified data qualities, is used in the AI scrutineer, a web-based tool designed to offer practitioners insight into ML model constituents and highlight any problems with adopted datasets, should they be found to have biased data or be otherwise discredited.},
   author = {Iain Barclay and Alun Preece and Ian Taylor and Swapna Krishnakumar Radha and Jarek Nabrzyski},
   doi = {https://doi.org/10.1002/cpe.6997},
   issue = {18},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {AI ethics,accountability,data provenance,explainable AI,self-sovereign identity},
   pages = {e6997},
   title = {Providing assurance and scrutability on shared data and machine learning models with verifiable credentials},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6997},
   year = {2023}
}
@article{Liu2021,
   abstract = {HTP test in psychometrics is a widely studied and applied psychological assessment technique. HTP test is a kind of projection test, which refers to the free expression of painting itself and its creativity. Therefore, the form of group psychological counselling is widely used in mental health education. Compared with traditional neural networks, deep learning networks have deeper and more network layers and can learn more complex processing functions. In this stage, image recognition technology can be used as an assistant of human vision. People can quickly get the information in the picture through retrieval. For example, you can take a picture of an object that is difficult to describe and quickly search the content related to it. Convolutional neural network, which is widely used in the image classification task of computer vision, can automatically complete feature learning on the data without manual feature extraction. Compared with the traditional test, the test can reflect the painting characteristics of different groups. After quantitative scoring, it has good reliability and validity. It has high application value in psychological evaluation, especially in the diagnosis of mental diseases. This paper focuses on the subjectivity of HTP evaluation. Convolutional neural network is a mature technology in deep learning. The traditional HTP assessment process relies on the experience of researchers to extract painting features and classification.},
   author = {Lin Liu},
   doi = {https://doi.org/10.1155/2021/6370509},
   issue = {1},
   journal = {Computational Intelligence and Neuroscience},
   pages = {6370509},
   title = {Image Classification in HTP Test Based on Convolutional Neural Network Model},
   volume = {2021},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/6370509},
   year = {2021}
}
@article{Tarafdar2024,
   abstract = {Abstract Technostress is an inevitable part of work life. This paper takes a step toward mastering it by focusing on positive stress that Information Systems (IS) creates for IS users, known as techno-eustress. Factors that create techno-eustress are known as techno-eustress creators, which we conceptualise as cognitions experienced by IS users, that IS positively challenges and motivates them to enhance their work. They are important to study because they represent foundational opportunities for professional achievement and growth emanating from IS use. Drawing from theories of psychological eustress, self-determination and proactive work, this paper theorises and validates an instrument to measure techno-eustress creators. We establish the construct's validity and examine its nomological relationships based on data collected from working professionals who used IT for their work. We draw on data from two qualitative studies (N = 35) and three quantitative surveys (N = 980) conducted at different points in time. We validate techno-eustress creators as a second-order reflective construct having four dimensions: techno-mastery, techno-autonomy, techno-enrichment and techno-relatedness. We examine its nomological relationships with factors that create techno-distress, IT strain, and user satisfaction. We contribute to the literature by theorising and validating four ways in which IS users are challenged and motivated by IS to enhance their work. We inform to managerial practice by drawing attention to how organisations can strengthen the different ways employees experience the creators of the ‘good’ stress that use of IS generates.},
   author = {Monideepa Tarafdar and Jean-François Stich and Christian Maier and Sven Laumer},
   doi = {https://doi.org/10.1111/isj.12515},
   issue = {6},
   journal = {Information Systems Journal},
   keywords = {IS-driven positive work cognition,IT strain,scale development,techno-eustress,techno-eustress creators,technostress},
   pages = {2097-2131},
   title = {Techno-eustress creators: Conceptualization and empirical validation},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/isj.12515},
   year = {2024}
}
@article{deAndrade2023,
   abstract = {Summary Despite the rapid growth and popularization of virtual reality (VR) applications, which have enabled new concepts for handling and solving existing problems through VR in various domains, practices related to software engineering have not kept up with this growth. Recent studies indicate that one of the topics that is still little explored in this area is software testing, as VR applications can be built for practically any type of purpose, making it difficult to generalize knowledge to be applied. In this paper, we present an approach that combines metamorphic testing, agent-based testing and machine learning to test VR applications, focusing on finding collision and camera-related faults. Our approach proposes the use of metamorphic relations to detect faults in collision and camera components in VR applications, as well as the use of intelligent agents for the automatic generation of test data. To evaluate the proposed approach, we conducted an experimental study on four VR applications, and the results showed an accuracy of the solution ranging from 93\% to 69\%, depending on the complexity of the application tested. We also discussed the feasibility of extending the approach to identify other types of faults in VR applications. In conclusion, we discussed important trends and opportunities that can benefit both academics and practitioners.},
   author = {Stevão Alves de Andrade and Fatima L S Nunes and Márcio Eduardo Delamaro},
   doi = {https://doi.org/10.1002/stvr.1863},
   issue = {8},
   journal = {Software Testing, Verification and Reliability},
   keywords = {metamorphic testing,software testing,virtual reality},
   pages = {e1863},
   title = {Exploiting deep reinforcement learning and metamorphic testing to automatically test virtual reality applications},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1863},
   year = {2023}
}
@article{Polo2020,
   abstract = {Abstract One of the main challenges of software testing research is the automated addition of oracles to the generated test cases: Whereas the automated generation of operation sequences (which is one of the essential components of test cases) is in practice a solved problem, the automated addition of the oracle (another indispensable element) is still an important problem and an open research question. This article proposes an approach to get executable test suites composed by complete test cases (i.e., they include the oracle). The core of the method is based on annotated regular expressions. The test generation process, which is supported by a tool, follows three steps: (1) creation of annotated regular expressions, where each regular expression describes a set of sequences of operations to be executed against the system under test; (2) expansion of the regular expressions to get sequences of operations, which still do not have parameter values; and (3) generation of the executable test cases with oracle. In this third step, each test case is generated with the suitable oracle, depending on the conditions specified in the regular expression.},
   author = {Macario Polo and Oscar Pedreira and Ángeles S. Places and Ignacio de Guzmán},
   doi = {https://doi.org/10.1002/smr.2273},
   issue = {12},
   journal = {Journal of Software: Evolution and Process},
   keywords = {oracles,regular expressions,software testing,test case generation},
   note = {e2273 JSME-19-0226.R1},
   pages = {e2273},
   title = {Automated generation of oracled test cases with regular expressions and combinatorial techniques},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2273},
   year = {2020}
}
@article{Ferrari2023,
   abstract = {Summary Model-based test design is increasingly being applied in practice and studied in research. Model-based testing (MBT) exploits abstract models of the software behaviour to generate abstract tests, which are then transformed into concrete tests ready to run on the code. Given that abstract tests are designed to cover models but are run on code (after transformation), the effectiveness of MBT is dependent on whether model coverage also ensures coverage of key functional code. In this article, we investigate how MBT approaches generate tests from model specifications and how the coverage of tests designed strictly based on the model translates to code coverage. We used snowballing to conduct a systematic literature review. We started with three primary studies, which we refer to as the initial seeds. At the end of our search iterations, we analysed 30 studies that helped answer our research questions. More specifically, this article characterizes how test sets generated at the model level are mapped and applied to the source code level, discusses how tests are generated from the model specifications, analyses how the test coverage of models relates to the test coverage of the code when the same test set is executed and identifies the technologies and software development tasks that are on focus in the selected studies. Finally, we identify common characteristics and limitations that impact the research and practice of MBT: (i) some studies did not fully describe how tools transform abstract tests into concrete tests, (ii) some studies overlooked the computational cost of model-based approaches and (iii) some studies found evidence that bears out a robust correlation between decision coverage at the model level and branch coverage at the code level. We also noted that most primary studies omitted essential details about the experiments.},
   author = {Fabiano C Ferrari and Vinicius H S Durelli and Sten F Andler and Jeff Offutt and Mehrdad Saadatmand and Nils Müllner},
   doi = {https://doi.org/10.1002/stvr.1860},
   issue = {8},
   journal = {Software Testing, Verification and Reliability},
   keywords = {model-based testing,systematic literature review,test case generation,test case transformation,test coverage criteria},
   pages = {e1860},
   title = {On transforming model-based tests into code: A systematic literature review},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1860},
   year = {2023}
}
@article{Shang2021,
   abstract = {The coverage of test cases is an important indicator for the security and robustness test of industrial control protocols. It is an important research topic to complete the test with less use cases. Taking Modbus protocol as an example, a calculation method of case similarity and population dispersion based on weight division is proposed in this paper. The method can describe the similarity of use cases and the dispersion degree of individuals in the population more accurately. Genetic algorithm is used to generate and optimize test cases, and individual similarity and population dispersion are used as fitness functions of genetic algorithm. Experimental results show that the proposed method can increase the population dispersion by 3.45\% compared with the conventional methods and effectively improve the coverage of test cases.},
   author = {Wenli Shang and Guanyu Zhang and Tianyu Wang and Rui Zhang},
   doi = {https://doi.org/10.1155/2021/6611732},
   issue = {1},
   journal = {Scientific Programming},
   pages = {6611732},
   title = {A Test Cases Generation Method for Industrial Control Protocol Test},
   volume = {2021},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/6611732},
   year = {2021}
}
@article{Tsimpourlas2022,
   abstract = {Abstract Classifying test executions automatically as pass or fail remains a key challenge in software testing and is referred to as the test oracle problem. It is being attempted to solve this problem with supervised learning over test execution traces. A programme is instrumented to gather execution traces as sequences of method invocations. A small fraction of the programme's execution traces is labelled with pass or fail verdicts. Execution traces are then embedded as fixed length vectors and a neural network (NN) component that uses the line-by-line information to classify traces as pass or fail is designed. The classification accuracy of this approach is evaluated using subject programs from different application domains—1. Module from Ethereum Blockchain, 2. Module from PyTorch deep learning framework, 3. Microsoft SEAL encryption library components, 4. Sed stream editor, 5. Nine network protocols from Linux packet identifier, L7-Filter and 6. Utilities library, commons-lang for Java. For all subject programs, it was found that test execution classification had high precision, recall and specificity, averaging to 93\%, 94\% and 96\%, respectively, while only training with an average 14\% of the total traces. Experiments show that the proposed NN-based approach is promising in classifying test executions from different application domains.},
   author = {Foivos Tsimpourlas and Gwenyth Rooijackers and Ajitha Rajan and Miltiadis Allamanis},
   doi = {https://doi.org/10.1049/sfw2.12038},
   issue = {3},
   journal = {IET Software},
   keywords = {Java,Linux,cryptography,execution trace,learning (artificial intelligence),neural nets,neural networks,pattern classification,program testing,software testing,test oracle},
   pages = {301-316},
   title = {Embedding and classifying test execution traces using neural networks},
   volume = {16},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12038},
   year = {2022}
}
@article{Vicente2021,
   abstract = {Abstract Technological changes pose the need to redefine educational models, incorporating programs, and methodologies, so that students acquire the scientific-technological competencies required in today's society. The objective of the research is aimed at establishing a generic method to develop STEAM projects, which are consistent with current curricula and at experimentally testing its validity by developing a STEAM project for a Primary Education class. As an example, and without loss of generality, the method proposed in this article is used to develop STEAM projects from the curriculum in the 4th, 5th, and 6th year of Primary Education in Spain. As a result, 11 areas of opportunity were detected, understood as areas that meet the necessary conditions to become the main theme of a STEAM project, and from which the “Sustainability” area was selected to develop a STEAM project. The project consists of an educational robotics kit, including a board that simulates a city and a robot programmed to travel on the board and activate the different elements that will make it a sustainable city. This first experience was carried out in a classroom with 30 students of 5th year, who showed great interest in the project and achieved a very satisfactory performance.},
   author = {Francisco Ruiz Vicente and Alberto Zapatera Llinares and Nicolas Montes Sánchez},
   doi = {https://doi.org/10.1002/cae.22373},
   issue = {1},
   journal = {Computer Applications in Engineering Education},
   keywords = {STEAM,learning,methodology,projects,robotics},
   pages = {160-174},
   title = {Curriculum analysis and design, implementation, and validation of a STEAM project through educational robotics in primary education},
   volume = {29},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.22373},
   year = {2021}
}
@article{Yan2024,
   abstract = {Abstract Many test coverage metrics have been proposed to measure the deep neural network (DNN) testing effectiveness, including structural coverage and nonstructural coverage. These test coverage metrics are proposed based on the fundamental assumption: They are correlated with test effectiveness. However, the fundamental assumption is still not validated sufficiently and reasonably, which brings question on the usefulness of DNN test coverage. This paper conducted a revisiting study on the existing DNN test coverage from the test effectiveness perspective, to effectively validate the fundamental assumption. Here, we carefully considered the diversity of subjects, three test effectiveness criteria, and both typical and state-of-the-art test coverage metrics. Different from all the existing studies that deliver negative conclusions on the usefulness of existing DNN test coverage, we identified some positive conclusions on their usefulness from the test effectiveness perspective. In particular, we found the complementary relationship between structural and nonstructural coverage and identified the practical usage scenarios and promising research directions for these existing test coverage metrics.},
   author = {Ming Yan and Junjie Chen and Xuejie Cao and Zhuo Wu and Yuning Kang and Zan Wang},
   doi = {https://doi.org/10.1002/smr.2561},
   issue = {4},
   journal = {Journal of Software: Evolution and Process},
   keywords = {deep neural network,empirical study,testing criterion},
   pages = {e2561},
   title = {Revisiting deep neural network test coverage from the test effectiveness perspective},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2561},
   year = {2024}
}
@article{Guo2022,
   abstract = {Abstract Combinatorial testing (CT) can efficiently detect failures caused by interactions of parameters of software under test. The CT study has undergone a transition from traditional CT to constrained CT, which is crucial for real-world systems testing. Under this scenario, constrained covering array generation (CCAG), a vital combinatorial optimisation issue targeted with constructing a test suite of minimal size while properly addressing constraints, remains challenging in CT. To the authors’ best knowledge, this paper presents a synergic method first based on quantum particle swarm optimisation (QPSO) for the CCAG problems. Three auxiliary strategies, including contraction-expansion coefficient adaptive change strategy, differential evolution strategy, and discretisation strategy, are proposed to improve the performance of QPSO. Meanwhile, the improved QPSO method combines with the three different constraint handling strategies and an enhanced one-test-at-a-time strategy as a synergic QPSO method named QPIO to solve the CCAG problem. In the experiment, we investigate the impacts of parameter settings on the performance of the QPIO. Extensive experimental results show that the QPIO algorithm is a competitive method compared to the representative methods for CCAG. Besides, the QPIO method enriches the application of the QPSO algorithm in the context of CT.},
   author = {Xu Guo and Xiaoyu Song and Jian-tao Zhou},
   doi = {https://doi.org/10.1049/sfw2.12054},
   issue = {3},
   journal = {IET Software},
   keywords = {greedy algorithms,particle swarm optimisation,search problems,software engineering},
   pages = {279-300},
   title = {A synergic quantum particle swarm optimisation for constrained combinatorial test generation},
   volume = {16},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12054},
   year = {2022}
}
@article{Qian2022,
   abstract = {Abstract Memory bloat frequently occurs in web applications. It affects system performance and may even cause out-of-memory crashes. For web applications, testers often do performance testing that repeatedly runs test scripts to reveal potential memory bloats. Under that kind of testing, without guidance to determine the running order of test scripts, time may be wasted on testing with those non-bloat-inducing scripts. To address the problem, a test script prioritisation approach is proposed for the testing of memory bloat in Java web applications. The approach predicts which test scripts are more likely to make the underlying web application exhibit memory bloat phenomena by using a learning-to-rank technique. With this prediction, the execution of test scripts can be prioritised, and the revealing of memory bloat can thereby be accelerated. The experiments on a group of web applications obtained from Github and SourceForge show that the proposed prioritisation approach is effective.},
   author = {Ju Qian and Xu Zhou and Hui Zhou},
   doi = {https://doi.org/10.1049/sfw2.12057},
   issue = {3},
   journal = {IET Software},
   keywords = {Internet,Java,machine learning,memory bloat,program testing,public domain software,software tools,test prioritisation,web application},
   pages = {317-330},
   title = {Prioritising test scripts for the testing of memory bloat in web applications},
   volume = {16},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12057},
   year = {2022}
}
@article{Huang2023,
   abstract = {Abstract A successful automated program proof is, in software verification, the ultimate triumph. In practice, however, the road to such success is paved with many failed proof attempts. Unlike a failed test, which provides concrete evidence of an actual bug in the program, a failed proof leaves the programmer in the dark. Can we instead learn something useful from it? The work reported here takes advantage of the rich information that some automatic provers internally collect about the program when attempting a proof. If the proof fails, the Proof2Test tool presented in this article uses the counterexample generated by the prover (specifically, the SMT solver underlying the Boogie tool used in the AutoProof system to perform correctness proofs of contract-equipped Eiffel programs) to produce a failed test, which provides the programmer with immediately exploitable information to correct the program. The discussion presents Proof2Test and the application of the ideas and tool to a collection of representative examples.},
   author = {Li Huang and Bertrand Meyer},
   doi = {https://doi.org/10.1002/stvr.1859},
   issue = {7},
   journal = {Software Testing, Verification and Reliability},
   keywords = {AutoProof,AutoTest,Eiffel,counterexample,program verification},
   pages = {e1859},
   title = {A failed proof can yield a useful test},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1859},
   year = {2023}
}
@article{Tekeli2021,
   abstract = {Abstract Two-parameter estimators have increasing usage in the linear regression model concerning mitigating the problem of multicollinearity. In this type of biased estimators, two different parameters contribute to the solution of two different problems. Previously defined two-parameter ridge estimator (TPRE) assures considerable merits in this context. This estimator eliminates unfavorable effects of multicollinearity as well as improves the coefficient of multiple determination for the linear regression model. Concerning the TPRE, both the mean square error comparisons and some conventional selection methods for the biasing parameters are available in the literature. In this article, we mainly focus on the simultaneous estimation of the biasing parameters of the TPRE through some new optimization techniques by genetic algorithm. To observe validation of the new approaches, we perform a numerical example in addition to a Monte Carlo study. The outcomes of these runnings prove the dominance of the new approaches in comparison to existing techniques in the literature.},
   author = {Erkut Tekeli and Nimet Özbay and Selahattin Kaçiranlar},
   doi = {https://doi.org/10.1002/cpe.6088},
   issue = {9},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {genetic algorithm,multicollinearity,optimization,ridge estimator,two-parameter ridge estimator},
   note = {e6088 CPE-19-1374.R1},
   pages = {e6088},
   title = {Implementation and validation of new optimization methods by genetic algorithm for two-parameter ridge estimator},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6088},
   year = {2021}
}
@article{Al-tekreeti2019,
   abstract = {Summary We propose a model-based test generation methodology to evaluate the impact of the interaction of the wireless network and application configurations on the performance of mobile networked applications. We consider waiting time delay to model wireless network quality. We classify mobile applications into two groups. Group I represents applications where end-user experience is mainly affected by waiting time delay during service consumption, while group II represents applications where end-user experience is affected by waiting time delay before service consumption. Test generation is formulated as an inversion problem. However, for group I applications, solving the inversion problem is expensive. Therefore, we utilize metamorphic testing to mitigate the cost of test oracles. We formulate metamorphic test generation as maximization of the distance between seed and follow-up test cases. Two test coverage criteria are proposed: user experience and user-experience-and-input interaction. Network models are developed for a mobile device that has network access through a WiFi hot spot and uses either transmission control protocol or user datagram protocol. Two mobile applications are used to demonstrate the methodology: multimedia streaming and web browsing. Application of the methodology when user actions are taken into consideration is also addressed. The effectiveness of the methodology is evaluated using two metrics: the incurred time cost and redundancy in the generated test suite. The obtained results show the advantage of casting test generation as an inversion problem, compared with random testing. For apps with intensive performance models, combining metamorphic testing with the methodology has tremendously reduced the cost of test oracles.},
   author = {Mustafa Al-tekreeti and Atef Abdrabou and Kshirasagar Naik},
   doi = {https://doi.org/10.1002/stvr.1713},
   issue = {6-7},
   journal = {Software Testing, Verification and Reliability},
   keywords = {coverage criteria,performance,software,testing},
   note = {e1713 stvr.1713},
   pages = {e1713},
   title = {An end-user-centric test generation methodology for performance evaluation of mobile networked applications},
   volume = {29},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1713},
   year = {2019}
}
@article{Singh2019,
   abstract = {The object-oriented (OO) systems have emerged as the core systems in every field. Test case generation (TCG) for these systems has been identified as one of the crucial activity of software testing. Many researchers have been working in the area of TCG to raise the quality and effectiveness of the OO software systems. It is impossible to test the system exhaustively because of limitations of time, monetary cost and human efforts involved in the process of generation of all test cases. Presently, there is no adequate method that considers the contextual demand for the generation of test cases. Therefore, this study proposes a contextual demand-based TCG for OO systems using test paths or scenarios of sequence diagrams (SDs). Unlike the existing approaches, the proposed method considers flexible approach for generation of test cases as per the contextual demand. For the implementation of the proposed technique, two case studies, i.e. the sample SD and the SD of automated teller machine (ATM) system, have been considered. The evaluation of the proposed method showed the significant consideration of the user demand for generating the final test cases.},
   author = {Rajvir Singh and Rajesh Bhatia and Anita Singhrova},
   doi = {https://doi.org/10.1049/iet-sen.2018.5043},
   issue = {5},
   journal = {IET Software},
   keywords = {ATM system,OO software systems,OO systems,contextual demand-based TCG,core systems,demand-based TCG,flexible approach,monetary cost,object-oriented methods,object-oriented systems,program testing,sequence diagrams,software testing,test case generation,test paths},
   pages = {403-413},
   title = {Demand Based Test Case Generation for Object Oriented System},
   volume = {13},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5043},
   year = {2019}
}
@article{Huang2022,
   abstract = {Abstract Test case prioritization (TCP) aims at scheduling test case execution so that more important test cases are executed as early as possible. Many TCP techniques have been proposed, according to different concepts and principles, with dissimilarity-based TCP (DTCP) prioritizing tests based on the concept of test case dissimilarity: DTCP chooses the next test case from a set of candidates such that the chosen test case is farther away from previously selected test cases than the other candidates. DTCP techniques typically only use one aspect/granularity of the information or features from test cases to support the prioritization process. In this article, we adopt the concept of data fusion to propose a new family of DTCP techniques, data-fusion-driven DTCP (DDTCP), which attempts to use different information granularities for prioritizing test cases by dissimilarity. We performed an empirical study involving 30 versions of five subject programs, investigating the testing effectiveness and efficiency by comparing DDTCP against DTCP techniques that use a dissimilarity granularity. The experimental results show that not only does DDTCP have better fault-detection rates than single-granularity DTCP techniques, but it also appears to only incur similar prioritization costs. The results also show that DDTCP remains robust over multiple system releases.},
   author = {Rubing Huang and Dave Towey and Yinyin Xu and Yunan Zhou and Ning Yang},
   doi = {https://doi.org/10.1002/spe.3068},
   issue = {6},
   journal = {Software: Practice and Experience},
   keywords = {data fusion,dissimilarity,regression testing,software testing,test case prioritization},
   pages = {1352-1377},
   title = {Dissimilarity-based test case prioritization through data fusion},
   volume = {52},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3068},
   year = {2022}
}
@article{Schuts2022,
   abstract = {Abstract This is an industrial experience report on a large semi-automated migration of legacy test code in C and C++. The particular migration was enabled by automating most of the maintenance steps. Without automation this particular large-scale migration would not have been conducted, due to the risks involved in manual maintenance (risk of introducing errors, risk of unexpected rework, and loss of productivity). We describe and evaluate the method of automation we used on this real-world case. The benefits were that by automating analysis, we could make sure that we understand all the relevant details for the envisioned maintenance, without having to manually read and check our theories. Furthermore, by automating transformations we could reiterate and improve over complex and large scale source code updates, until they were “just right.” The drawbacks were that, first, we have had to learn new metaprogramming skills. Second, our automation scripts are not readily reusable for other contexts; they were necessarily developed for this ad-hoc maintenance task. Our analysis shows that automated software maintenance as compared to the (hypothetical) manual alternative method seems to be better both in terms of avoiding mistakes and avoiding rework because of such mistakes. It seems that necessary and beneficial source code maintenance need not to be avoided, if software engineers are enabled to create bespoke (and ad-hoc) analysis and transformation tools to support it.},
   author = {Mathijs T W Schuts and Rodin T A Aarssen and Paul M Tielemans and Jurgen J Vinju},
   doi = {https://doi.org/10.1002/spe.3082},
   issue = {7},
   journal = {Software: Practice and Experience},
   keywords = {parsers,pattern matching,program analysis,refactoring,source code generation},
   pages = {1543-1580},
   title = {Large-scale semi-automated migration of legacy C/C++ test code},
   volume = {52},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3082},
   year = {2022}
}
@article{Bertolino2023,
   abstract = {Abstract To enter the production stage, in DevOps practices candidate software releases have to pass quality gates, where they are assessed to meet established target values for key indicators of interest. We believe software reliability should be an important such indicator, as it greatly contributes to the end-user satisfaction. We propose DevOpRET, an approach for reliability testing as part of the acceptance testing stage in DevOps. DevOpRET relies on operational-profile–based testing, a common reliability assessment technique. DevOpRET leverages usage and failure data monitored in operations to continuously refine its estimate. We evaluate accuracy and efficiency of DevOpRET through controlled experiments with a real-world open source platform and with a microservice architectures benchmark. The results show that DevOpRET provides accurate and efficient estimates of the true reliability over subsequent DevOps cycles.},
   author = {Antonia Bertolino and Guglielmo De Angelis and Antonio Guerriero and Breno Miranda and Roberto Pietrantuono and Stefano Russo},
   doi = {https://doi.org/10.1002/smr.2298},
   issue = {3},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,acceptance test,operational profile,quality gate,software reliability testing},
   note = {e2298 smr.2298},
   pages = {e2298},
   title = {DevOpRET: Continuous reliability testing in DevOps},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2298},
   year = {2023}
}
@article{Zhang2024,
   abstract = {Abstract Internet-scale distributed systems often replicate data at multiple geographic locations to provide low latency and high availability, despite node and network failures. According to the CAP theorem, low latency and high availability can only be achieved at the cost of accepting weak consistency. The conflict-free replicated data type (CRDT) is a framework that provides a principled approach to maintaining eventual consistency among data replicas. CRDTs have been notoriously difficult to design and implement correctly. Subtle deep bugs lie in the complex and tedious handling of all possible cases of conflicting data updates. We argue that the CRDT design should be formally specified and model checked, to uncover deep bugs which are beyond human reasoning. The implementation further needs to be systematically tested. On the one hand, the testing needs to inherit the exhaustive nature of the model checking and ensures the coverage of testing. On the other hand, the testing is expected to find coding errors which cannot be detected by design level verification. Toward the challenges above, we propose the model-checking-driven explorative testing (MET) framework. At the design level, MET uses TLA+ to specify and model check CRDT designs. At the implementation level, MET conducts model-checking-driven explorative testing, in the sense that the test cases are automatically generated from the model-checking traces. The system execution is controlled to proceed deterministically, following the model-checking trace. The explorative testing systematically controls and permutes all nondeterministic choices of message reorderings. We apply MET in our practical development of CRDTs. The bugs in both designs and implementations of CRDTs are found. As for bugs which can be found by traditional testing techniques, MET greatly reduces the cost of fixing the bugs. Moreover, MET can find subtle deep bugs which cannot be found by existing techniques at a reasonable cost. Based on our practical use of MET, we discuss how MET provides us with sufficient confidence in the correctness of our CRDT designs and implementations. Conflict-free replicated data type (CRDT) is a framework that provides a principled approach to maintaining eventual consistency among data replicas in distributed systems. CRDTs have been notoriously difficult to design and implement correctly. We propose model-checking-driven explorative testing (MET) framework for dealing with such problem. We apply MET in our practical development of CRDTs. MET successfully finds subtle deep bugs and provides us with sufficient confidence in the correctness of our CRDT designs and implementations.},
   author = {Yuqi Zhang and Yu Huang and Hengfeng Wei and Xiaoxing Ma},
   doi = {https://doi.org/10.1002/smr.2555},
   issue = {4},
   journal = {Journal of Software: Evolution and Process},
   keywords = {CRDT,TLA+,explorative testing,model checking},
   pages = {e2555},
   title = {Model-checking-driven explorative testing of CRDT designs and implementations},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2555},
   year = {2024}
}
@article{Lizcano2023,
   abstract = {Abstract With years of frantic development, when release fast and release often was the mandatory rule for web technologies and services, the open source paradigm and online distribution repositories have imposed de facto standards for quality assessment in fast-paced innovation processes. Nowadays, however, in pursuit of productivity, security, and user satisfaction, the industry is beginning, through the introduction of new standards such as ECMAScript 6 or web components, to consider software engineering mandates for web technologies. This article reports a quality model aligned with international standard ISO/IEC 25010, covering web components technology, which ultimately aims to improve adoption by the software engineering industry, traditionally wary of agile Internet practices, the open source paradigm, and public repositories. Our research also presents an experimentation platform on which end users have validated the quality properties, highlighting the implicit connection with the perceived quality. The key result of our research convinces us that user ratings are suitable as a testing mechanism for product quality and quality-in-use metrics in order to define an absolute scale of comparison for web component quality.},
   author = {David Lizcano and Andrés-Leonardo Martínez-Ortíz and Genoveva López and Arnaud Grignard},
   doi = {https://doi.org/10.1002/smr.2256},
   issue = {3},
   journal = {Journal of Software: Evolution and Process},
   keywords = {end-user programming,end-user software engineering,human-computer interaction,human-inspired metrics,reliability,software quality},
   note = {e2256 JSME-19-0245.R1},
   pages = {e2256},
   title = {End-user modeling of quality for web components},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2256},
   year = {2023}
}
@article{Ren2023,
   abstract = {Abstract The software engineering community aims to achieve and maintain high-efficient software engineering practical activities. One of the techniques used for this purpose is Test-Driven Development (TDD), which is a cyclic and test-centered development method positively related to maintainability, which may vary depending on the context and application. Previous research has indicated that TDD's benefits on maintainability may be due to the improved focus on coding and testing activities. However, limited research has investigated the role of engagement in software quality. Therefore, this study investigates the relationship between TDD methods, engagement level in development activities, and maintainability. The primary research questions addressed in this study are does following TDD methods improve the engagement level in development activities? And does a higher engagement level leads to better maintainability? The study employs a statistical analysis technique, regression analysis to explore the research questions. The results show that TDD behaviours improve the engagement level, and engaging in development activities has a significant positive impact on maintainability. Additionally, the study finds that the positive impact of focussing on testing, such as writing more test cases, is more pronounced compared to coding activities. Our study adds to current software engineering literature that not only personal expertise but the engagement level in the development process are associated with software quality and calls for the emphasis on developer's engagement.},
   author = {Wei Ren and Stephen Barrett},
   doi = {https://doi.org/10.1049/sfw2.12135},
   issue = {4},
   journal = {IET Software},
   keywords = {software engineering,software management,software process improvement,software quality},
   pages = {509-525},
   title = {Test-driven development, engagement in activity, and maintainability: An empirical study},
   volume = {17},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12135},
   year = {2023}
}
@article{Liu2022-2,
   abstract = {Abstract A major cause of gynecological cancer -related deaths worldwide, ovarian cancer is characterized by heterogeneity in both tumor cells and the tumor microenvironment (TME). Our study aimed to characterize tumor cell heterogeneity and the infiltration of M2 tumor-associated macrophages (TAMs) in the ovarian cancer TME by single-cell RNA-Seq (scRNA-Seq) analysis combined with bulk RNA sequencing (bulk RNA-Seq). Several highly variable genes were identified in ovarian cancer tissues, and tumor cell heterogeneity and infiltrating immune tumor cell heterogeneity were characterized in ovarian cancer cells. M2 TAMs in the TME were the predominant phenotype of TAM. Further, M2 TAM infiltration in the TME was negatively correlated with poor prognosis of ovarian cancer patients. Four M2 TAM-associated genes (SLAMF7, GNAS, TBX2-AS1, and LYPD6) correlated with the prognostic survival of ovarian cancer patients. Knockdown of SLAMF7 or GNAS mRNA repressed malignancy and cisplatin resistance of ovarian cancer cells. ScRNA-Seq combined with bulk RNA-Seq identified the same four genes associated with M2 TAMs. The prognostic risk score model based on these four genes may hold favorable predictive value for the prognosis of ovarian cancer patients.},
   author = {Chang Liu and Ying Zhang and Xiaohan Li and Dandan Wang},
   doi = {https://doi.org/10.1111/nyas.14748},
   issue = {1},
   journal = {Annals of the New York Academy of Sciences},
   keywords = {M2 tumor-associated macrophage,bulk RNA sequencing,epithelial-mesenchymal transition,ovarian cancer,single-cell RNA sequencing,tumor cell heterogeneity,tumor microenvironment},
   pages = {154-173},
   title = {Ovarian cancer-specific dysregulated genes with prognostic significance: scRNA-Seq with bulk RNA-Seq data and experimental validation},
   volume = {1512},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14748},
   year = {2022}
}
@article{Habib2023,
   abstract = {Abstract Regression testing remains a promising research area for the last few decades. It is a type of testing that aims at ensuring that recent modifications have not adversely affected the software product. After the introduction of a new change in the system under test, the number of test cases significantly increases to handle the modification. Consequently, it becomes prohibitively expensive to execute all of the generated test cases within the allocated testing time and budget. To address this situation, the test suite reduction (TSR) technique is widely used that focusses on finding a representative test suite without compromising its effectiveness such as fault-detection capability. In this work, a systematic review study is conducted that intends to provide an unbiased viewpoint about TSR based on various types of search algorithms. The study's main objective is to examine and classify the current state-of-the-art approaches used in search-based TSR contexts. To achieve this, a systematic review protocol is adopted and, the most relevant primary studies (57 out of 210) published between 2007 and 2022 are selected. Existing search-based TSR approaches are classified into five main categories, including evolutionary-based, swarm intelligence-based, human-based, physics-based, and hybrid, grounded on the type of employed search algorithm. Moreover, the current work reports the parameter settings according to their category, the type of considered operator(s), and the probabilistic rate that significantly impacts on the quality of the obtained solution. Furthermore, this study describes the comparison baseline techniques that support the empirical comparison regarding the cost-effectiveness of a search-based TSR approach. Finally, it isconcluded that search-based TSR has great potential to optimally solve the TSR problem. In this regard, several potential research directions are outlined as useful for future researchers interested in conducting research in the TSR domain.},
   author = {Amir Sohail Habib and Saif Ur Rehman Khan and Ebubeogu Amarachukwu Felix},
   doi = {https://doi.org/10.1049/sfw2.12104},
   issue = {2},
   journal = {IET Software},
   keywords = {Pareto optimisation,minimisation,optimisation,particle swarm optimisation,reviews,software engineering},
   pages = {93-136},
   title = {A systematic review on search-based test suite reduction: State-of-the-art, taxonomy, and future directions},
   volume = {17},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12104},
   year = {2023}
}
@article{Garousi2020-1,
   abstract = {Abstract Context Software testing is an important and costly software engineering activity in the industry. Despite the efforts of the software testing research community in the last several decades, various studies show that still many practitioners in the industry report challenges in their software testing tasks. Objective To shed light on industry's challenges in software testing, we characterize and synthesize the challenges reported by practitioners. Such concrete challenges can then be used for a variety of purposes, eg, research collaborations between industry and academia. Method Our empirical research method is opinion survey. By designing an online survey, we solicited practitioners' opinions about their challenges in different testing activities. Our dataset includes data from 72 practitioners from eight different countries. Results Our results show that test management and test automation are considered the most challenging among all testing activities by practitioners. Our results also include a set of 104 concrete challenges in software testing that may need further investigations by the research community. Conclusion We conclude that the focal points of industrial work and academic research in software testing differ. Furthermore, the paper at hand provides valuable insights concerning practitioners' “pain” points and, thus, provides researchers with a source of important research topics of high practical relevance.},
   author = {Vahid Garousi and Michael Felderer and Marco Kuhrmann and Kadir Herkiloğlu and Sigrid Eldh},
   doi = {https://doi.org/10.1002/smr.2251},
   issue = {8},
   journal = {Journal of Software: Evolution and Process},
   keywords = {challenges,opinion survey,software industry,software testing},
   note = {e2251 JSME-18-0181.R1},
   pages = {e2251},
   title = {Exploring the industry's challenges in software testing: An empirical study},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2251},
   year = {2020}
}
@article{Huang2024,
   abstract = {Abstract In software evolution, keeping the test code co-change with the production code is important, because the outdated test code may not work and is ineffective in revealing faults in the production code. However, due to the tight development time, the production and test code may not be co-changed immediately by developers. For example, we analysed the top 1003 popular Java projects on GitHub and found that nearly 9.3\% of cases (i.e., 464,417) did not update their production and test code at the same time, that is, the production code is updated first, and then the test code is updated at intervals. The result indicates that much test code will not be updated in time. In this paper, we propose a novel approach, Jtup, to remind developers to co-change the production code and test code in time. Specifically, we first define the co-changed production and test code as a positive instance, while unchanged test code (i.e., production code changed and test code unchanged) as a negative instance. Then, we extract multidimensional features from the production code to characterize the possibility of their co-change, including code change features, code complexity features, and code semantic features. Finally, several machine learning-based methods are employed to identify the co-changed production and test code. We conduct comprehensive experiments on 20 datasets, and the results show that the Accuracy, Precision, and Recall achieved by Jtup are 76.7\%, 78.1\%, and 77.4\%, which outperforms the state-of-the-art method.},
   author = {Yuan Huang and Zhicao Tang and Xiangping Chen and Xiaocong Zhou},
   doi = {https://doi.org/10.1002/stvr.1870},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {co-change,code change,machine learning,production code,test code},
   pages = {e1870},
   title = {Towards automatically identifying the co-change of production and test code},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1870},
   year = {2024}
}
@article{Wang2021,
   abstract = {Abstract Digital competence is critical for university students to adapt to and benefit from digitally enhanced learning. Prior studies on its measurement mostly focus on educators and relied on factor analyses. However, there is a lack of valid and convenient tools to measure university students' digital competence. This study aimed to develop a digital competence scale for university students (DC-US) in digitally enhanced learning with robust psychometric properties. An initial DC-US with 23 items was proposed to measure the single latent trait of digital competence. It was validated and refined continuously through a pilot study, a main study and a predictive validity study in three datasets involving 825 participants altogether, using factor analyses, Rasch analyses and the partial least squares modelling. The final DC-US turned out to comprise two subscales: technical literacy and digital skills, with 10 items retained, and manifested high internal consistency, unidimensionality and measurement invariance. The scale also demonstrated strong predictive validity, with technical literacy greatly predicting digital skills, which negatively predicted technostress. The DC-US enables instructors and school administrators to conveniently obtain preliminary information of university students' digital competence, informing their digital class preparation and development of timely interventions for addressing digital deficiencies.},
   author = {Xinghua Wang and Zhuo Wang and Qiyun Wang and Wenli Chen and Zhongling Pi},
   doi = {https://doi.org/10.1111/jcal.12546},
   issue = {4},
   journal = {Journal of Computer Assisted Learning},
   keywords = {Rasch analyses,digital competence,digitally enhanced learning,scale,university students},
   pages = {1063-1076},
   title = {Supporting digitally enhanced learning through measurement in higher education: Development and validation of a university students' digital competence scale},
   volume = {37},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12546},
   year = {2021}
}
@article{Song2020,
   abstract = {Abstract Web service has developed the managed IoT application to let connected devices easily and securely interact with cloud applications and other devices. As an important factor for web service, the reliability of web services refers to the probability of web service running success. For modeling web service composition, we should abstract the process of web service composition. Due to the diversity and complexity of web service composition, it is unlikely to do exhaustive testing. In order to improve the quality of web service composition test cases and find out which path leads to the greatest probability of service combination failure, heuristic test case generation method is adopted to obtain the optimal test path. First, the web service composition test is abstracted into the MDP model. The QoS of the web service composition is taken as the software test optimization goal, and the cross-entropy strategy is used to optimize the test case. The experimental results show that the test profile given by the cross-strategy is better than the random test strategy. Detect and exclude the same number of software defects. Cross-entropy strategy can significantly reduce the number of test cases, reduce test costs, and improve defect detection efficiency.},
   author = {Yang Song and Yun-Zhan Gong},
   doi = {https://doi.org/10.1111/coin.12302},
   issue = {4},
   journal = {Computational Intelligence},
   keywords = {MDP model,QoS,cross entropy,test,web service composite},
   pages = {1650-1662},
   title = {Web service composition on IoT reliability test based on cross entropy},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/coin.12302},
   year = {2020}
}
@article{Arshad2022,
   abstract = {Abstract Background In this technologically advanced era, media literacy is necessary to effectively evaluate the information and understand various biases inherent in media messages. Several media literacy (ML) tools are available; however, we need generic and objective tools that can be applied to all forms of media messages. Objectives The current study aimed to develop and validate an objective and generalized measure of media literacy based on the previously available tools. This study suggested that the access component should be removed from the media literacy tools as recommended in previous literature. Methods The total of 386 respondents, both males and females, were recruited from different universities in Lahore. The age of the sample ranged from 18 to 25 (M=20.98, SD=2.12), with an approximately equal proportion of males (47\%) and females. Results and Conclusions This study proposed a compact Media Literacy Scale (MLS) with 3 constructs: analyze (09 items; α=.76), evaluate (08 items; α=.72), and comprehend (07 items; α =76). This 24 items scale explains 55.4\% variance was administered to 386 respondents aged 18 to 30 years (M=20.98, SD=2.12). This developed scale will help assess the baseline level of media literacy in the audience so that in the future, evaluation of the efficacy of media literacy, and media literacy programs could be provided.},
   author = {Arooj Arshad and Saima Ghazal and Noshina Saleem and Mian Ahmad Hanan and Muhammad Haseeb Arshad},
   doi = {https://doi.org/10.1111/jcal.12682},
   issue = {5},
   journal = {Journal of Computer Assisted Learning},
   keywords = {AMOS,confirmatory factor analysis,convergent and discriminant validity,media literacy scale,scale development},
   pages = {1371-1378},
   title = {Revisiting media literacy measurement: Development and validation of 3-factor media literacy scale},
   volume = {38},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12682},
   year = {2022}
}
@article{Pillet2023,
   abstract = {Abstract Scale adaptation, where authors alter the wording of an already published scale, is a deeply rooted social practice in IS research. This paper argues that the time is ripe to question this activity as well as the beliefs that have progressively formed around it. We identify and challenge five fallacious scale adaptation beliefs that hinder the development of more robust measure development norms. Contributing to this area of research, this paper offers a conceptual definition of the cognitive validity concept, defined as the extent to which a scale is free of problematic item characteristics (PICs) that bias the survey response process and subsequent empirical results. Building on this conceptualization effort, a new methodological process for assessing the cognitive validity of adapted IS measures is introduced. Through a series of three programmatic studies, we find converging evidence that the method can benefit the IS field by making the scale adaptation process more robust, transparent, and consistent. Along with the method, we introduce a new index that IS scholars can use to benchmark the cognitive quality of their scales against venerable IS measures. We discuss the implications of our work for IS research (including detailed implementation guidelines) and provide directions for future research on measurement in IS.},
   author = {Jean-Charles Pillet and Kevin D Carillo and Claudio Vitari and Federico Pigni},
   doi = {https://doi.org/10.1111/isj.12428},
   issue = {4},
   journal = {Information Systems Journal},
   keywords = {measure development,scale adaptation,survey research,survey response model},
   pages = {842-889},
   title = {Improving scale adaptation practices in information systems research: Development and validation of a cognitive validity assessment method},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/isj.12428},
   year = {2023}
}
@article{Karthick2022,
   abstract = {Abstract Cloud computing is the delivery of on-demand computing resources. It shares the resources or provides vir-utilization that enables single user to access various Cloud services such as CPU, memory, storage devices, network, and so on. However, more commercial cloud services offered by several cloud service providers (CSPs) are available in the market place. Most CSPs must, therefore, deal with the dynamic resource allocation where the mobile services are migrating from one cloud to another cloud environment to provide heterogeneous resources based on user needs. There is still a lack of heuristics that are able to check requested resources and available resources to allocate and deallocate before it begins the secure service migration. We proposed a resource allocation security protocol that allows resources to be allocated and migrated efficiently in a secure service migration between cloud infrastructures. Furthermore, formal methods can be used for protocols to verify the desired properties, detecting attacks and producing accurate outcomes. This article presents formal modeling and verification of this abstract protocol using ProVerif cryptographic tool to validate the security properties such as secrecy of resources, authentication from both parties and key exchange in order to securely migrate resources in commercial cloud environments.},
   author = {Gayathri Karthick and Glenford Mapp and Florian Kammueller and Mahdi Aiash},
   doi = {https://doi.org/10.1111/coin.12421},
   issue = {3},
   journal = {Computational Intelligence},
   keywords = {QoS,cloud systems,formal methods,mobile services,resource allocation,security protocols},
   note = { COIN-SA-04-20-3722.R1},
   pages = {811-828},
   title = {Modeling and verifying a resource allocation algorithm for secure service migration for commercial cloud systems},
   volume = {38},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/coin.12421},
   year = {2022}
}
@article{Sachtleben2022,
   abstract = {Summary For partial, nondeterministic, finite state machines, a new conformance relation called strong reduction is presented. It complements other existing conformance relations in the sense that the new relation is well suited for model-based testing of systems whose inputs are enabled or disabled, depending on the actual system state. Examples of such systems are graphical user interfaces and systems with interfaces that can be enabled or disabled in a mechanical way. We present a new test generation algorithm producing complete test suites for strong reduction. The suites are executed according to the grey-box testing paradigm: it is assumed that the state-dependent sets of enabled inputs can be identified during test execution, while the implementation states remain hidden, as in black-box testing. We show that this grey-box information is exploited by the generation algorithm in such a way that the resulting best-case test suite size is only linear in the state space size of the reference model. Moreover, examples show that this may lead to significant reductions of test suite size in comparison to true black-box testing for strong reduction.},
   author = {Robert Sachtleben and Jan Peleska},
   doi = {https://doi.org/10.1002/stvr.1806},
   issue = {2},
   journal = {Software Testing, Verification and Reliability},
   keywords = {complete test suites,conformance testing,grey-box testing,model-based testing,partial finite state machines},
   pages = {e1806},
   title = {Effective grey-box testing with partial FSM models},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1806},
   year = {2022}
}
@article{Ye2022,
   abstract = {Abstract Fuzzing is considered to be an essential approach to guarantee the reliability of deep neural networks (DNNs) based systems. The DNN fuzzing leverages various inputs prioritization methods to guide the testing process. The current research mainly focus on constructing testing metrics that symbolize the logical representation of the DNN to guide the generation of test cases, which neglects the potential performance brought by implementing heuristic algorithm. Moreover, the straightforward implementation of queue structure can not represent the metamorphic relationships between generated inputs in DNN fuzzing. Therefore, developing the appropriate heuristic algorithm-based inputs prioritization method is critical to improve the performance of DNN fuzzers. The technique introduces an innovative tree-structure design that schedules inputs from the statistical perspective. Different from traditional DNN testing, the batch pool is maintained in the form of nodes in MCTS. The links between nodes precisely represent the metamorphic relationship between input batches, which indicates the potential value for in-depth search. Furthermore, a novel simulation mechanism is implemented to adapt MCTS in DNN testing, which attain better coverage feedback. The effectiveness of our method is comprehensively investigated on six popular deep learning models from LeNet and VGG families. The comparison experiments are conducted between DeepHunter, TensorFuzz, and DeepSmartFuzzer to demonstrate efficacy on various testing metrics},
   author = {Aoshuang Ye and Lina Wang and Lei Zhao and Jianpeng Ke},
   doi = {https://doi.org/10.1002/int.23072},
   issue = {12},
   journal = {International Journal of Intelligent Systems},
   keywords = {Inputs prioritization,Monte Carlo Tree Search,deep neural networks,fuzz testing},
   pages = {11966-11984},
   title = {Ex2: Monte Carlo Tree Search-based test inputs prioritization for fuzzing deep neural networks},
   volume = {37},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/int.23072},
   year = {2022}
}
@article{Zimmermann2020,
   abstract = {Abstract Machine Learning (ML) and Data Mining (DM) build tools intended to help users solve data-related problems that are infeasible for “unaugmented” humans. Tools need manuals, however, and in the case of ML/DM methods, this means guidance with respect to which technique to choose, how to parameterize it, and how to interpret derived results to arrive at knowledge about the phenomena underlying the data. While such information is available in the literature, it has not yet been collected in one place. We survey three types of work for clustering and pattern mining: (1) comparisons of existing techniques, (2) evaluations of different parameterization options and studies providing guidance for setting parameter values, and (3) work comparing mining results with the ground truth. We find that although interesting results exist, as a whole the body of work on these questions is too limited. In addition, we survey recent studies in the field of community detection, as a contrasting example. We argue that an objective obstacle for performing needed studies is a lack of data and survey the state of available data, pointing out certain limitations. As a solution, we propose to augment existing data by artificially generated data, review the state-of-the-art in data generation in unsupervised mining, and identify shortcomings. In more general terms, we call for the development of a true “Data Science” that—based on work in other domains, results in ML, and existing tools—develops needed data generators and builds up the knowledge needed to effectively employ unsupervised mining techniques. This article is categorized under: Fundamental Concepts of Data and Knowledge > Key Design Issues in Data Mining Ensemble Methods > Structure Discovery Internet > Society and Culture Fundamental Concepts of Data and Knowledge > Motivation and Emergence of Data Mining},
   author = {Albrecht Zimmermann},
   doi = {https://doi.org/10.1002/widm.1330},
   issue = {2},
   journal = {WIREs Data Mining and Knowledge Discovery},
   keywords = {algorithmic comparison,clustering,parameter selection,pattern mining,result verification},
   pages = {e1330},
   title = {Method evaluation, parameterization, and result validation in unsupervised data mining: A critical survey},
   volume = {10},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1330},
   year = {2020}
}
@article{Yan2021,
   abstract = {Abstract In cybersecurity practice, two methodological challenges exist, that is, how to improve validity and reliability of cybersecurity assessment as a weak hypothesis and how to assess and mitigate cybersecurity risks among the K-12 school population as the weakest link. To address these two challenges, the present study aimed to verify the validity and reliability of one newly developed questionnaire, Cybersecurity Judgment Questionnaire for Middle and High School Students, using the Rasch modeling. A total of 398 middle and high school students from the GenCyber Camps participated in this study. The results of the Rasch model analysis show that (1) the Cybersecurity Judgment Questionnaire was unidimensional, measuring one single construct of cybersecurity judgment; (2) the questionnaire had acceptable item construct validity and person construct validity; (3) the item reliability and person reliability of the questionnaire were also acceptable, and (4) a few items might need revisions to increase item difficulty and reduce redundancy, if this would be confirmed by more replication studies with more representative samples. Suggestions for improvement and administration of the questions are discussed.},
   author = {Zheng Yan and Panpan Yang and Yukang Xue and Yaosheng Lou and Melissa Nealon},
   doi = {https://doi.org/10.1002/hbe2.312},
   issue = {5},
   journal = {Human Behavior and Emerging Technologies},
   keywords = {Cybersecurity Judgment Questionnaire,K-12 students,Rasch analysis,cybersecurity,emerging technologies,human behavior,reliability,validity},
   pages = {776-787},
   title = {Validity and reliability of Cybersecurity Judgment Questionnaire for middle and high school students: A validation study with Rasch analysis},
   volume = {3},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbe2.312},
   year = {2021}
}
@article{Zivkovic2023,
   abstract = {Abstract Nowadays, we heavily depend on software, as we utilize it daily. Bugs in the software can cause significant damage, loss of private data and money, and even loss of human lives. Software testing and quality assurance discipline, which belongs to the software engineering domain, aims to ensure that the software is good enough before its release to the market. Therefore, it is vital to ensure that software engineers are highly trained in this domain. Unfortunately, this discipline is frequently neglected throughout software engineering courses. Students are mostly taught just the theoretical principles of software testing and gain little practical experience. Additionally, the software testing lectures are considered to be dull, especially if they are not followed by practical tasks where the students could apply learned techniques and gain hands-on experience. Several tools, games, and collaborative learning environments have been proposed to make the lectures more interesting and increase student engagement. The main goal of this article is to provide a systematic survey of such environments, where each environment was evaluated in terms of major software testing topics covered and the approach used to engage the students.},
   author = {Tamara Zivkovic and Drazen Draskovic and Bosko Nikolic},
   doi = {https://doi.org/10.1002/cae.22657},
   issue = {6},
   journal = {Computer Applications in Engineering Education},
   keywords = {computer science education,engagement,gamification,learning environments,software testing,survey,teaching practices},
   pages = {1497-1521},
   title = {Learning environments in software testing education: An overview},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.22657},
   year = {2023}
}
@article{Oliveira2021,
   abstract = {Abstract People's perception of social robots is essential in determining their responses and acceptance of this type of agent. Currently, there are few instruments validated for the European Portuguese population that measure the perception of social robots. Our goal was to translate, validate, and evaluate the psychometric properties of the Robotic Social Attributes Scale (RoSAS) to European Portuguese. To achieve this goal, we conducted a validation study using a sample of 185 participants. We measured the temporal validity of the scale (over a 2-week interval) and its divergent and convergent validity using the Portuguese Negative Attitudes toward Robots Scale and the Godspeed scales. Our data analysis resulted in a shortened version of the Portuguese RoSAS with 11 items while retaining the original three-factor structure. The scale presented poor to acceptable levels of temporal reliability. We found a positive correlation between the warmth and competence dimensions. Further validation studies are needed to investigate the psychometric properties of this scale.},
   author = {Raquel Oliveira and Patrícia Arriaga and Steven J Stroessner and Ana Paiva},
   doi = {https://doi.org/10.1002/hbe2.311},
   issue = {5},
   journal = {Human Behavior and Emerging Technologies},
   keywords = {Portuguese,RoSAS,perception,social robots,validation},
   pages = {750-758},
   title = {Preliminary validation of the European Portuguese version of the Robotic Social Attributes Scale (RoSAS)},
   volume = {3},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbe2.311},
   year = {2021}
}
@article{Ali2020,
   abstract = {To reduce costs and improve organizational efficiency, the adoption of innovative services such as Cloud services is the current trend in today’s highly competitive global business venture. The aim of the study is to guide the software development organization (SDO) for Cloud-based testing (CBT) adoption. To achieve the aim, this study first explores the determinants and predictors of Cloud adoption for software testing. Grounded on the collected data, this study designs a technology acceptance model using fuzzy multicriteria decision-making (FMCDM) approach. For the stated model development, this study identifies a list of predictors (main criteria) and factors (subcriteria) using systematic literature review (SLR). In the results of SLR, this study identifies seventy subcriteria also known as influential factors (IFs) from a sample of 136 papers. To provide a concise understanding of the facts, this study classifies the identified factors into ten predictors. To verify the SLR results and to rank the factors and predictors, an empirical survey was conducted with ninety-five experts from twenty different countries. The application value in the industrial field and academic achievement of the present study is the development of a general framework incorporating fuzzy set theory for improving MCDM models. The model can be applied to predict organizational Cloud adoption possibility taking various IFs and predictors as assessment criteria. The developed model can be divided into two main parts, ranking and rating. To measure the success or failure contribution of the individual IFs towards successful CBT adoption, the ranking part of the model will be used, while for a complete organizational assessment in order to identify the weak area for possible improvements, the assessment part of the model will be used. Collectively, it can be used as a decision support system to gauge SDO readiness towards successful CBT.},
   author = {Sikandar Ali and Niamat Ullah and Muhammad Faisal Abrar and Zhongguo Yang and Jiwei Huang},
   doi = {https://doi.org/10.1155/2020/6597316},
   issue = {1},
   journal = {Scientific Programming},
   pages = {6597316},
   title = {Fuzzy Multicriteria Decision-Making Approach for Measuring the Possibility of Cloud Adoption for Software Testing},
   volume = {2020},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2020/6597316},
   year = {2020}
}
@article{Rafi2022,
   abstract = {Abstract Testing is a complex phase in DevOps process due to need of an automated process that provides feedback at different strategies of continuous development and operations pipeline. Software organization face several challenges during the testing phase due to lack of understanding on testing best practices for the DevOps paradigm. The objective of this study is to prioritize DevOps best testing practices, which can facilitate the selection of testing practices during DevOps process. To perform this research, we have extended the work done by Hornbeek, using the 15 DevOps testing practices discussed in his study. First, we categorize the test practices against culture, automation, lean, measurement, and sharing (CALMS) pillars of DevOps adoption principles. Next, a questionnaire-based survey was conducted to collect feedback from industry practitioners on the DevOps test practices and their categorization against CALMS criteria. Finally, we applied Interpretive Structure Modeling (ISM) to find the interrelationship between CALMS criteria, and fuzzy TOPSIS was used to prioritize the DevOps test practices that will assist practitioners to better manage the testing activities during DevOps process.},
   author = {Saima Rafi and Muhammad Azeem Akbar and Sajjad Mahmood and Ahmed Alsanad and Abdulrahman Alothaim},
   doi = {https://doi.org/10.1002/smr.2448},
   issue = {5},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,ISM,fuzzy TOPSIS,prioritization,test practices},
   pages = {e2448},
   title = {Selection of DevOps best test practices: A hybrid approach using ISM and fuzzy TOPSIS analysis},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2448},
   year = {2022}
}
@article{Jimnez-Ramrez2023,
   abstract = {Robotic process automation (RPA) has received increasing attention in recent years. It enables task automation by software components, which interact with user interfaces in a similar way to that of humans. An RPA project life cycle is closely resembling a software project one. However, in certain contexts (e.g., business process outsourcing), a testing environment is not always available. Thus, deploying the robots in the production environment entails high risk. To mitigate it, an innovative approach to automatically generate a testing environment and a test suite for an RPA project is presented. The activities of the humans whose processes are to be robotized are monitored and a UI log is confirmed. On one side, the test environment is generated as a fake application, which mimics the real environment by leveraging the UI log information. The control flow of the application is governed by an invisible control layer that decides which image to show depending on the interface actions that it receives. On the other side, the test case checks whether the robot can reproduce the behaviour of the UI log. Promising results were obtained and a number of limitations were identified such that it may be applied in more realistic domains.},
   author = {Andres Jiménez-Ramírez and Jesús Chacón-Montero and Tomasz Wojdynsky and José González Enríquez},
   doi = {https://doi.org/10.1002/smr.2259},
   issue = {3},
   journal = {Journal of Software: Evolution and Process},
   keywords = {automated testing,robotic process automation},
   note = {e2259 smr.2259},
   pages = {e2259},
   title = {Automated testing in robotic process automation projects},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2259},
   year = {2023}
}
@article{Cai2023,
   abstract = {Abstract Test case generation techniques based on adversarial examples are commonly used to enhance the reliability and robustness of image-based and text-based machine learning applications. However, efficient techniques for speech recognition systems are still absent. This paper proposes a family of methods that generate targeted adversarial examples for speech recognition systems. All are based on the firefly algorithm (F), and are enhanced with gauss mutations and / or gradient estimation (F-GM, F-GE, F-GMGE) to fit the specific problem of targeted adversarial test case generation. We conduct an experimental evaluation on three different types of speech datasets, including Google Command, Common Voice and LibriSpeech. In addition, we recruit volunteers to evaluate the performance of the adversarial examples. The experimental results show that, compared with existing approaches, these approaches can effectively improve the success rate of the targeted adversarial example generation. The code is publicly available at https://github.com/HanboCai/FGMGE.},
   author = {Hanbo Cai and Pengcheng Zhang and Hai Dong and Lars Grunske and Shunhui Ji and Tianhao Yuan},
   doi = {https://doi.org/10.1002/stvr.1848},
   issue = {5},
   journal = {Software Testing, Verification and Reliability},
   keywords = {Gauss mutation,adversarial example,firefly algorithm,gradient estimation method,speech recognition,test case generation},
   pages = {e1848},
   title = {Adversarial example-based test case generation for black-box speech recognition systems},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1848},
   year = {2023}
}
@article{Sheikh2022,
   abstract = {Abstract The best practices of agile software development have had a significant positive impact on the quality of software and time-to-delivery. As a result, many leading software companies employ some form of agile software development practices. Some of the most important best practices of agile software development, which have received significant attention in recent years, are automated unit testing (AUT) and test-driven development (TDD). Both of these practices work in conjunction to provide numerous benefits. AUT leads to reduced time to test, discover bugs, fix bugs, and implement new features; wider and measurable test coverage; reproducibility, reusability, consistency, and reliability of tests; improved accuracy, regression testing, parallel testing, faster feedback cycle, reduced cost, and higher team morale. The benefits of TDD include flexible and adaptive program design, cleaner interfaces, higher code quality, maintainability, extensibility, reliability, detailed evolving specification, reduced time on bug fixes and feature implementation, reliable refactoring, and code changes, reduced cost, reduced development time, and increased programmer productivity. Unfortunately, students in introductory programming courses are generally not introduced to AUT and TDD. This leads to the development of bad programming habits and practices which become harder to change later on. By introducing the students earlier to these industry-standard best practices, not only the motivation and interest of students in this area can be increased but also their academic success and job marketability can be enhanced. This paper presents the detailed design and efficacy study of an introductory C++ programming course designed using the principles of AUT and TDD. The paper presents the pedagogical techniques employed to build industry-proven agile software development practices in the students. As part of the paper, all the course material including the source code for the labs and the automated unit tests are being made available to encourage people to incorporate these best practices into their curricula.},
   author = {Waseem Sheikh},
   doi = {https://doi.org/10.1002/cae.22488},
   issue = {3},
   journal = {Computer Applications in Engineering Education},
   keywords = {C++ programming,automated unit testing (AUT),computer engineering education,curriculum design,efficacy study,test-driven development (TDD)},
   pages = {821-851},
   title = {Teaching C++ programming using automated unit testing and test-driven development—Design and efficacy study},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.22488},
   year = {2022}
}
@article{Althomali2021,
   abstract = {Summary Since it is common for the users of a web page to access it through a wide variety of devices—including desktops, laptops, tablets and phones—web developers rely on responsive web design (RWD) principles and frameworks to create sites that are useful on all devices. A correctly implemented responsive web page adjusts its layout according to the viewport width of the device in use, thereby ensuring that its design suitably features the content. Since the use of complex RWD frameworks often leads to web pages with hard-to-detect responsive layout failures (RLFs), developers employ testing tools that generate reports of potential RLFs. Since testing tools for responsive web pages, like ReDeCheck, analyse a web page representation called the Document Object Model (DOM), they may inadvertently flag concerns that are not human visible, thereby requiring developers to manually confirm and classify each potential RLF as a true positive (TP), false positive (FP), or non-observable issue (NOI)—a process that is time consuming and error prone. The conference version of this paper presented Viser, a tool that automatically classified three types of RLFs reported by ReDeCheck. Since Viser was not designed to automatically confirm and classify two types of RLFs that ReDeCheck's DOM-based analysis could surface, this paper introduces Verve, a tool that automatically classifies all RLF types reported by ReDeCheck. Along with manipulating the opacity of HTML elements in a web page, as does Viser, the Verve tool also uses histogram-based image comparison to classify RLFs in web pages. Incorporating both the 25 web pages used in prior experiments and 20 new pages not previously considered, this paper's empirical study reveals that Verve's classification of all five types of RLFs frequently agrees with classifications produced manually by humans. The experiments also reveal that Verve took on average about 4 s to classify any of the RLFs among the 469 reported by ReDeCheck. Since this paper demonstrates that classifying an RLF as a TP, FP, or NOI with Verve, a publicly available tool, is less subjective and error prone than the same manual process done by a human web developer, we argue that it is well-suited for supporting the testing of complex responsive web pages.},
   author = {Ibrahim Althomali and Gregory M Kapfhammer and Phil McMinn},
   doi = {https://doi.org/10.1002/stvr.1756},
   issue = {4},
   journal = {Software Testing, Verification and Reliability},
   keywords = {automated layout failure classification,automated web testing,empirical studies,responsive layout failures,responsive web design,web presentation failures},
   note = {e1756 stvr.1756},
   pages = {e1756},
   title = {Automated visual classification of DOM-based presentation failure reports for responsive web pages},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1756},
   year = {2021}
}
@article{Abid2022,
   abstract = {Abstract The COVID-19 pandemic has emerged as a highly transmissible disease which has caused a disastrous impact worldwide by adversely affecting the global economy, health, and human lives. This sudden explosion and uncontrolled worldwide spread of COVID-19 has revealed the limitations of existing healthcare systems regarding handling public health emergencies. As governments seek to effectively re-establish their economies, open workplaces, ensure safe travels and progressively return to normal life, there is an urgent need for technologies that may alleviate the severity of the losses. This article explores a promising solution for secure Digital Health Certificate, called NovidChain, a Blockchain-based privacy-preserving platform for COVID-19 test/vaccine certificates issuing and verifying. More precisely, NovidChain incorporates several emergent concepts: (i) Blockchain technology to ensure data integrity and immutability, (ii) self-sovereign identity to allow users to have complete control over their data, (iii) encryption of Personally Identifiable Information to enhance privacy, (iv) W3C verifiable credentials standard to facilitate instant verification of COVID-19 proof, and (v) selective disclosure concept to permit user to share selected pieces of information with trusted parties. Therefore, NovidChain is designed to meet a high level of protection of personal data, in compliant with the GDPR and KYC requirements, and guarantees the user's self-sovereignty, while ensuring both the safety of populations and the user's right to privacy. To prove the security and efficiency of the proposed NovidChain platform, this article also provides a detailed technical description, a proof-of-concept implementation, different experiments, and a comparative evaluation. The evaluation shows that NovidChain provides better financial cost and scalability results compared to other solutions. More precisely, we note a high difference in time between operations (i.e., between 46\% and 56\%). Furthermore, the evaluation confirms that NovidChain ensures security properties, particularly data integrity, forge, binding, uniqueness, peer-indistinguishability, and revocation.},
   author = {Amal Abid and Saoussen Cheikhrouhou and Slim Kallel and Mohamed Jmaiel},
   doi = {https://doi.org/10.1002/spe.2983},
   issue = {4},
   journal = {Software: Practice and Experience},
   keywords = {Blockchain,COVID-19 pandemic,GDPR,KYC,W3C verifiable credentials,digital health certificate,privacy self-sovereignty},
   pages = {841-867},
   title = {NovidChain: Blockchain-based privacy-preserving platform for COVID-19 test/vaccine certificates},
   volume = {52},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2983},
   year = {2022}
}
@article{Villalobos-Arias2020,
   abstract = {Model-based testing (MBT) automates the design and generation of test cases from a model. This process includes model building, test selection criteria, test case generation, and test case execution stages. Current tools support this process at various levels of automation, most of them supporting three out of four stages. Among them is MBT4J, a platform that extends ModelJUnit with several techniques, offering a high level of automation for testing Java applications. In this study, the authors evaluate the efficacy of the MBT4J platform, in terms of the number of test cases generated, errors detected, and coverage metrics. A case study is conducted using two open-source Java systems from public repositories, and 15 different configurations. MBT4J was able to automatically generate five models from the source code. It was also able to generate up to 2025 unique test cases for one system and up to 1044 for the other, resulting in 167 and 349 failed tests, respectively. Transition and transition pair coverage reached 100\% for all models. Code coverage ranged between 72 and 84\% for the one system and between 59 and 76\% for the other. The study found that Greedy and Random were the most effective testers for finding errors.},
   author = {Leonardo Villalobos-Arias and Christian Quesada-López and Alexandra Martínez and Marcelo Jenkins},
   doi = {https://doi.org/10.1049/iet-sen.2019.0036},
   issue = {2},
   journal = {IET Software},
   keywords = {Java,Java applications,MBT4J platform,ModelJUnit,automation support,model building,model-based testing platform,open-source Java systems,program testing,test case execution,test case generation,test selection criteria},
   pages = {115-128},
   title = {Evaluation of a model-based testing platform for Java applications},
   volume = {14},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2019.0036},
   year = {2020}
}
@article{Yao2022,
   abstract = {With the development of computer science and technology, online education is gradually accepted by more and more users, and has become one of the main teaching methods in schools under the influence of the epidemic in recent years. In addition, online vocational education or higher education has also captured the psychology of national learning, and online tutoring has gradually entered into nonstudent groups such as office workers. Although online education has the advantages of low cost and no limitation of time and space, it also has some problems, such as low classroom satisfaction, delayed feedback from students, and poor teaching effect. Therefore, this paper aims at this problem by combining the development background of foreign intelligent answer system and self-test system, and introducing it into our computer application, using artificial intelligence for natural language processing, aiming at online teaching students cannot get timely answer, and self-test questions intelligent system design. The final experimental results show that the system effectively alleviates the problems of students’ delay in answering questions and poor self-test effect, and increases the satisfaction of online classroom by 13\%.},
   author = {Xiaoyan Yao},
   doi = {https://doi.org/10.1155/2022/2156111},
   issue = {1},
   journal = {Advances in Multimedia},
   pages = {2156111},
   title = {[Retracted] Design and Research of Artificial Intelligence in Multimedia Intelligent Question-Answering System and Self-Test System},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/2156111},
   year = {2022}
}
@article{Daoudagh2023,
   abstract = {Abstract Automated testing in DevOps represents a key factor for providing fast release of new software features assuring quality delivery. In this paper, we introduce DOXAT, an automated framework for continuous development and testing of access control mechanisms based on the XACML standard. It leverages mutation analysis for the selection and assessment of the test strategies and provides automated facilities for test oracle definition, test execution, and results analysis, in order to speedup and automate the Plan, Code, Build, and Test phases of DevOps process. We show the usage of the framework during the planning and testing phases of the software development cycle of a PDP example.},
   author = {Said Daoudagh and Francesca Lonetti and Eda Marchetti},
   doi = {https://doi.org/10.1002/smr.2306},
   issue = {3},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,XACML,access control systems,continuous development and testing,mutation analysis},
   note = {e2306 smr.2306},
   pages = {e2306},
   title = {An automated framework for continuous development and testing of access control systems},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2306},
   year = {2023}
}
@article{Alekseev2024,
   abstract = {Abstract Software projects grow larger every year, which, in turn, makes the testing process harder. One of the most useful methods for testing large projects is unit-test generation. However, some tests can repeatedly cover the same parts of the code, making it difficult to maintain a growing test codebase. In software testing, test suite minimization plays a crucial role in reducing the cost of testing and improving the efficiency of the testing process. In this paper, we provide an extensible minimization engine that detects redundant tests using one of the supported minimization algorithms without changing the coverage metrics. We also performed a comprehensive analysis of existing approaches and techniques, developed an engine structure, and implemented multiple algorithms of different kinds. Finally, we evaluated our tool on various open-source projects to demonstrate its effectiveness and efficiency.},
   author = {Yaroslav Alekseev and Mikhail Onischuck and Arseniy Zorin and Vitaliy Chernyi and Evgeniy Iliyn and Vladimir Itsykson},
   doi = {https://doi.org/10.1002/smr.2621},
   issue = {6},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Java,REST API,automation,code coverage,test suite minimization},
   pages = {e2621},
   title = {ATSM: A coverage-based framework and a tool for test suite minimization},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2621},
   year = {2024}
}
@inbook{FIORE2022,
   abstract = {Summary The simultaneous achievement of security and efficiency makes Verifiable computation (VC) an intriguing notion that attracts interest from both the theory and the practice sides of computer science. This chapter presents the definition of VC, and describes a construction of VC for computations expressible as polynomial-size arithmetic circuits. It focuses on showing how to construct a VC scheme for an important class of computations that are arithmetic circuits of polynomial size. The chapter presents the notion of a succinct non-interactive argument (SNARG) and shows how any SNARG for NP can be used to build a VC for polynomial time functions. It presents a construction that follows a modular approach based on combining two objects: an information theoretic proof system working in an ideal abstract model and a cryptographic component that turns the information-theoretic proof into an efficient computationally sound argument.},
   author = {Dario FIORE},
   doi = {https://doi.org/10.1002/9781394188369.ch11},
   isbn = {9781394188369},
   booktitle = {Asymmetric Cryptography},
   keywords = {NP-complete language,cryptographic component,information theoretic proof system,polynomial time functions,polynomial-size arithmetic circuits,succinct non-interactive argument,verifiable computation},
   pages = {257-281},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Verifiable Computation and Succinct Arguments for NP},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781394188369.ch11},
   year = {2022}
}

@article{Sadri-Moshkenani2022,
   abstract = {Summary A cyber-physical system (CPS) is a collection of computing devices that communicate with each other, operate in the target environment via actuators and interact with the physical world through sensors in a feedback loop. CPSs need to be safe and reliable and function in accordance with their requirements. Testing, focusing on a CPS model and/or its code, is the primary approach used by engineers to achieve this. Generating, selecting and prioritizing test cases that can reveal faults in CPSs, from the wide range of possible input values and stimuli that affect their operation, are of central importance in this process. To date, however, in our search of the literature, we have found no comprehensive survey of research on test case generation, selection and prioritization for CPSs. In this article, therefore, we report the results of a survey of approaches for generating, selecting and prioritizing test cases for CPSs; the results illustrate the progress that has been made on these approaches to date, the properties that characterize the approaches and the challenges that remain open in these areas of research.},
   author = {Zahra Sadri-Moshkenani and Justin Bradley and Gregg Rothermel},
   doi = {https://doi.org/10.1002/stvr.1794},
   issue = {1},
   journal = {Software Testing, Verification and Reliability},
   keywords = {cyber-physical system,embedded-control systems,test case generation,test case prioritization,test case selection,testing},
   pages = {e1794},
   title = {Survey on test case generation, selection and prioritization for cyber-physical systems},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1794},
   year = {2022}
}
@article{Miranda2022,
   author = {Breno Miranda and Javier Tuya and Alejandra Garrido},
   doi = {https://doi.org/10.1002/smr.2510},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   pages = {e2510},
   title = {Guest editors' introduction to the special issue “Automatic Software Testing from the Trenches”},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2510},
   year = {2022}
}
@article{Mahesh2022,
   abstract = {As a result of technology improvements, various features have been collected for heart disease diagnosis. Large data sets have several drawbacks, including limited storage capacity and long access and processing times. For medical therapy, early diagnosis of heart problems is crucial. Disease of heart is a devastating human disease that is quickly increasing in developed and also developing countries, resulting in death. In this type of disease, the heart normally fails to provide enough blood to different body parts in order to allow them to perform their regular functions. Early, as well as, proper diagnosis of this condition is very critical for averting further damage and also to save patients’ lives. In this work, machine learning (ML) is utilized to find out whether a person has cardiac disease or not. Both the types of ensemble classifiers, namely, homogeneous as well as heterogeneous classifiers (formed by combining two separate classifiers), have been implemented in this work. The data mining preprocessing using Synthetic Minority Oversampling Technique (SMOTE) has been employed to cope with the imbalance problem of the class as well as noise. The proposed work has two steps. SMOTE is used in the initial phase to reduce the impact of data imbalance and the second phase is classifying data using Naive Bayes (NB), decision tree (DT) algorithms, and their ensembles. The experimental results demonstrate that the AdaBoost-Random Forest classifier provides 95.47\% accuracy in the early detection of heart disease.},
   author = {T R Mahesh and V Dhilip Kumar and V Vinoth Kumar and Junaid Asghar and Oana Geman and G Arulkumaran and N Arun},
   doi = {https://doi.org/10.1155/2022/9005278},
   issue = {1},
   journal = {Computational Intelligence and Neuroscience},
   pages = {9005278},
   title = {AdaBoost Ensemble Methods Using K-Fold Cross Validation for Survivability with the Early Detection of Heart Disease},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/9005278},
   year = {2022}
}
@article{Luckow2020,
   abstract = {Summary We describe techniques based on symbolic execution for finding software vulnerabilities that are due to algorithmic complexity. Such vulnerabilities allow an attacker to mount denial-of-service attacks to deny service to benign users or to otherwise disable a software system. The techniques use an efficient guided symbolic execution of a program to compute bounds on the worst-case complexity (for increasing input sizes) and to generate test values that trigger the worst-case behaviours. The resulting bounds are fitted to a function to obtain a prediction of the worst-case program behaviour at any input size. Scalability is achieved by using path policies that guide the symbolic execution towards worst-case paths. The policies are learned from the worst-case results obtained with exhaustive exploration at small input sizes and are applied to guide exploration at larger input sizes, where unguided exhaustive exploration is not possible. To achieve precision in the analysis, the path policies take into account the history of choices made along the path when deciding which branch to execute next. Furthermore, the computation is contextpreserving, meaning that the decision for each branch depends on the history computed with respect to the enclosing method. We further report preliminary results on a complementary technique that uses machine learning for building the path policies that guide the search. The techniques are implemented in open-source projects that build on the Symbolic Pathfinder tool for analysing Java programs. Experimental evaluation shows that the techniques can find vulnerabilities in complex Java programs and can outperform previous symbolic approaches.},
   author = {Kasper Luckow and Rody Kersten and Corina Pasareanu},
   doi = {https://doi.org/10.1002/stvr.1716},
   issue = {7-8},
   journal = {Software Testing, Verification and Reliability},
   keywords = {complexity analysis,guided exploration,machine learning,symbolic execution},
   note = {e1716 stvr.1716},
   pages = {e1716},
   title = {Complexity vulnerability analysis using symbolic execution},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1716},
   year = {2020}
}
@article{AlGhamdi2023,
   abstract = {Abstract The performance of large-scale systems must be thoroughly tested under various levels of workload, as load-related issues can have a disastrous impact on the system. However, load testing often requires a large amount of time, running from hours to even days. In our prior work, we reduced the execution time of a load test by detecting repetitiveness in individual performance metric values, such as CPU utilization, that are observed during the test. However, as we explain in this paper, disregarding combinations of performance metrics may miss important information about the load-related behavior of a system. In this paper we revisit our prior approach, by proposing an approach that reduces the execution time of a load test by detecting whether a test no longer exercises new combinations of the observed performance metrics. We study three open source systems, in which we use our new and prior approaches to reduce the execution time of a 24-hour load test. We show that our new approach is capable of reducing the execution time of the test to less than 8.5 hours, while preserving a coverage of at least 95\% of the combinations that are observed between the performance metrics during the 24-hour tests.},
   author = {Hammam M AlGhamdi and Cor-Paul Bezemer and Weiyi Shang and Ahmed E Hassan and Parminder Flora},
   doi = {https://doi.org/10.1002/smr.2276},
   issue = {3},
   journal = {Journal of Software: Evolution and Process},
   keywords = {load testing,performance analysis,performance testing},
   note = {e2276 smr.2276},
   pages = {e2276},
   title = {Towards reducing the time needed for load testing},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2276},
   year = {2023}
}
@article{Amankwah2023,
   abstract = {Abstract Previous studies have demonstrated the usefulness of employing automated static analysis tools (ASAT) and techniques to detect security bugs in software systems. However, these studies are usually focused on analyzing the effectiveness of the tools using open-source tools based on C/C++ source code. The choice for making an appropriate decision on the most suitable tool for bug detection in Java code software remains a relatively unexplored domain. To address this deficiency, this study empirically evaluates eight widely used ASATs, namely, Findbug, PMD, YASCA, LAPSE+, JLint, Bandera, ESC/Java, and Java Pathfinder using the Juliet Test Suite (Test Suite v1.2). Additionally, we assessed the performance of the detection capabilities for the aforementioned bug detection tools using robust performance measures such as precision, recall, Youden index, and the OWASP web benchmark evaluation (WBE). The experimental results show that the tools obtain precision values ranging from 83\% to 90.7\% based on the studied datasets. Specifically, the Java Pathfinder achieves the best precision score of 90.7\%, followed by YASCA and Bandera with a precision score of 88.7\% and 83\%, respectively. Similarly, Bandera, ESC/Java, and Java Pathfinder obtain a Youden index of 0.8, which indicates the effectiveness of the tools in detecting security bugs in Java source code.},
   author = {Richard Amankwah and Jinfu Chen and Heping Song and Patrick Kwaku Kudjo},
   doi = {https://doi.org/10.1002/spe.3181},
   issue = {5},
   journal = {Software: Practice and Experience},
   keywords = {SAMATE,bug detection,common weakness enumeration,static analysis tools},
   pages = {1125-1143},
   title = {Bug detection in Java code: An extensive evaluation of static analysis tools using Juliet Test Suites},
   volume = {53},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3181},
   year = {2023}
}
@article{Kim2020,
   abstract = {Regression testing is an important but costly activity for verifying a programme with the changed code. Regression test selection (RTS) aims to reduce this cost by selecting only the test cases affected by the changes. Among the several ways of selecting such affected test cases, call graphs have been statically constructed to select the test cases at the method-level granularity. However, RTS techniques will reduce the cost of regression testing less than expected unless the call graphs are efficiently one-to-one matched with the test cases. In this study, the authors propose overlap-aware rapid type analysis (ORTA). ORTA is designed to minimise the redundant cost of creating the matched call graphs using rapid type analysis (RTA). The one-to-one matching and ORTA were evaluated on 1487 commits selected from 30 Java projects. RTA-based RTS with the one-to-one matching selected 46.90\% fewer test cases with 2.76\% longer end-to-end time of regression testing than without the one-to-one matching. The time increased with the one-to-one matching was reduced by 22.58\% when ORTA substituted for RTA. ORTA achieved the cost reduction while removing 82.77\% of the duplicate edges that RTA created on 993 commits.},
   author = {Mingwan Kim and Jongwook Jeong and Neunghoe Kim and Hoh Peter In},
   doi = {https://doi.org/10.1049/iet-sen.2018.5442},
   issue = {4},
   journal = {IET Software},
   keywords = {Java,ORTA,affected test cases,call graphs,fewer test cases,important but costly activity,matching selected 46,overlap-aware rapid type analysis,program testing,redundant cost,regression analysis,regression test selection,regression testing,software maintenance},
   pages = {423-432},
   title = {Overlap-aware rapid type analysis for constructing one-to-one matched call graphs in regression test selection},
   volume = {14},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5442},
   year = {2020}
}
@article{Luz2024,
   abstract = {Summary Graphic processors offer an accessible solution for high-performance computing, addressing challenges across various fields. The Compute Unified Device Architecture (CUDA) programming model has emerged to enhance the performance of general-purpose applications on graphic processors. However, developing CUDA programs is far from straightforward, and developers' lack of experience in parallel programming has led to numerous issues. This article presents a structural testing model and criteria to improve the quality of CUDA programs. These criteria facilitate the selection of test cases and aid in identifying faults. The ValiCUDA tool was developed to implement and validate this testing model and criteria. This tool instruments and analyzes programs, generating the necessary elements for each testing criterion. It also facilitates program execution and evaluation of criterion coverage. A statistical validation experiment assessed these criteria' effectiveness, cost, and strength metrics. The results demonstrate that the criteria can identify nontrivial faults in CUDA programs and assist testers in their testing endeavors for such applications.},
   author = {Helder J F Luz and Paulo S L Souza and Simone R S Souza},
   doi = {https://doi.org/10.1002/cpe.8105},
   issue = {14},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {CUDA,GPU,data flow testing,heterogeneous parallel programming,structural testing criteria,testing model,testing tool},
   pages = {e8105},
   title = {Structural testing for CUDA programming model},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.8105},
   year = {2024}
}
@article{Schulz2020,
   abstract = {Summary Directly affecting the user experience, performance is a crucial aspect of today's software applications. Representative load testing allows to effectively test and preserve the performance before delivery by mimicking the actually expected workload. In the literature, various approaches have been proposed for extracting representative load tests from recorded user sessions. However, these approaches require manual parameterization for specifying input data and adjusting static properties such as a request's domain name. This manual effort accumulates when load tests need to be updated due to changing production workloads and APIs. In this paper, we address the reduction of the maintenance effort for representative load testing. We introduce input data and properties annotations (IDPAs) that store manual parameterizations and can be evolved automatically. Experts only have to parameterize extracted load tests initially. For dealing with API changes, we develop approaches to evolve IDPAs for the types of changes described in the literature. We evaluated our approach in two experimental studies, by deriving effort estimation models, and in an industrial case study including four different software projects. Our evaluation shows that IDPAs can parameterize generated load tests for restoring the representativeness, especially for applications with workloads dominated by request orders and rates. The maintenance effort can be reduced from a quadratic cumulative effort over time to a linear cumulative effort for a typical mix of API changes. Furthermore, we were able to express all parameterizations required by the industrial projects using the IDPA but also had to integrate extensions using the provided extension mechanisms.},
   author = {Henning Schulz and André van Hoorn and Alexander Wert},
   doi = {https://doi.org/10.1002/stvr.1712},
   issue = {1},
   journal = {Software Testing, Verification and Reliability},
   keywords = {load test parameterization,load testing,workload evolution,workload model extraction},
   note = {e1712 stvr.1712},
   pages = {e1712},
   title = {Reducing the maintenance effort for parameterization of representative load tests using annotations},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1712},
   year = {2020}
}
@article{Climent2023,
   abstract = {Abstract In this paper, we focus on developing automatic assessment (AA) for a topic that has some difficulties in its practical assessment: object oriented programming (OOP). For evaluating that the OOP principles have been correctly applied to a real application, we use unit testing. In this paper, we focus on prioritizing that the students understand and apply correctly complex OOP principles and that they design properly the classes (including their relationships). In addition, we focus on the Python programming language rather than the typical previous works' focus in this area. Thus, we present a real case study of a practical assignment, in which the students have to implement characters for a video game. This assignment has the particularities and advantages that it is incremental and that it applies all four OOP principles within a single assignment. We also present its solution with the UML class diagram description. Furthermore, we provide unit testing for this case study and give general advice for generalizing the unit tests to other real case scenarios. Finally, we corroborate the effectiveness of our approach with positive student evaluations.},
   author = {Laura Climent and Alejandro Arbelaez},
   doi = {https://doi.org/10.1002/cae.22642},
   issue = {5},
   journal = {Computer Applications in Engineering Education},
   keywords = {automatic assessment,object oriented programming,python programming language,unit testing},
   pages = {1321-1338},
   title = {Automatic assessment of object oriented programming assignments with unit testing in Python and a real case assignment},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.22642},
   year = {2023}
}
@article{Fontes2023,
   abstract = {Abstract Machine learning (ML) may enable effective automated test generation. We characterize emerging research, examining testing practices, researcher goals, ML techniques applied, evaluation, and challenges in this intersection by performing. We perform a systematic mapping study on a sample of 124 publications. ML generates input for system, GUI, unit, performance, and combinatorial testing or improves the performance of existing generation methods. ML is also used to generate test verdicts, property-based, and expected output oracles. Supervised learning—often based on neural networks—and reinforcement learning—often based on Q-learning—are common, and some publications also employ unsupervised or semi-supervised learning. (Semi-/Un-)Supervised approaches are evaluated using both traditional testing metrics and ML-related metrics (e.g., accuracy), while reinforcement learning is often evaluated using testing metrics tied to the reward function. The work-to-date shows great promise, but there are open challenges regarding training data, retraining, scalability, evaluation complexity, ML algorithms employed—and how they are applied—benchmarks, and replicability. Our findings can serve as a roadmap and inspiration for researchers in this field.},
   author = {Afonso Fontes and Gregory Gay},
   doi = {https://doi.org/10.1002/stvr.1845},
   issue = {4},
   journal = {Software Testing, Verification and Reliability},
   keywords = {automated test generation,machine learning,test case generation,test input generation,test oracle generation},
   pages = {e1845},
   title = {The integration of machine learning into automated test generation: A systematic mapping study},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1845},
   year = {2023}
}
@article{Tang2023,
   abstract = {Abstract Automated program assessment systems have been widely adopted in many universities. Many of these systems judge the correctness of student programs by comparing their actual outputs with predefined expected outputs for selected test inputs. A common weakness of such systems is that student programs would be marked as incorrect as long as their outputs deviate from the predefined ones, even if the deviations are only minor, insignificant, and considered acceptable by a human assessor that the programs have satisfied the specifications. This critical weakness caused undue frustration to students and undesirable pedagogical consequences that undermine these systems’ benefits. To address this issue, we developed an improved mechanism for program output comparison to serve as a versatile test oracle that brings the results of automated assessment much closer to those of human assessors. We evaluated the new mechanism in real programming classes using an existing automated program assessment system. We found that the new mechanism achieved zero false-positive error (did not wrongly accept any incorrect output) and very low (0\%–0.02\%) false-negative error (that wrongly rejected correct outputs), with very high accuracy (99.8\%–100\%) in correctly recognizing outputs deemed acceptable by instructors. This represents a major improvement over an existing assessment mechanism, which had 56.4\%–64.1\% false-negative error with an accuracy of 25.4\%–40.9\%. Moreover, about 67\%–96\% of students achieved their best results in their first attempt, which could be encouraging to them and reduce their frustration. Furthermore, students generally welcomed the new assessment mechanism and agreed it was beneficial to their learning.},
   author = {Chung M Tang and Yuen T Yu and Chung K Poon},
   doi = {https://doi.org/10.1002/cae.22577},
   issue = {1},
   journal = {Computer Applications in Engineering Education},
   keywords = {automated program assessment system,computer science education,learning computer programming,program assessment,test oracle},
   pages = {176-199},
   title = {An automated system with a versatile test oracle for assessing student programs},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.22577},
   year = {2023}
}
@article{Shah2022,
   abstract = {It has been a software trend to build large-scale complex systems with high reliability. Due to the size of the software and the dynamic requirements of the stakeholders, it becomes hard to test those software systems manually. This may lead the software to fatal failures and cause irrecoverable catastrophic damage. To be safe, the software system must be investigated thoroughly before it is too late. Test sequence generation for Unified Modeling Language (UML) class models from their semiformal Object Constraint Language specifications can be helpful in identifying the defects in the early phase of the software life cycle. The existing approaches suffer from inherent problems of exhaustive exploration of finite state machines (infeasible paths, exponential number of test sequences, and uncertainty of completion of testing). Evolutionary algorithms can greatly help by optimizing the test sequences to get optimal coverage, minimal cost, and higher quality. The proposed approach helps us to improve the testing of Unified Modeling Language (UML) model-based software, by testing the conformance to semiformal class operation contract specifications (specified in the form of Object Management Group (OMG) standard and Object Constraint Language (OCL) semiformal language). The presented research achieved two main goals: (1) automation of testing process and conformance to standards of the current technique of test sequence generation, bridging the gap between the research and industry; (2) improvement in the state of the art approach through the application of multiobjective genetic algorithms (MOGAs). A case study along with the results achieved through the proposed technique is presented as well, clearly reflecting the significance of the proposed research.},
   author = {Syed Muhammad Saqlain Shah and Rehan Farooq and Abdullah Alharbi and Hashem Alyami and Islam Zada and Faiz Ali Shah},
   doi = {https://doi.org/10.1155/2022/3708422},
   issue = {1},
   journal = {Scientific Programming},
   pages = {3708422},
   title = {Multiobjective Genetic Algorithm for Class Testing using OCL Class Contract Specifications: A Framework},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/3708422},
   year = {2022}
}
@article{Nguyen2021,
   abstract = {Summary Handling hardware-dependent properties at a low level is usually required in developing microcontroller-based applications. One of these hardware-dependent properties is cautions, which are described in microcontrollers hardware manuals. The process of verifying these cautions is performed manually, as there is currently no single tool that can directly handle this task. This research aims at automating the verification of these cautions. To obtain the typical cautions of microcontrollers, we investigate two sections which have a considerable number of required cautions in the hardware manual of a popular microcontroller. Subsequently, we analyse these cautions and categorize them into several groups. Based on this analysis, we propose a semi-automatic approach for verifying the cautions which integrates two static programme analysis techniques (i.e., pattern matching and abstract interpretation). To evaluate our approach, we conducted experiments with generated source code, benchmark source code, and industrial source code. The generated source code, which was created automatically based on several aspects of the C programme, was used to evaluate the performance of the approach based on these aspects. The benchmark and the industrial source code, which were provided by Aisin Software Co., Ltd., were used to assess the feasibility and applicability of the approach. The results show that all expected violations in the benchmark source code were detected. Unexpected but real violations in the benchmark programme were also detected. For the industrial source code, the approach successfully handled and detected most of the expected violations. These results show that the approach is promising in verifying the cautions.},
   author = {Thuy Nguyen and Takashi Tomita and Junpei Endo and Toshiaki Aoki},
   doi = {https://doi.org/10.1002/stvr.1788},
   issue = {8},
   journal = {Software Testing, Verification and Reliability},
   keywords = {abstract interpretation,caution,microcontroller,pattern matching},
   pages = {e1788},
   title = {Integrating pattern matching and abstract interpretation for verifying cautions of microcontrollers},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1788},
   year = {2021}
}
@article{Xu2022,
   abstract = {Abstract Named entity recognition (NER) is a widely used natural language processing technique; it plays a key role in information extraction from sentences. To be able to test the correctness of NER systems is important, but it is expensive because an automated test oracle is normally unavailable. To address the oracle problem, this study proposes to apply metamorphic testing (MT). The authors conduct a case study with Litigant, an industrial NER system of the Ant Group, and show that MT can effectively detect real-life bugs in the absence of an ideal oracle. The authors further investigate the causes for a series of entity recognition failures detected. Outcomes of this research further justify the application of MT to the natural language processing domain as well as provide hints for practitioners to improve the quality process of their NER systems.},
   author = {Yezi Xu and Zhi Quan Zhou and Xiaoxia Zhang and Jing Wang and Mingyue Jiang},
   doi = {https://doi.org/10.1049/sfw2.12058},
   issue = {4},
   journal = {IET Software},
   keywords = {metamorphic relations,metamorphic testing,named entity recognition,natural language processing},
   pages = {386-404},
   title = {Metamorphic testing of named entity recognition systems: A case study},
   volume = {16},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12058},
   year = {2022}
}
@article{Izumi2023,
   abstract = {Abstract Digital TV (DTV) receivers are usually submitted to testing systems for conformity and robustness assessment, and their approval implies correct operation under a given DTV specification protocol. However, many broadcasters inadvertently misconfigure their devices and transmit the wrong information concerning data structures and protocol format. Since most receivers were not designed to operate under such conditions, malfunction and incorrect behaviour may be noticed, often recognized as field problems, thus compromising a given system's operation. Moreover, the way those problems are usually introduced in DTV signals presents some randomness, but with known restrictions given by the underlying transport protocols used in DTV systems, which resembles fuzzing techniques. Indeed, everything may happen since any deviation can incur problems, depending on each specific implementation. This error scenario is addressed here, and a novel receiver robustness evaluation methodology based on non-compliance tests using grammar-based guided fuzzing is proposed. In particular, devices are submitted to unforeseen conditions and incorrect configuration. They are created with guided fuzzing based on real problems, protocol structure, and system architecture to provide resources for handling them, thus ensuring correct operation. Experiments using such a fuzzing scheme have shown its efficacy and provided opportunities to improve robustness regarding commercial DTV platforms.},
   author = {Fabricio Izumi and Eddie B de Lima Filho and Lucas C Cordeiro and Orlewilson Maia and Rômulo Fabrício and Bruno Farias and Aguinaldo Silva},
   doi = {https://doi.org/10.1002/stvr.1833},
   issue = {1},
   journal = {Software Testing, Verification and Reliability},
   keywords = {digital TV,fuzzing,robustness testing,testing methodology,transport stream},
   pages = {e1833},
   title = {A fuzzing-based test-creation approach for evaluating digital TV receivers via transport streams},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1833},
   year = {2023}
}
@article{Lee2023,
   abstract = {Abstract The development of artificial intelligence (AI) agents capable of human-level understanding of video content and conducting conversations with humans on this basis is a promising application that people expect. However, this is a challenging task that requires the holistic integration of multimodal information with temporal dependencies and reasoning, as well as social and physical commonsense. In addition, the development of appropriate systematic evaluation methods is essential. In this context, we introduce the Video Turing Test (VTT), a blind test used to evaluate human-likeness in terms of video comprehension ability. Moreover, we propose Vincent as a video understanding AI. We explain the configuration of VTT, the architecture of Vincent to prepare for VTT and the proposed evaluation methods for video comprehension. We also estimate the current intelligence level of AI based on our results and discuss future research directions.},
   author = {Minsu Lee and Yu-Jung Heo and Seongho Choi and Woo Suk Choi and Byoung-Tak Zhang},
   doi = {https://doi.org/10.1002/aaai.12128},
   issue = {4},
   journal = {AI Magazine},
   pages = {537-554},
   title = {Video Turing Test: A first step towards human-level AI},
   volume = {44},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12128},
   year = {2023}
}
@article{Erharter2021,
   abstract = {Abstract In geotechnical field investigations, cone penetration tests (CPT) are increasingly used for ground characterization of fine-grained soils. Test results are different parameters that are typically visualized in CPT based data interpretation charts. In this paper we propose a novel methodology which is based on supervised machine learning that permits a redefinition of the boundaries within these charts to account for unique soil conditions. We train ensembles of randomly generated artificial neural networks to classify six soil types based on a database of hundreds of CPT tests from Austria and Norway. After training we combine the multiple unique solutions for this classification problem and visualize the new decision boundaries in between the soil types. The generated boundaries between soil types are comprehensible and are a step towards automatically adjusted CPT interpretation charts for specific local conditions.},
   author = {Georg H Erharter and Simon Oberhollenzer and Anna Fankhauser and Roman Marte and Thomas Marcher},
   doi = {https://doi.org/10.1111/mice.12662},
   issue = {4},
   journal = {Computer-Aided Civil and Infrastructure Engineering},
   pages = {489-503},
   title = {Learning decision boundaries for cone penetration test classification},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12662},
   year = {2021}
}
@article{Amalfitano2019,
   abstract = {Abstract Despite the high relevance of traceability in software processes, the activities of traceability creation and management are not always adequately supported in practice. The lack of integration between the tools adopted in the development processes is one of the main causes of such an ineffective management, where traceability relationships are still manually generated and maintained. In this paper we present an industrial experience we performed for improving the traceability management in a testing process performed in the Fiat Chrysler Automobiles company. In this context, we carried out a process for analyzing and identifying the main issues due to the ineffective traceability management and proposed a solution for addressing them. We designed and implemented a software architecture for integrating the existing application lifecycle management platform with the tools used in the process with the aim of automating the process execution and the traceability links management. The new architecture was validated by a case study that showed how the integration solution produced beneficial effects on quality attributes of the testing process.},
   author = {Domenico Amalfitano and Vincenzo De Simone and Raffaele Rodolfo Maietta and Stefano Scala and Anna Rita Fasolino},
   doi = {https://doi.org/10.1002/smr.2171},
   issue = {6},
   journal = {Journal of Software: Evolution and Process},
   keywords = {automotive,industrial case study,software process improvement,tool integration,traceability management},
   note = {e2171 smr.2171},
   pages = {e2171},
   title = {Using tool integration for improving traceability management testing processes: An automotive industrial experience},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2171},
   year = {2019}
}
@article{Boussaa2020,
   abstract = {SUMMARY Generative software development has paved the way for the creation of multiple code generators that serve as a basis for automatically generating code to different software and hardware platforms. In this context, the software quality becomes highly correlated to the quality of code generators used during software development. Eventual failures may result in a loss of confidence for the developers, who will unlikely continue to use these generators. It is then crucial to verify the correct behaviour of code generators in order to preserve software quality and reliability. In this paper, we leverage the metamorphic testing approach to automatically detect inconsistencies in code generators via so-called “metamorphic relations”. We define the metamorphic relation (i.e., test oracle) as a comparison between the variations of performance and resource usage of test suites running on different versions of generated code. We rely on statistical methods to find the threshold value from which an unexpected variation is detected. We evaluate our approach by testing a family of code generators with respect to resource usage and performance metrics for five different target software platforms. The experimental results show that our approach is able to detect, among 95 executed test suites, 11 performance and 15 memory usage inconsistencies.},
   author = {Mohamed Boussaa and Olivier Barais and Gerson Sunyé and Benoit Baudry},
   doi = {https://doi.org/10.1002/stvr.1721},
   issue = {1},
   journal = {Software Testing, Verification and Reliability},
   keywords = {code generators,metamorphic testing,non-functional properties,software quality,test automation,test oracle},
   note = {e1721 stvr.1721},
   pages = {e1721},
   title = {Leveraging metamorphic testing to automatically detect inconsistencies in code generator families},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1721},
   year = {2020}
}
@article{Patel2024,
   abstract = {Summary Cloud computing is an emerging paradigm that enables on-demand data storage without considering the local infrastructure limitations of end-users. The extensive growth in number of servers, resources, and networks intensifies security and privacy concerns in clouds. This necessitates secure mutual authentication and key agreement mechanism to verify the legitimacy of participating entities. In accordance, several authentication protocols have been designed to authenticate end-users over insecure channels. This article presents security analysis of recent protocols that claim to render security and privacy features. We demonstrate that Kaur et al.'s protocol is vulnerable to replay attack, Dharminder et al.'s protocol is prone to user traceability and known session-specific temporary information (KSSTI) attacks, and Bouchaala et al.'s protocol is prone to replay and KSSTI attacks. To withstand the aforementioned attacks, we propose an enhanced protocol based on fuzzy verifier and elliptic curve cryptography for clouds. Our protocol safeguards against security attacks while providing privacy functionalities. The security analysis is illustrated under real-or-random model, and correctness is verified under the Scyther security verification tool. Finally, comparative analysis with existing protocols shows that our protocol delivers robust security with reasonable computational and communication overheads than state-of-the-art protocols.},
   author = {Khushboo A Patel and Shivangi Shukla and Sankita J Patel},
   doi = {https://doi.org/10.1002/cpe.7889},
   issue = {2},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {anonymity,authentication,cloud computing,elliptic curve cryptography,smart card,untraceability},
   pages = {e7889},
   title = {A novel and provably secure mutual authentication protocol for cloud environment using elliptic curve cryptography and fuzzy verifier},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7889},
   year = {2024}
}
@article{Bai2019,
   abstract = {Accessibility has become an important focus in software development; the goal is to allow as many people as possible, regardless of their capabilities, to use software. We have investigated the methods that software teams prefer when testing the accessibility of their software. We conducted a large-scale study to evaluate six methods, using a sample of 53 people who work on various software teams. We present a detailed breakdown of the results for each testing method and analyze the differences between the methods. Our findings show that there are statistically significant differences in team members’ preferences, particularly for those with different roles. This implies that a software team should not choose a single method for all team members.},
   author = {Aleksander Bai and Viktoria Stray and Heidi Mork},
   doi = {https://doi.org/10.1155/2019/3271475},
   issue = {1},
   journal = {Advances in Human-Computer Interaction},
   pages = {3271475},
   title = {What Methods Software Teams Prefer When Testing Web Accessibility},
   volume = {2019},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2019/3271475},
   year = {2019}
}
@article{Ma2022,
   abstract = {The mining and analysis of student achievement data is of great importance to teaching management. Using the relevant data of college students’ physical fitness test and sports performance as the object of research, a BP neural network model is developed to predict performance. Based on BP neural network, an algorithm for predicting students’ endurance performance is proposed, which is applied to the sunshine long-distance intelligent sports testing system at Hangzhou Dianzi University. The nonlinear relationship between students’ performance in sunshine running and endurance performance is determined, and students’ performance in sunshine running is used to predict their endurance performance the following year. Experimental results indicate that the accuracy of the model is above 85\%. At the same time, the prediction results are combined with the Internet of things technology to produce a student sports prescription management system, which sets different sunshine running parameters for students with different predicted results and provides personalized sports prescriptions for students with different physical conditions, which has extensive and far-reaching application value.},
   author = {Zhanju Ma and Yao Wang},
   doi = {https://doi.org/10.1155/2022/1701687},
   issue = {1},
   journal = {Advances in Multimedia},
   pages = {1701687},
   title = {Analysis and Prediction of Body Test Results Based on Improved Backpropagation Neural Network Algorithm},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/1701687},
   year = {2022}
}
@article{Deng2020,
   abstract = {In recent years, increased attention is being given to software quality assurance and protection. With considerable verification and protection schemes proposed and deployed, today’s software unfortunately still fails to be protected from cyberattacks, especially in the presence of insecure organization of heap metadata. In this paper, we aim to explore whether heap metadata could be corrupted and exploited by cyberattackers, in an attempt to assess the exploitability of vulnerabilities and ensure software quality. To this end, we propose RELAY, a software testing framework to simulate human exploitation behavior for metadata corruption at the machine level. RELAY employs the heap layout serialization method to construct exploit patterns from human expertise and decomposes complex exploit-solving problems into a series of intermediate state-solving subproblems. With the heap layout procedural method, RELAY makes use of the fewer resources consumed to solve a layout problem according to the exploit pattern, activates the intermediate state, and generates the final exploit. Additionally, RELAY can be easily extended and can continuously assimilate human knowledge to enhance its ability for exploitability evaluation. Using 20 CTF&RHG programs, we then demonstrate that RELAY has the ability to evaluate the exploitability of metadata corruption vulnerabilities and works more efficiently compared with other state-of-the-art automated tools.},
   author = {Fenglei Deng and Jian Wang and Bin Zhang and Chao Feng and Zhiyuan Jiang and Yunfei Su},
   doi = {https://doi.org/10.1155/2020/8883746},
   issue = {1},
   journal = {Scientific Programming},
   pages = {8883746},
   title = {A Pattern-Based Software Testing Framework for Exploitability Evaluation of Metadata Corruption Vulnerabilities},
   volume = {2020},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2020/8883746},
   year = {2020}
}
@article{Zhou2022,
   abstract = {Objective. To systematically evaluate the correlation between the positive rate of sputum bacterial culture and the results of drug sensitivity test and the severity of the disease and its clinical significance, so as to provide evidence-based medicine for clinical application. Methods. PubMed, Embase, ScienceDirect, Cochrane Library, China Knowledge Network Database (CNKI), China VIP Database, Wanfang Database, and China Biomedical Literature Database (CBM) online database were used. The retrieval time limit was from the establishment of the database to the present. Data for all included studies were extracted by two independent researchers, and the risk of bias for the quality of each included study was assessed by the Cochrane Handbook 5.1.0 criteria. RevMan5.4 statistical software was used to analyze the collected data by meta. Results. In the end, 6 RCT articles were included. Overall, 613 samples were included in 6 RCT studies. The correlation between the positive rate of sputum bacterial culture in inpatients and the severity of the disease was meta-analyzed. The heterogeneity test results showed that Chi2 = 177.20, df = 3, P < 0.00001, and I2 = 98\%, indicating that there was obvious heterogeneity among the included research data. It was considered that there was a correlation between the positive rate of sputum bacterial culture and the severity of the disease. The correlation between the results of the drug sensitivity test of inpatients and the severity of the disease was evaluated. The results of the heterogeneity test showed that Chi2 = 0.00, df = 1, P = 1 > 0.05, and I2 = 0\%, indicating that there was no heterogeneity among the included research data. In addition, the combined effect of WMD was analyzed by the fixed effect model. The combined effect dose WMD test was Z = 6.58 (P < 0.00001). It was considered that there was a correlation between the results of the drug sensitivity test and the severity of the disease. Conclusion. There is a correlation between positive sputum culture and drug sensitivity test results and the severity of the disease in hospitalized patients. In clinical practice, for hospitalized patients, the positive sputum bacterial culture rate and drug sensitivity test results can be used to guide the appropriate use of antibiotics. Due to the low input from the literature, more studies with higher methodological quality and longer follow-up are needed for further validation.},
   author = {Wenjing Zhou and Jing Li},
   doi = {https://doi.org/10.1155/2022/5102100},
   issue = {1},
   journal = {Computational Intelligence and Neuroscience},
   pages = {5102100},
   title = {[Retracted] Correlation between Sputum Bacterial Culture Positive Rate and Drug Sensitivity Test Results and Disease Severity inInpatients and Its Clinical Significance: A SystematicReview and Meta-Analysis},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/5102100},
   year = {2022}
}
@article{Xu2022,
   abstract = {This study investigates the mechanism of digital linguistic landscapes in enabling engineering education for smart construction according to the educational dimensions of A (ability), S (skill), and K (knowledge). A questionnaire survey was conducted based on the core concepts of the informative dimension and symbolic dimension in digital language landscape as well as the ability dimension, knowledge dimension, and skill dimension in engineering education. Structural equation modeling (SEM) was used as the test method. The results of the research demonstrate that the informative dimension and symbolic dimension are two main aspects of DLL in education of engineering students for smart construction. Additionally, DLL has a significant positive impact on the ability, knowledge, and skill education of engineering students for smart construction. The research has theoretical and practical significance, as it not only enriches research on the relationship between DLL and engineering education for smart construction but also expands the theoretical understanding of engineering education from the perspective of linguistics. Furthermore, the study explores the path of the practical application of digital language landscape to engineering education for smart construction.},
   author = {Lin Xu and Jingxiao Zhang and Yin Yuan and Junwei Zheng and Simon P Philbin and Brian H W Guo and Ruoyu Jin},
   doi = {https://doi.org/10.1155/2022/4077516},
   issue = {1},
   journal = {Computational Intelligence and Neuroscience},
   pages = {4077516},
   title = {Testing the Effects of the Digital Linguistic Landscape on Engineering Education for Smart Construction},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/4077516},
   year = {2022}
}
@article{Vos2021,
   abstract = {Summary Covering all the possible paths of the graphical user interface (GUI) with test scripts would take too much effort and result in serious maintenance issues. We propose complementing scripted testing with scriptless test automation using the open-source testar tool. This paper gives a comprehensive overview of testar and its latest extensions together with the ongoing and future research. With this paper, we hope we can help and encourage other researchers to use testar for their GUI testing-related research and pave the way for an international research agenda in GUI testing built upon stable and open-source infrastructure.},
   author = {Tanja E J Vos and Pekka Aho and Fernando Pastor Ricos and Olivia Rodriguez-Valdes and Ad Mulders},
   doi = {https://doi.org/10.1002/stvr.1771},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {artificial intelligence,graphical user interface,model inference,monkey testing,test automation},
   note = {e1771 stvr.1771},
   pages = {e1771},
   title = {testar – scriptless testing through graphical user interface},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1771},
   year = {2021}
}
@inbook{Petra2022,
   abstract = {Summary This chapter explains the main image analysis approaches used to verify document integrity in the context of printable and scanable documents. Printing and scanning documents introduces degradations, which pose a major challenge for integrity verification methods. In cases of forgery, a document image may be modified in different ways using image or text editing software. Imitation fraud is the reproduction of the intrinsic properties of the document, including font characteristics such as type, size and color. Scanned documents are visually similar to originals, but are deformed by the print and scan process. Active approaches aim to protect a document by introducing security elements in the form of extrinsic fingerprints that are used for future verification. There are two main types of approach: watermarking and digital signatures. Printer identification can be carried out at different levels: in terms of printer type brand, or model.},
   author = {Gomez-Krämer Petra},
   doi = {https://doi.org/10.1002/9781119987390.ch3},
   isbn = {9781119987390},
   booktitle = {Multimedia Security 2},
   keywords = {digital signatures,document image,document integrity,extrinsic fingerprints,image analysis approaches,imitation fraud,print process,printer identification,scanning documents,watermarking},
   pages = {59-89},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Verifying Document Integrity},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119987390.ch3},
   year = {2022}
}
@article{Pietrantuono2020,
   abstract = {Summary Microservice architectures (MSA) is an emerging software architectural paradigm for service-oriented applications, well-suited for dynamic contexts requiring loosely coupled independent services, frequent software releases and decentralized governance. A key problem in the engineering of MSA applications is the estimate of their reliability, which is difficult to perform prior to release due frequent releases/service upgrades, dynamic service interactions, and changes in the way customers use the applications. This paper presents an in vivo testing method, named EMART, to faithfully assess the reliability of an MSA application in operation. EMART is based on an adaptive sampling strategy, leveraging monitoring data about microservices usage and failure/success of user demands. We present results of evaluation of estimation accuracy, confidence and efficiency, through a set of controlled experiments with publicly available subjects. © 2019 John Wiley \& Sons, Ltd.},
   author = {Roberto Pietrantuono and Stefano Russo and Antonio Guerriero},
   doi = {https://doi.org/10.1002/stvr.1725},
   issue = {2},
   journal = {Software Testing, Verification and Reliability},
   keywords = {in vivo testing,microservice architecture,software reliability},
   note = {e1725 stvr.1725},
   pages = {e1725},
   title = {Testing microservice architectures for operational reliability},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1725},
   year = {2020}
}
@article{Wang2019,
   abstract = {Summary Various online contents on Internet platforms or search engines are related to the corporate reputation. Facing the huge amount of online contents, we need a mining method that can automatically extract and analyze a large number of network-related information and obtain the real reliability of aspect for the content claimed by companies. In this paper, we propose to generate a ranking model to verify whether the sales-rankings claimed by companies are trustworthy. The key idea is that the company that has higher confidence score should be supported by the online media. We use a unique data set of public opinion data related with a specific company, which we supplement with data from various online news platform and retrieval webpages using a distributed and generic Web crawler. Meanwhile, basic information and open financial data of companies are also collected for auxiliary analysis. We present a Maximal Marginal Relevance-based ranking model to compute the confidence score of each company, taking into consideration the two technologies of word embedding and KL-Divergence to filter the irrelevant documents. Extensive experiments show that the proposed method outperforms the state-of-the-art MMR-based method, and we showcase three representative cases about the corporate reputation built by us that gives positive, neutral, and negative support respectively to the sales-ranking claim of companies.},
   author = {Youquan Wang and Changjian Fang and Dongqin Shen and Zhiang Wu and Jie Cao},
   doi = {https://doi.org/10.1002/cpe.5466},
   issue = {24},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {KL-Divergence,corporate reputation,maximal marginal relevance-based ranking,public opinion,word embedding},
   note = {e5466 cpe.5466},
   pages = {e5466},
   title = {Verifying the claimed sale-ranking trustworthy: A maximum marginal relevance-based ranking method},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5466},
   year = {2019}
}
@article{Popescu2022,
   abstract = {Abstract Melodic Intonation Therapy (MIT) is a prominent rehabilitation program for individuals with post-stroke aphasia. Our meta-analysis investigated the efficacy of MIT while considering quality of outcomes, experimental design, influence of spontaneous recovery, MIT protocol variant, and level of generalization. Extensive literature search identified 606 studies in major databases and trial registers; of those, 22 studies—overall 129 participants—met all eligibility criteria. Multi-level mixed- and random-effects models served to separately meta-analyze randomized controlled trial (RCT) and non-RCT data. RCT evidence on validated outcomes revealed a small-to-moderate standardized effect in noncommunicative language expression for MIT—with substantial uncertainty. Unvalidated outcomes attenuated MIT's effect size compared to validated tests. MIT's effect size was 5.7 times larger for non-RCT data compared to RCT data (g̅case report = 2.01 vs. g̅RCT = 0.35 for validated Non-Communicative Language Expression measures). Effect size for non-RCT data decreased with number of months post-stroke, suggesting confound through spontaneous recovery. Deviation from the original MIT protocol did not systematically alter benefit from treatment. Progress on validated tests arose mainly from gains in repetition tasks rather than other domains of verbal expression, such as everyday communication ability. Our results confirm the promising role of MIT in improving trained and untrained performance on unvalidated outcomes, alongside validated repetition tasks, and highlight possible limitations in promoting everyday communication ability.},
   author = {Tudor Popescu and Benjamin Stahl and Brenton M Wiernik and Felix Haiduk and Michaela Zemanek and Hannah Helm and Theresa Matzinger and Roland Beisteiner and W Tecumseh Fitch},
   doi = {https://doi.org/10.1111/nyas.14848},
   issue = {1},
   journal = {Annals of the New York Academy of Sciences},
   keywords = {Melodic Intonation Therapy,apraxia of speech,experimental design,formulaic language,meta-analysis,post-stroke aphasia,rhythmic pacing,singing},
   pages = {76-84},
   title = {Melodic Intonation Therapy for aphasia: A multi-level meta-analysis of randomized controlled trials and individual participant data},
   volume = {1516},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14848},
   year = {2022}
}
@article{Zolfaghari2021,
   abstract = {Abstract A flaky test is a test that may lead to different results in different runs on a single code under test without any change in the test code. Test flakiness is a noxious phenomenon that slows down software deployment, and increases the expenditures in a broad spectrum of platforms such as software-defined networks and Internet of Things environments. Industrial institutes and labs have conducted a whole lot of research projects aiming at tackling this problem. Although this issue has been receiving more attention from academia in recent years, the academic research community is still behind the industry in this area. A systematic review and trend analysis on the existing approaches for detecting and root causing flaky tests can pave the way for future research on this topic. This can help academia keep pace with industrial advancements and even lead the research in this field. This article first presents a comprehensive review of recent achievements of the industry as well as academia regarding the detection and mitigation of flaky tests. In the next step, recent trends in this line of research are analyzed and a roadmap is established for future research.},
   author = {Behrouz Zolfaghari and Reza M Parizi and Gautam Srivastava and Yoseph Hailemariam},
   doi = {https://doi.org/10.1002/spe.2929},
   issue = {5},
   journal = {Software: Practice and Experience},
   keywords = {detection,flaky testing,software,tools},
   pages = {851-867},
   title = {Root causing, detecting, and fixing flaky tests: State of the art and future roadmap},
   volume = {51},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2929},
   year = {2021}
}
@article{Yoosuf2021,
   abstract = {Summary IoT healthcare applications have become very popular among medical societies. At regular intervals, the healthcare IoT devices collect the patient's health information and transfer it to cloud storage. Storing the health records in the cloud environment increases the vulnerability. Integrity verification performed at regular intervals ensures the safety and security of the healthcare data stored in the cloud. Existing schemes use RSA and BLS signatures to verify the authenticity. However, the size of the signature is too large in RSA and BLS schemes. Using large signatures for proving the authenticity increases the computation overhead. IoT devices have minimal resource capacity, hence performing heavily weighted integrity schemes will create an unbearable workload. Likewise, private auditing increases the communication overhead between IoT devices and cloud storage. To overcome these issues, a fog-centric auditing scheme is proposed based on the Cramer–Shoup cryptosystem. The proposed scheme allows the fog nodes to perform integrity verification without compromising security. It reduces the communication overhead by 55\%–60\%. The size of the CS signature used in the scheme is far lesser than the existing signatures. Moreover, performing the inline deduplication on the fog layer improves the efficiency of cloud storage by 30\%.},
   author = {Mohamed Sirajudeen Yoosuf and Anitha R},
   doi = {https://doi.org/10.1002/cpe.6450},
   issue = {24},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {CS signature,IoT healthcare,data auditing,fog computing,integrity verification},
   pages = {e6450},
   title = {Lightweight fog-centric auditing scheme to verify integrity of IoT healthcare data in the cloud environment},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6450},
   year = {2021}
}
@article{Zhou2023,
   abstract = {Abstract As an important approach to overcome data silos and privacy concerns in deep learning, federated learning, which can jointly train the global model and keep data local, has shown remarkable performance in a range of industrial applications. However, federated learning still suffers from the problem that shared gradients may be subject to tampering, inference functions, and falsification. To address this issue, we propose a verifiable federated learning framework to deal with malicious aggregators. Initially, we propose a reputation calculation mechanism to solve the problem of selecting a reliable aggregator based on a multiweight subjective logic model. Furthermore, we design a verifiable federated learning scheme to ensure data confidentiality, integrity, and verifiability, as well as support the client's dynamic withdrawal. Security analyses indicate that our framework is secure against malicious adversaries. Furthermore, experimental results on real datasets show that our verifiable federated learning has high accuracy and feasible efficiency.},
   author = {Zhou Zhou and Youliang Tian and Changgen Peng and Nan Yang and Shigong Long},
   doi = {https://doi.org/10.1002/cpe.7193},
   issue = {20},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {federated learning,malicious aggregator,privacy protection,revocation,verifiability},
   pages = {e7193},
   title = {VFLF: A verifiable federated learning framework against malicious aggregators in Industrial Internet of Things},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7193},
   year = {2023}
}
@article{Chaieb2021,
   abstract = {Summary One of the most critical properties that must be ensured to have a secure electronic voting is verifiability. Political parties, observers, and especially voters want to be able to verify that all eligible votes are cast as intended and counted as cast without compromising votes secrecy or voters privacy. Over the past few decades, an important number of e-voting protocols attempt to deal with this issue by using cryptographic techniques and/or a public bulletin board. Recently, some blockchain-based e-voting systems have been proposed, but were not found practical in the real world, because they do not support situations with large numbers of candidates and voters. In this article, we design and implement a verifiable blockchain-based online voting protocol, called verify-your-vote . Our protocol ensures several security properties thanks to some cryptographic primitives and blockchain technology. We also evaluate its performance in terms of time, cost, and the number of voters and candidates that can be supported.},
   author = {Marwa Chaieb and Souheib Yousfi and Pascal Lafourcade and Riadh Robbana},
   doi = {https://doi.org/10.1002/cpe.5813},
   issue = {1},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {blockchain technology,cryptographic primitives,e-voting,practical implementation,security and performance evaluation},
   pages = {e5813},
   title = {Design and practical implementation of verify-your-vote protocol},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5813},
   year = {2021}
}
@article{Siqueira2021,
   abstract = {Summary Adaptive systems (ASs) and context-aware systems (CASs) are able to evaluate their own behaviour and to adapt it when the system fails to accomplish its goals or when better functionality or performance is possible. Ensuring the reliability of ASs and CASs is demanding because failures might have undesirable consequences. Testing ASs and CASs effectively is not trivial because of the inherent characteristics of these systems. The literature lacks a comprehensive review that provides a broad picture of the area; current reviews are outdated and incomplete. The objectives of this study are characterizing the state of the art in AS and CAS testing and discussing approaches, challenges, observed trends, and research limitations and directions. We performed a systematic literature review (SLR) and a thematic analysis of studies, reporting up-to-date, refined and extended results when compared with existing reviews. Based on 102 selected studies, we (i) characterized testing approaches by grouping techniques for ASs and CASs; (ii) updated and refined a characterization of testing challenges for ASs and CASs; and (iii) analysed and discussed research trends and implications for AS and CAS testing. There are recurring research concerns regarding AS and CAS testing. Examples are the generation of test cases and built-in tests. Moreover, we also identified recurring testing challenges such as context monitoring and runtime decisions. Moreover, there are some trends such as model-based testing and hybrid techniques and some little investigated issues like uncertainty and prediction of changes. All in all, our results may provide guidance for developers and researchers with respect to the practice and the future research on AS and CAS testing.},
   author = {Bento R Siqueira and Fabiano C Ferrari and Kathiani E Souza and Valter V Camargo and Rogério de Lemos},
   doi = {https://doi.org/10.1002/stvr.1772},
   issue = {7},
   journal = {Software Testing, Verification and Reliability},
   keywords = {adaptive systems,context-aware systems,software testing,systematic literature review,systematic mapping study},
   note = {e1772 stvr.1772},
   pages = {e1772},
   title = {Testing of adaptive and context-aware systems: approaches and challenges},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1772},
   year = {2021}
}
@article{Delgado-Prez2024,
   abstract = {Abstract Smart contracts (SC) are programs embodying certain business logic stored on a blockchain network like Ethereum. The execution of transactions on SC has a cost, measured in gas units, that depends on the low-level operations performed. Therefore, a poor choice of high-level language constructs could lead to overcharging users for their transactions. Thus, a testing process focused on possible deviations of the gas used in diverse scenarios could provide substantial global savings. This paper presents a gas-centered mutation testing approach for taking care of the gas consumed by Solidity SCs. This approach can be useful to improve the test quality to detect gas-related problems, reason about performance issues that only manifest in certain situations, and identify alternative more optimal implementations. We define and implement several mutation operators specifically designed to perturb gas consumption while preserving contract semantics in general. Our experiments using several real-world SCs show the feasibility of the technique, with some mutants reproducing meaningful differences in the consumption and exposing some gas limits not tight enough in historic transactions. Therefore, our approach is shown to be a good ally to prevent the appearance of gas-related issues and lays the groundwork for researchers seeking to improve performance testing practices.},
   author = {Pedro Delgado-Pérez and Ignacio Meléndez-Lapi and Juan Boubeta-Puig},
   doi = {https://doi.org/10.1002/smr.2672},
   issue = {9},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Ethereum,blockchain,gas consumption,mutation testing,smart contract},
   pages = {e2672},
   title = {Gas-centered mutation testing of Ethereum Smart Contracts},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2672},
   year = {2024}
}

@article{Garn2022,
   abstract = {Summary In this paper, we report on applying combinatorial testing to Internet of Things (IoT) home automation hub systems. We detail how to create a dedicated input parameter model of an IoT home automation hub system for use with combinatorial test case generation strategies. Further, we developed an automated test execution framework and two test oracles for evaluation purposes. We applied and evaluated our proposed methodological approach to a real-world IoT system and analysed the obtained results of various combinatorial test sets with different properties generated based on the derived input model. Additionally, we compare these results to a random testing approach. Our empirical testing evaluations revealed multiple errors in the tested devices and also showed that all considered approaches performed nearly equally well.},
   author = {Bernhard Garn and Dominik-Philip Schreiber and Dimitris E Simos and Rick Kuhn and Jeff Voas and Raghu Kacker},
   doi = {https://doi.org/10.1002/stvr.1805},
   issue = {2},
   journal = {Software Testing, Verification and Reliability},
   keywords = {IoT,combinatorial testing,software testing},
   pages = {e1805},
   title = {Combinatorial methods for testing Internet of Things smart home systems},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1805},
   year = {2022}
}
@article{Goyal2023,
   abstract = {Abstract Software maintenance is an important phase in the software development life cycle. Software projects maintain bug repositories to gather, organize, and keep track of bug reports. These bug reports are resolved by numerous software developers. Whenever the reported bug does not get resolved by the assigned developer, he marks the resolution of bug report as Non-Reproducible (NR). When NR bugs are reconsidered, few of them get resolved, and their resolution changes from NR to fix (NRF). The main aim of this paper is to predict these fixable NRF bug reports. A major challenge in predicting NRF bugs from NR bugs is that only a small portion of NR bugs get fixed, i.e., class-imbalance problem. For example, NRF bugs account for only 8.64\%, 4.73 \%, 4.56\%, and 1.06\% in NetBeans, Eclipse, Open Office, and Mozilla Firefox projects respectively. In this paper, we work on improving the classification performance on these imbalanced datasets. We propose IMNRFixer, a novel and hybrid NRF prediction tool. IMNRFixer uses three different techniques to combat class-imbalance problem: undersampling, oversampling, and ensemble models. We evaluate the performance of IMNRFixer models on four large and open-source projects of Bugzilla repository. Our results show that IMNRFixer outperforms conventional machine learning techniques. IMNRFixer achieves performance up to 71.7\%, 93.1\%, 91.7\%, and 96.5\% while predicting the minority class (NRF) for NetBeans, Eclipse, Open Office, and Mozilla Firefox projects, respectively.},
   author = {Anjali Goyal and Neetu Sardana},
   doi = {https://doi.org/10.1002/smr.2290},
   issue = {3},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Non-Reproducible bugs,bug fixing,bug report,class-imbalance,classification,ensemble techniques,machine learning,mining software repositories,prediction tool,sampling},
   note = {e2290 smr.2290},
   pages = {e2290},
   title = {IMNRFixer: A hybrid approach to alleviate class-imbalance problem for predicting the fixability of Non-Reproducible bugs},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2290},
   year = {2023}
}
@article{Ahmed2019,
   abstract = {Combinatorial interaction testing (CIT) is a useful testing technique to address the interaction of input parameters in software systems. CIT has been used as a systematic technique to sample the enormous test possibilities. Most of the research activities focused on the generation of CIT test suites as a computationally complex problem. Less effort has been paid for the application of CIT. To apply CIT, practitioners must identify the input parameters for the Software-under-test (SUT), feed these parameters to the CIT test generation tool, and then run those tests on the application with some pass and fail criteria for verification. Using this approach, CIT is used as a black-box testing technique without knowing the effect of the internal code. Although useful, practically, not all the parameters having the same impact on the SUT. This paper introduces a different approach to use the CIT as a gray-box testing technique by considering the internal code structure of the SUT to know the impact of each input parameter and thus use this impact in the test generation stage. The case studies results showed that this approach would help to detect new faults as compared to the equal impact parameter approach.},
   author = {Bestoun S Ahmed and Angelo Gargantini and Kamal Z Zamli and Cemal Yilmaz and Miroslav Bures and Marek Szeles},
   doi = {https://doi.org/10.1049/iet-sen.2018.5315},
   issue = {6},
   journal = {IET Software},
   keywords = {CIT test suites,CIT tool,SUT,black-box testing technique,code-aware combinatorial interaction testing,computationally complex problem,grey-box testing technique,impact parameter approach,internal code structure,program testing,program verification,software fault tolerance,software systems,systematic sampling technique,test generation stage,testing technique},
   pages = {600-609},
   title = {Code-aware combinatorial interaction testing},
   volume = {13},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5315},
   year = {2019}
}
@article{Tao2023,
   abstract = {Abstract Despite the tremendous development of artificial intelligence (AI)-based mobile apps, they suffer from quality issues. Data-driven AI software poses challenges for maintenance and quality assurance. Metamorphic testing has been successfully adopted to AI software. However, most previous studies require testers to manually identify metamorphic relations in an ad hoc and arbitrary manner, thereby encountering difficulties in reflecting real-world usage scenarios. Previous work showed that information available in user reviews is effective for maintenance and testing tasks. Yet, there is a lack of studies leveraging reviews to facilitate AI function maintenance and testing activities. This paper proposes METUR, a novel approach to supporting maintenance and testing for AI functions based on reviews. Firstly, METUR automatically classifies reviews that can be exploited for supporting AI function maintenance and evolution activities. Then, it identifies test contexts from reviews in the usage scenario category. METUR instantiates the metamorphic relation pattern for deriving concrete metamorphic relations based on test contexts. The follow-up test dataset is constructed for conducting metamorphic testing. Empirical studies on plant identification apps indicate that METUR effectively categorizes reviews that are related to AI functions. METUR is feasible and effective in detecting inconsistent behaviors by using the metamorphic relations constructed based on reviews.},
   author = {Chuanqi Tao and Hongjing Guo and Jingxuan Zhang and Zhiqiu Huang},
   doi = {https://doi.org/10.1002/smr.2444},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   keywords = {AI software testing,metamorphic relation,mobile app,user review mining},
   pages = {e2444},
   title = {Supporting maintenance and testing for AI functions of mobile apps based on user reviews: An empirical study on plant identification apps},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2444},
   year = {2023}
}
@article{Vercammen2024,
   abstract = {Abstract Mutation testing is the state-of-the-art technique for assessing the fault detection capacity of a test suite. Unfortunately, a full mutation analysis is often prohibitively expensive. The CppCheck project for instance, demands a build time of 5.8 min and a test execution time of 17 s on our desktop computer. An unoptimised mutation analysis, for 55,000 generated mutants took 11.8 days in total, of which 4.3 days is spent on (re)compiling the project. In this paper, we present a feasibility study, investigating how a number of optimisation strategies can be implemented based on the Clang front-end. These optimisation strategies allow to eliminate the compilation and execution overhead in order to support efficient mutation testing for the C language family. We provide a proof-of-concept tool that achieves a speedup of between 2 × and 30 ×. We make a detailed analysis of the speedup induced by the optimisations, elaborate on the lessons learned and point out avenues for further improvements.},
   author = {Sten Vercammen and Serge Demeyer and Markus Borg and Niklas Pettersson and Görel Hedin},
   doi = {https://doi.org/10.1002/stvr.1865},
   issue = {1},
   journal = {Software Testing, Verification and Reliability},
   keywords = {C++,CLANG,mutant schemata,mutation testing},
   pages = {e1865},
   title = {Mutation testing optimisations using the Clang front-end},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1865},
   year = {2024}
}
@article{Koikara2021,
   abstract = {Abstract Secret sharing schemes are being widely used to distribute a secret between various participants so that an authorized subset of participants belonging to appropriate access structures can reconstruct this secret. However, a dealer might get corrupted by adversaries and may influence this secret sharing or the reconstruction process. Verifiable secret sharing (VSS) overcomes this issue by adding a verifiability protocol to the original secret sharing scheme. This article proposes a computationally secure publicly verifiable secret sharing scheme based on the three-dimensional cellular automata (3D-CA). Unlike the more widely used linear secret sharing schemes or secret sharing scheme based on the Chinese remainder theorem, our proposed scheme performs the secret sharing using 3D-CA. The secret is considered one of the initial configurations of the 3D-CA, and the following configurations are devised to be the shares distributed among the participants. Update mechanisms and various rules make it hard for an adversary to corrupt or duplicate a share. To make it even more efficient, we have added a verifiability layer such that a dealer posts a public share and private share to each shareholder. The verifiability layer reduces the interaction between dealer and participants and hence increases the security. The randomness of the shares has been calculated using the National Institute of Standards and Technology statistical test suite.},
   author = {Rosemary Koikara and Eun-Jun Yoon and Anand Paul},
   doi = {https://doi.org/10.1002/cpe.6146},
   issue = {22},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {cellular automata,cryptography,threshold secret sharing,verifiable secret sharing},
   note = {e6146 CPE-20-1123.R2},
   pages = {e6146},
   title = {Publicly verifiable threshold secret sharing based on three-dimensional-cellular automata},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6146},
   year = {2021}
}
@article{Leotta2021,
   abstract = {Summary By ensuring adequate functional coverage, End-to-End (E2E) testing is a key enabling factor of continuous integration. This is even more true for web applications, where automated E2E testing is the only way to exercise the full stack used to create a modern application. The test code used for web testing usually relies on DOM locators, often expressed as XPath expressions, to identify the web elements and to extract the data checked in assertions. When applications evolve, the most dominant cost for the evolution of test code is due to broken locators, which fail to locate the target element in the novel versions and must be repaired. In this paper, we formulate the robust XPath locator generation problem as a graph exploration problem, instead of relying on ad-hoc heuristics as the one implemented by the state of the art tool robula+. Our approach is based on a statistical adaptive algorithm implemented by the tool sidereal, which outperforms robula+'s heuristics in terms of robustness by learning the potential fragility of HTML properties from previous versions of the application under test. sidereal was applied to six applications and to a total of 611 locators and was compared against two baseline algorithms, robula+ and Montoto. The adoption of sidereal results in a significant reduction of the number of broken locators (respectively -55\% and -70\%). The time for generating such robust locators was deemed acceptable being in the order of hundredths of second.},
   author = {Maurizio Leotta and Filippo Ricca and Paolo Tonella},
   doi = {https://doi.org/10.1002/stvr.1767},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {DOM-based testing,Web locators,Web testing,XPath,XPath locator,automated testing,robust locator,test automation},
   note = {e1767 stvr.1767},
   pages = {e1767},
   title = {Sidereal: Statistical adaptive generation of robust locators for web testing},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1767},
   year = {2021}
}
@article{Zhang2021,
   abstract = {Summary Outsourcing data to cloud servers is a popular service for data owners, however, how to check the integrity and freshness of the outsourced data is very challenge. Recently, Jin et al. proposed a cloud auditing protocol with full integrity and freshness support for cloud data, unfortunately in this article, we show their proposal is not secure. Concretely, the cloud servers can forge the authentication tag and thus has the ability to forge proof of data possession, which obviously invalidates their cloud auditing protocol. We also give a new cloud auditing protocol and analysis its security and performance. The results show our protocol is more efficient and secure.},
   author = {Jindan Zhang and Chuan Lin and Urszula Ogiela and Nadia Nedjah and Arun K Sangaiah and Xuan Wang},
   doi = {https://doi.org/10.1002/cpe.6049},
   issue = {23},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {attack,auditing protocol,cloud storage,tag generation},
   pages = {e6049},
   title = {Improved publicly verifiable auditing protocol for cloud storage},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6049},
   year = {2021}
}
@article{Silva2022,
   abstract = {Summary The use of mutation testing for mobile applications (apps for short) is still a challenge. Mobile apps are usually event-driven and encompass graphical user interfaces (GUIs) and a complex execution environment. Then, they require mutant operators to describe specific apps faults, and the automation of the mutation process phases like execution and analysis of the mutants is not an easy task. To encourage research addressing such challenges, this paper presents results from a mapping study on mutation testing for mobile apps. Following a systematic plan, we found 16 primary studies that were analysed according to three aspects: (i) trends and statistics about the field; (ii) study characteristics such as focus, proposed operators and automated support for the mutation testing phases; and (iii) evaluation aspects. The great majority of studies (98\%) have been published in the last 3 years. The most addressed language is Java, and Android is the only operating system considered. Mutant operators of GUI and configuration types are prevalent in a total of 138 operators found. Most studies implement a supporting tool, but few tools support mutant execution and analysis. The evaluation conducted by the studies includes apps mainly from the finance and utility domain. Nevertheless, there is a lack of benchmarks and more rigorous experiments. Future research should address other specific types of faults, languages, and operating systems. They should offer support for mutant execution and analysis, as well as to reduce the mutation testing cost and limitations in the mobile context.},
   author = {Henrique Neves Silva and Jackson Prado Lima and Silvia Regina Vergilio and Andre Takeshi Endo},
   doi = {https://doi.org/10.1002/stvr.1801},
   issue = {8},
   journal = {Software Testing, Verification and Reliability},
   keywords = {android,fault-based testing,mutation operators},
   pages = {e1801},
   title = {A mapping study on mutation testing for mobile applications},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1801},
   year = {2022}
}
@article{Minhas2023,
   abstract = {Abstract Regression testing is challenging because of its complexity and the amount of effort and time it requires, especially in large-scale environments with continuous integration and delivery. Regression test selection and prioritization techniques have been proposed in the literature to address the regression testing challenges, but adoption rates of these techniques in industry are not encouraging. One of the possible reasons could be the disparity in the regression testing goals in industry and literature. This work compares the research perspective to industry practice on regression testing goals, corresponding information needs, and metrics required to evaluate these goals. We have conducted a literature review of 44 research papers and a survey with 56 testing practitioners. The survey comprises 11 interviews and 45 responses to an online questionnaire. We identified that industry and research accentuate different regression testing goals. For instance, the literature emphasizes increasing the fault detection rates of test suites and early identification of critical faults. In contrast, the practitioners' focus is on test suite maintenance, controlled fault slippage, and awareness of changes. Similarly, the literature suggests maintaining information needs from test case execution histories to evaluate regression testing techniques based on various metrics, whereas, at large, the practitioners do not use the metrics suggested in the literature. To bridge the research and practice gap, based on the literature and survey findings, we have created a goal–question–metric (GQM) model that maps the regression testing goals, associated information needs, and metrics from both perspectives. The GQM model can guide researchers in proposing new techniques closer to industry contexts. Practitioners can benefit from information needs and metrics presented in the literature and can use GQM as a tool to follow their regression testing goals.},
   author = {Nasir Mehmood Minhas and Thejendar Reddy Koppula and Kai Petersen and Jürgen Börstler},
   doi = {https://doi.org/10.1002/smr.2506},
   issue = {2},
   journal = {Journal of Software: Evolution and Process},
   keywords = {GQM,goals,measures,metrics,objectives,regression testing},
   pages = {e2506},
   title = {Using goal–question–metric to compare research and practice perspectives on regression testing},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2506},
   year = {2023}
}
@article{Aghamohammadi2021,
   abstract = {Summary Predictive mutation testing (PMT) is a technique to predict whether a mutant is killed, using machine learning approaches. Researchers have proposed various methods for PMT over the years. However, the impact of unreached mutants on PMT is not fully addressed. A mutant is unreached if the statement on which the mutant is generated is not executed by any test cases. We aim at showing that unreached mutants can inflate PMT results. Moreover, we propose an alternative approach to PMT, suggesting a different interpretation for PMT. To this end, we replicated the previous PMT research. We empirically evaluated the suggested approach on 654 Java projects provided by prior literature. Our results indicate that the performance of PMT drastically decreases in terms of area under a receiver operating characteristic curve (AUC) from 0.833 to 0.517. Furthermore, PMT performs worse than random guesses on 27\% of the projects. The proposed approach improves the PMT results, achieving the average AUC value of 0.613. As a result, we recommend researchers to remove unreached mutants when reporting the results.},
   author = {Alireza Aghamohammadi and Seyed-Hassan Mirian-Hosseinabadi},
   doi = {https://doi.org/10.1002/stvr.1784},
   issue = {7},
   journal = {Software Testing, Verification and Reliability},
   keywords = {machine learning,mutation testing,predictive mutation testing,software testing},
   note = {e1784 stvr.1784},
   pages = {e1784},
   title = {An ensemble-based predictive mutation testing approach that considers impact of unreached mutants},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1784},
   year = {2021}
}
@article{Quattrocchi2022,
   abstract = {Abstract This work consolidates and compounds previous investigations in recognizing defects for infrastructure-as-code (IaC) scripts using general software development quality metrics with a focus on defect severity but adding to previous work an explorative look at creating datasets, which may boost the predictive power of provided models—we call this notion a fluid dataset. More specifically, we experiment with 50 different metrics harnessing a multiple dataset creation process whereby different versions of the same datasets are rigged with auto-training facilities for model retraining and redeployment in a DataOps fashion. At this point, with a focus on the Ansible infrastructure code language—as a de facto standard for industrial-strength infrastructure code—we build defect prediction models and manage to improve on the state of the art by finding an F1 score of 0.52 and a recall of 0.57 using a Naive–Bayes classifier. On the one hand, by improving state-of-the-art defect prediction models using metrics generalizable for different IaC languages, we provide interesting leads for the future of infrastructure-as-code. On the other hand, we have barely scratched the surface on the novel approach of fluid-datasets creation and automated retraining of Machine Learning (ML) defect prediction models, warranting for more research on the same direction in the future.},
   author = {Giovanni Quattrocchi and Damian Andrew Tamburri},
   doi = {https://doi.org/10.1002/smr.2480},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,defect prediction,fluid datasets,infrastructure code},
   pages = {e2480},
   title = {Predictive maintenance of infrastructure code using “fluid” datasets: An exploratory study on Ansible defect proneness},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2480},
   year = {2022}
}
@article{Polo-Usaola2021,
   abstract = {Summary When applied to mobile software, mutation testing is particularly costly due to the deployment of the app under test onto the device: if one deployment is made for each generated mutant, the execution time becomes unapproachable. This paper analyses how the combination of different cost reduction techniques improves the execution time of mutation testing in mobile apps. The techniques reviewed and combined are mutant schema, parallel execution and two different ways of executing tests against the mutants (all against all and all against mutants remaining alive), as well as greedy algorithm for reducing the test suite size. This paper also presents a mathematical model of cost reduction and checks its validity with several experiments. Furthermore, the exhaustive and long experimentation has led the authors to compile a set of good practices which are also presented in a set of lessons learned.},
   author = {Macario Polo-Usaola and Isyed Rodríguez-Trujillo},
   doi = {https://doi.org/10.1002/stvr.1769},
   issue = {7},
   journal = {Software Testing, Verification and Reliability},
   keywords = {cost prediction,mobile testing,mutant schema,mutation testing,parallel execution},
   note = {e1769 STVR-19-0011.R3},
   pages = {e1769},
   title = {Analysing the combination of cost reduction techniques in Android mutation testing},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1769},
   year = {2021}
}
@article{Saadatmand2020,
   author = {Mehrdad Saadatmand and Birgitta Lindström and Bernhard K Aichernig},
   doi = {https://doi.org/10.1002/stvr.1726},
   issue = {1},
   journal = {Software Testing, Verification and Reliability},
   pages = {e1726},
   title = {Special issue on testing extra-functional properties},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1726},
   year = {2020}
}
@article{Arts2023,
   abstract = {Abstract Blockchain implementations have become more and more advanced, combining many different features in the same framework (e.g., oracles, names, and state channels). Since the cost of errors in reputation and represented value is high in the blockchain world, software quality is of the utmost importance. One of the main methods used to assure such high software quality is careful testing. However, the number of tests needed to achieve a high level of assurance grows quadratic with the pairs of features of the blockchain, and when testing triples features the growth is cubic. To manually craft the required large number of tests is an almost impossible undertaking in practice. In this article, we describe how property-based testing (PBT) techniques have been used to automate testing of the core part of the Aeternity blockchain, ensuring the high software quality of the blockchain. Even though PBT is a powerful testing technique, applying it to the task of testing a complex system such as a blockchain, is far from trivial. The structure of the Aeternity property-based test model follows the structure of the blockchain, that is, it cleanly separates different blockchain features (e.g., oracles, smart contracts) into different model parts, and moreover, reduces the amount of boilerplate test model code by focusing on the identification of valid blockchain transactions. The test model is evaluated through a careful instrumentation of test code which permits observations of which combinations of features have been tested during a test run, and with which frequency. This article documents the details of how these issues were addressed in the development of the Aeternity test model, providing insights into both the testing of other blockchains as well as the testing of other complex feature based systems.},
   author = {Thomas Arts and Hans Svensson and Clara Benac Earle and Lars-Åke Fredlund},
   doi = {https://doi.org/10.1002/spe.3183},
   issue = {5},
   journal = {Software: Practice and Experience},
   keywords = {blockchain,property-based testing},
   pages = {1144-1173},
   title = {Testing feature-rich blockchains},
   volume = {53},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3183},
   year = {2023}
}
@article{Chaim2023,
   abstract = {Summary Data flow testing creates test requirements as definition-use (DU) associations, where a definition is a program location that assigns a value to a variable and a use is a location where that value is accessed. Data flow testing is expensive, largely because of the number of test requirements. Luckily, many DU-associations are redundant in the sense that if one test requirement (e.g. node, edge and DU-association) is covered, other DU-associations are guaranteed to also be covered. This relationship is called subsumption. Thus, testers can save resources by only covering DU-associations that are not subsumed by other testing requirements. Although this has the potential to significantly decrease the cost of data flow testing, there are roadblocks to its application. Finding data flow subsumptions correctly and efficiently has been an elusive goal; the savings provided by data flow subsumptions and the cost to find them need to be assessed; and the fault detection ability of a reduced set of DU-associations and the advantages of data flow testing over node and edge coverage need to be verified. This paper presents novel solutions to these problems. We present algorithms that correctly find data flow subsumptions and are asymptotically less costly than previous algorithms. We present empirical data that show that data flow subsumption is effective at reducing the number of DU-associations to be tested and can be found at scale. Furthermore, we found that using reduced DU-associations decreased the fault detection ability by less than 2\%, and data flow testing adds testing value beyond node and edge coverage.},
   author = {Marcos Lordello Chaim and Kesina Baral and Jeff Offutt and Mario Concilio Neto and Roberto Paulo Andrioli de Araujo},
   doi = {https://doi.org/10.1002/stvr.1843},
   issue = {6},
   journal = {Software Testing, Verification and Reliability},
   keywords = {algorithms,data flow analysis frameworks,data flow testing,empirical analysis,software testing,structural testing,subsumption relationship},
   pages = {e1843},
   title = {On subsumption relationships in data flow testing},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1843},
   year = {2023}
}
@article{Klein2024,
   abstract = {Abstract Voice user interface (VUI) systems, such as Alexa, Siri, and Google Assistant, are popular and widely available. Still, challenges such as privacy and the ability to have a dialog remain. In the latter example, the user expects a human-like conversation, that is, that the VUI understands the dialog and its context. However, this VUI feature of context-aware interaction is rather error prone. For this reason, we intend to explore the VUI context of use and its impact on interaction, that is, relevant user experience (UX). We see a demand for context-dependent UX measurement because analyzing the context of use and UX assessment are both critical human-centered design (HCD) methods. Therefore, we examine the VUI context of use by asking users about how, where, and for what they use VUIs, as well as their UX and improvement proposals. We interviewed people with disabilities who rely on VUIs and people without disabilities who use VUIs for convenience or fun. We identified VUI context-of-use categories and factors and explored their impacts on relevant UX qualities. Our result is a matrix containing these elements; thus, it provides an overview of the contextual UX of our target group's VUI interaction. We intend to develop a VUI context-of-use conceptual structure in the future based on this matrix, which is needed to create an automated context-dependent UX measurement recommendation tool for VUIs. This conceptual structure could also be useful for automated UX testing in the context of VUI.},
   author = {Andreas M Klein and Jana Deutschländer and Kristina Kölln and Maria Rauschenberger and Maria José Escalona},
   doi = {https://doi.org/10.1002/smr.2618},
   issue = {7},
   journal = {Journal of Software: Evolution and Process},
   keywords = {UX,VUI,context of use,user experience testing,voice user interface},
   pages = {e2618},
   title = {Exploring the context of use for voice user interfaces: Toward context-dependent user experience quality testing},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2618},
   year = {2024}
}
@article{MelesseVergara2024,
   abstract = {Summary In this article, we summarize the deployment of the Air Force Weather (AFW) HPC11 system at Oak Ridge National Laboratory (ORNL) including the process followed to successfully complete acceptance testing of the system. HPC11 is the first HPE/Cray EX 3000 system that has been successfully released to its user community in a federal facility. HPC11 consists of two identical 800-node supercomputers, Fawbush and Miller, with access to two independent and identical lustre parallel file systems. HPC11 is equipped with Slingshot 10 interconnect technology and relies on the HPE Performance Cluster Manager software for system configuration. ORNL has a clearly defined acceptance testing process used to ensure that every new system deployed can provide the necessary capabilities to support user workloads. We worked closely with HPE and AFW to develop a set of tests that used the United Kingdom's Meteorological Office's Unified Model and 4-dimensional variational data assimilation. We also included benchmarks and applications from the Oak Ridge Leadership Computing Facility portfolio to fully exercise the HPE/Cray programming environment and evaluate the functionality and performance of the system. Acceptance testing of HPC11 required parallel execution of each element on Fawbush and Miller. In addition, careful coordination was needed to ensure successful acceptance of the newly deployed lustre file systems alongside the compute resources. In this work, we present test results from specific system components and provide an overview of the issues identified, challenges encountered, and the lessons learned along the way.},
   author = {Verónica G Melesse Vergara and Reuben Budiardja and Paul Peltz and Jeffery Niles and Christopher Zimmer and Daniel Dietz and Christopher Fuson and Hong Liu and Paul Newman and James Simmons and Christopher Muzyn},
   doi = {https://doi.org/10.1002/cpe.7914},
   issue = {3},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {acceptance testing,system test},
   pages = {e7914},
   title = {A step towards the final frontier: Lessons learned from acceptance testing of the first HPE/Cray EX 3000 system at ORNL},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7914},
   year = {2024}
}
@article{Yang2024-5,
   abstract = {Abstract Recent advances in artificial intelligence technology and perception components have promoted the rapid development of autonomous vehicles. However, as safety-critical software, autonomous driving systems often make wrong judgments, seriously threatening human and property safety. LiDAR is one of the most critical sensors in autonomous vehicles, capable of accurately perceiving the three-dimensional information of the environment. Nevertheless, the high cost of manually collecting and labeling point cloud data leads to a dearth of testing methods for LiDAR-based perception modules. To bridge the critical gap, we introduce MetaLiDAR, a novel automated metamorphic testing methodology for LiDAR-based autonomous driving systems. First, we propose three object-level metamorphic relations for the domain characteristics of autonomous driving systems. Next, we design three transformation modules so that MetaLiDAR can generate natural-looking follow-up point clouds. Finally, we define corresponding evaluation metrics based on metamorphic relations. MetaLiDAR automatically determines whether source and follow-up test cases meet the metamorphic relations based on the evaluation metrics. Our empirical research on five state-of-the-art LiDAR-based object detection models shows that MetaLiDAR can not only generate natural-looking test point clouds to detect 181,547 inconsistent behaviors of different models but also significantly enhance the robustness of models by retraining with synthetic point clouds.},
   author = {Zhen Yang and Song Huang and Changyou Zheng and Xingya Wang and Yang Wang and Chunyan Xia},
   doi = {https://doi.org/10.1002/smr.2644},
   issue = {7},
   journal = {Journal of Software: Evolution and Process},
   keywords = {autonomous driving systems,metamorphic testing,object detection systems,test data generation},
   pages = {e2644},
   title = {MetaLiDAR: Automated metamorphic testing of LiDAR-based autonomous driving systems},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2644},
   year = {2024}
}
@article{Zhang2023,
   abstract = {Abstract As agile software development and extreme programing have become increasingly popular, continuous integration (CI) has become a widely used collaborative work method. However, it is common to make changes frequently to a project during CI. If existing testing methods are applied to CI directly, it will be difficult to make testing resources focus on changes generated by CI, which results in insufficient testing for changes. To solve this problem, we propose a fuzz testing method for CI. First, differential analysis is performed to determine the change points generated during CI, change points are added to the taint source set, and static analysis is conducted to calculate the distances between each basic block and the taint sources. Then, the project under test is instrumented according to the distances. During fuzz testing, testing resources are allocated based on seed coverage to test the change points effectively. Using the proposed methods, we implement CIDFuzz as a prototype tool, and experiments are conducted on four open-source projects that use CI. Experimental results show that, compared with AFL and AFLGo, CIDFuzz can reduce the time costs of covering change points up to 39.59\% and 41.64\%, respectively. Also, CIDFuzz can reduce the time costs of reproducing vulnerabilities up to 34.78\% and 25.55\%.},
   author = {Jiaming Zhang and Zhanqi Cui and Xiang Chen and Huiwen Yang and Liwei Zheng and Jianbin Liu},
   doi = {https://doi.org/10.1049/sfw2.12125},
   issue = {3},
   journal = {IET Software},
   keywords = {program testing,quality assurance,software quality},
   pages = {301-315},
   title = {CIDFuzz: Fuzz testing for continuous integration},
   volume = {17},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12125},
   year = {2023}
}
@article{Diaz2021,
   abstract = {Abstract There is a growing demand for correct parallel programs, mainly due to nowadays availability of parallel architectures. Structural testing allows identifying defects by analyzing the internal structures of a program. However, communication and synchronization in parallel programs bring new challenges to the testing activity, such as nondeterminism. Message-passing parallel programs require structural testing criteria to support test models and tools capable of covering synchronization events with dynamic behaviors. Testing such primitives inside loops in message-passing programs is still challenging with nontrivial solutions for criteria, models, and tools. This article proposes new structural testing criteria to guide the selection of test cases, improving the quality of message-passing programs by revealing nondeterminism-related defects present in loops. We present a new test model to support our criteria for structural testing of MPI-applications and implement the proposed criteria in the tool ValiMPI. The analysis of nondeterminism-related defects, paths inside loops, loop iterations, and nested loops allow us to establish a structured testing model. We validate the testing criteria through experimental studies using ValiMPI. Our results show that our criteria reveal unknown defects from communication and synchronization events in different loop iterations, increasing the quality of message-passing parallel programs.},
   author = {Silvia M D Diaz and Paulo S L Souza and Simone R S Souza},
   doi = {https://doi.org/10.1002/cpe.6082},
   issue = {18},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {MPI,message-passing,parallel programs,structural testing criteria},
   pages = {e6082},
   title = {Structural testing for communication events into loops of message-passing parallel programs},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6082},
   year = {2021}
}
@article{Chen2024-2,
   abstract = {Abstract The Java virtual machine (JVM) is the cornerstone of the Java platforms. A JVM's exception handling implementation interrupts, when the objective application encounters an exception (or an error), the normal execution of the application and performs specific handling tasks. However, little research has been done in systematically validating JVMs' exception handling implementations—test programs or even applications need to be carefully designed for throwing/catching exceptions at runtime; a JVM's exception handling implementation is also complicated, making it challenging to design tests for testing all of its functionalities. Inspired by the recent success of fuzz testing of compilers and JVM implementations, we introduce EHCBuilder, the first technique for fuzzing JVMs' exception handling implementations. The key idea is to construct exception handling chains, each of which abstracts a program's execution into a sequence of exception throwings, catchings, and/or handlings. A classfile seed can then be mutated into test programs with diverse exception handling chains, enabling (1) exceptions to be continuously thrown and caught at runtime, and (2) JVMs' exception handling implementations to be much more thoroughly tested. We have implemented EHCBuilder and evaluated EHCBuilder on popular JVM implementations including OpenJDK's HotSpot, Eclipse's OpenJ9, Azul's Zulu, and Oracle's GraalVM. Our results show that EHCBuilder can generate programs with very intricate exception handling chains and reveal differences among JVMs' exception handling implementations: Up to thousands of lines of source code in HotSpot's exception handling implementation are covered more than the original benchmarks; during 39 K iterations, EHCBuilder generates exception handling chains of different lengths, revealing 258 runtime differences. We classify the differences into four categories, and reveal a fast throw issue confirmed by HotSpot developers and another initCause issue confirmed by the OpenJ9 community.},
   author = {Bochuan Chen and Xiao Guo and Yuting Chen and Xiaofeng Yu and Lei Bu},
   doi = {https://doi.org/10.1002/smr.2562},
   issue = {4},
   journal = {Journal of Software: Evolution and Process},
   keywords = {Java virtual machine,chain,exception handling,mutation},
   pages = {e2562},
   title = {Constructing exception handling chains for testing Java virtual machine implementations},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2562},
   year = {2024}
}
@article{Villamae2024,
   abstract = {Abstract Usability evaluation is a key element to ensure a positive user experience with any software and it is especially important in educational software tools where there are many different actors involved (lecturers, students, administrators, etc.). However, evaluating usability is not an easy task for nonexpert evaluators. To facilitate this evaluation task, this article presents a Methodology for Usability Testing (MUT) and a system (CALMUT) that assists nonexpert evaluators in the application of the methodology by automatizing the calculations and facilitating their interpretation. This can be very useful for learning and instructional designers but also to people involved in the decision of introducing or not a new educational software. To develop the proposal, a literature review of different usability metrics, methods, and systems was carried out first, followed by a selection and adaptation for novice usability evaluators. This article also presents a case study where lecturers tested the usability of an educational software following the proposal and shows that using MUT and CALMUT helps people without previous experience detect the main usability problems of educational systems before deciding whether to use them or not.},
   author = {Mikel Villamañe and Ainhoa Alvarez},
   doi = {https://doi.org/10.1002/cae.22725},
   issue = {3},
   journal = {Computer Applications in Engineering Education},
   keywords = {educational technologies,usability testing},
   pages = {e22725},
   title = {Facilitating and automating usability testing of educational technologies},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.22725},
   year = {2024}
}
@article{Nguyen2021,
   abstract = {Summary Web user interface (UI) test automation strategies have been dominated by programmable and record–playback approaches. Of these, record–playback allows creating automation tests easily and reduces the cost of test generation. However, this approach increases the cost of test maintenance due to its unstable generated locators for identifying UI objects during playback. In this paper, we propose a new approach to generating and selecting resilient and maintainable locators. Our approach consists of two parts, a new XPath construction method and selecting the best XPath to locate the target element. Our XPath construction method relies on semantic structures of Web pages to locate the target element using its neighbors. We conducted an experiment on 15 popular websites. The results show that our approach outperforms the state-of-the-practice/art Selenium IDE and Robula+ in locating target elements by effectively avoiding wrong locators. It also produces more readable XPaths (hence more maintainable tests) than do these approaches.},
   author = {Vu Nguyen and Thanh To and Gia-Han Diep},
   doi = {https://doi.org/10.1002/stvr.1760},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {XPath,automated testing,web UI test automation,web locators},
   note = {e1760 stvr.1760},
   pages = {e1760},
   title = {Generating and selecting resilient and maintainable locators for Web automated testing},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1760},
   year = {2021}
}
@article{Lee2022,
   abstract = {Summary Testing vessel surveillance monitoring queries requires synthesized trajectory generation data of free-moving objects. This article presents a method for generating trajectories of free moving objects with approach, u-turn, detour, and hover patterns to verify a set of continuous queries expressed in spatiotemporal conditions. We present an algorithm that generates simplified paths, moving patterns, and inertia-based trajectories to make various types of trajectory patterns for a given spatiotemporal query. The experimental results show that the generated synthesized trajectory dataset can validate given spatiotemporal queries with different velocities, trajectory patterns, and travel paths.},
   author = {Sanghyun Lee and Bonghee Hong and Woochan Kim},
   doi = {https://doi.org/10.1002/cpe.7147},
   issue = {20},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {continuous queries,moving object,spatial query,trajectory},
   pages = {e7147},
   title = {A generation of synthesized trajectories of free moving objects for testing a set of continuous spatio-temporal queries},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7147},
   year = {2022}
}
@article{Trace2023,
   abstract = {Abstract Knowledge of ovarian cancer patients' information practices around cancer genetic testing (GT) is needed to inform interventions that promote patient access to GT-related information. We interviewed 21 ovarian cancer patients and survivors who had GT as part of the treatment process and analyzed the transcripts using the qualitative content analysis method. We found that patients' information practices, manifested in their information-seeking mode, information sources utilized, information assessment, and information use, showed three distinct styles: passive, semi-active, and active. Patients with the passive style primarily received information from clinical sources, encountered information, or delegated information-seeking to family members; they were not inclined to assess information themselves and seldom used it to learn or influence others. Women with semi-active and active styles adopted more active information-seeking modes to approach information, utilized information sources beyond clinical settings, attempted to assess the information found, and actively used it to learn, educate others, or advocate GT to family and friends. Guided by the social ecological model, we found multiple levels of influences, including personal, interpersonal, organizational, community, and societal, acting as motivators or barriers to patients' information practice. Based on these findings, we discussed strategies to promote patient access to GT-related information.},
   author = {Ciaran B Trace and Yan Zhang and Siqi Yi and Marian Yvette Williams-Brown},
   doi = {https://doi.org/10.1002/asi.24823},
   issue = {11},
   journal = {Journal of the Association for Information Science and Technology},
   pages = {1265-1281},
   title = {Information practices around genetic testing for ovarian cancer patients},
   volume = {74},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24823},
   year = {2023}
}
@article{DiMartino2021,
   abstract = {Summary Exploratory testing and fully automated testing tools represent two viable and cheap alternatives to traditional test-case-based approaches for graphical user interface (GUI) testing of Android apps. The former can be executed by capture and replay tools that directly translate execution scenarios registered by testers in test cases, without requiring preliminary test-case design and advanced programming/testing skills. The latter tools are able to test Android GUIs without tester intervention. Even if these two strategies are widely employed, to the best of our knowledge, no empirical investigation has been performed to compare their performance and obtain useful insights for a project manager to establish an effective testing strategy. In this paper, we present two experiments we carried out to compare the effectiveness of exploratory testing approaches using a capture and replay tool (Robotium Recorder) against three freely available automatic testing tools (AndroidRipper, Sapienz, and Google Robo). The first experiment involved 20 computer engineering students who were asked to record testing executions, under strict temporal limits and no access to the source code. Results were slightly better than those of fully automated tools, but not in a conclusive way. In the second experiment, the same students were asked to improve the achieved testing coverage by exploiting the source code and the coverage obtained in the previous tests, without strict temporal constraints. The results of this second experiment showed that students outperformed the automated tools especially for long/complex execution scenarios. The obtained findings provide useful indications for deciding testing strategies that combine manual exploratory testing and automated testing.},
   author = {Sergio Di Martino and Anna Rita Fasolino and Luigi Libero Lucio Starace and Porfirio Tramontana},
   doi = {https://doi.org/10.1002/stvr.1754},
   issue = {3},
   journal = {Software Testing, Verification and Reliability},
   keywords = {Android app testing,GUI testing,automatic input generation,capture and replay,testing effectiveness},
   note = {e1754 stvr.1754},
   pages = {e1754},
   title = {Comparing the effectiveness of capture and replay against automatic input generation for Android graphical user interface testing},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1754},
   year = {2021}
}
@article{Garn2022,
   abstract = {Summary This work presents an extended and enhanced gray-box combinatorial security testing methodology for SQL injection vulnerabilities in web applications. We propose multiple new attack grammars modelling SQLi attacks against MySQL-compatible databases, each one targeting a different injection context. Additionally, these grammars are also dynamically refined at the beginning of each attack against an endpoint of a web application, as a further optimization of the used attack model by taking into account the specifics of the generated query of that endpoint. Our goal is to enhance existing combinatorial approaches for detecting SQL injection vulnerabilities. The newly developed methodology is implemented in a prototype security testing tool called SQLInjector+, which is an extension of an earlier prototype developed by us in prior work. This improved tool can attack (i.e. test) any web application that uses a MySQL-compatible database management system. We evaluate our revised approach and improved prototype tool in a case study comprising of different kinds of web applications to which SQLi is a potential security threat. The case study contains the well-known verification framework WAVSEP among other five real-world web applications and one web application firewall. Our generated attack vectors, constructed via combinatorial methods applied to our improved and dynamically optimized attack grammars, are capable of injecting every known vulnerable endpoint in WAVSEP and also of finding new vulnerable parameters in some of the real-world applications investigated in this paper. Our approach performs equally well or better when compared with existing state-of-art of SQL injection security testing tools (sqlmap, w3af, wapiti and fuzzdb) across all tested web applications in the case study.},
   author = {Bernhard Garn and Jovan Zivanovic and Manuel Leithner and Dimitris E Simos},
   doi = {https://doi.org/10.1002/stvr.1826},
   issue = {6},
   journal = {Software Testing, Verification and Reliability},
   keywords = {SQL injection,combinatorial testing,gray-box testing,security testing,web applications},
   pages = {e1826},
   title = {Combinatorial methods for dynamic gray-box SQL injection testing},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1826},
   year = {2022}
}
@article{Mao2023,
   abstract = {Abstract In recent years, deep learning (DL) systems are increasingly used in the safety-critical fields such as autonomous driving, medical diagnosis, and financial service. Although these systems have demonstrated an outstanding performance in enhancing the accuracy of decision-making, they pose significant challenges to the trustworthiness due to their limited interpretability and inherent uncertainty. Adaptive random testing (ART) has been proved as an effective approach for ensuring the reliability of DL systems. However, existing ART methods for DL systems incur a heavy overhead in test case selection due to the computation of distances. To address this issue, we propose a lightweight adaptive random testing (Lw-ARTDL) method for DL systems. In our improved algorithm, we employ the K-Means technique to divide the entire test suite into several subsets. Then, for a candidate test case, we only calculate distances between it and the test cases within the category to which it belongs. This partition strategy ensures that the selected test cases are more representative while significantly reducing the computational cost. To validate the proposed algorithm, the comparison experiments between Lw-ARTDL and the original ARTDL algorithm are conducted on two typical DL systems. The experimental results show that Lw-ARTDL significantly reduces the overhead of failure detection, and exhibits stronger failure detection capability compared to ARTDL in most similarity metrics.},
   author = {Chengying Mao and Yue Song and Jifu Chen},
   doi = {https://doi.org/10.1002/spe.3256},
   issue = {11},
   journal = {Software: Practice and Experience},
   keywords = {adaptive random testing,cluster analysis,deep learning systems,efficiency,failure detection},
   pages = {2271-2295},
   title = {A lightweight adaptive random testing method for deep learning systems},
   volume = {53},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3256},
   year = {2023}
}
@article{Windsor2022,
   abstract = {Summary We present a technique and automated toolbox for randomized testing of C compilers. Unlike prior compiler-testing approaches, we generate concurrent test cases in which threads communicate using fine-grained atomic operations, and we study actual compiler implementations rather than abstract mappings. Our approach is (1) to generate test cases with precise oracles directly from an axiomatization of the C concurrency model; (2) to apply metamorphic fuzzing to each test case, aiming to amplify the coverage they are likely to achieve on compiler codebases; and (3) to execute each fuzzed test case extensively on a range of real machines. Our tool, C4, benefits compiler developers in two ways. First, test cases generated by C4 can achieve line coverage of parts of the LLVM C compiler that are reached by neither the LLVM test suite nor an existing (sequential) C fuzzer. This information can be used to guide further development of the LLVM test suite and can also shed light on where and how concurrency-related compiler optimizations are implemented. Second, C4 can be used to gain confidence that a compiler implements concurrency correctly. As evidence of this, we show that C4 achieves high strong mutation coverage with respect to a set of concurrency-related mutants derived from a recent version of LLVM and that it can find historic concurrency-related bugs in GCC. As a by-product of concurrency-focused testing, C4 also revealed two previously unknown sequential compiler bugs in recent versions of GCC and the IBM XL compiler.},
   author = {Matt Windsor and Alastair F Donaldson and John Wickerson},
   doi = {https://doi.org/10.1002/stvr.1812},
   issue = {4},
   journal = {Software Testing, Verification and Reliability},
   keywords = {C11,atomics,concurrency,fuzz testing,program generation,weak memory},
   pages = {e1812},
   title = {High-coverage metamorphic testing of concurrency support in C compilers},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1812},
   year = {2022}
}
@article{ORiordan2021,
   abstract = {Abstract Learners’ progress within computer-supported collaborative learning environments is typically measured via analysis and interpretation of quantitative web interaction measures. However, the usefulness of these “proxies for learning” is questioned as they do not necessarily reflect critical thinking—an essential component of collaborative learning. Research indicates that pedagogical content analysis schemes have value in measuring critical discourse in small scale, formal, online learning environments, but research using these methods on high volume, informal, Massive Open Online Course (MOOC) forums is less common. The challenge in this setting is to develop valid and reliable indicators that operate successfully at scale. In this study, we test two established coding schemes used for the pedagogical content analysis of online discussions in a large-scale review of MOOC comment data. Pedagogical Scores are derived from manual ratings applied to comments by raters and correlated with automatically derived linguistic and interaction indicators. Results show that the content analysis methods are reliable, and are very strongly correlated with each other, suggesting that their specific format is not significant in this setting. In addition, the methods are strongly associated with the relevant linguistic indicators of higher levels of learning and have weaker correlations with other linguistic and interaction metrics. This suggests promise for further research using Machine Learning techniques, with the goal of providing realistic feedback to instructors, learners, and learning designers.},
   author = {Tim O'Riordan and David E Millard and John Schulz},
   doi = {https://doi.org/10.1002/cae.22314},
   issue = {4},
   journal = {Computer Applications in Engineering Education},
   keywords = {MOOC,content analysis,discussion forums,education},
   pages = {690-709},
   title = {Is critical thinking happening? Testing content analysis schemes applied to MOOC discussion forums},
   volume = {29},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.22314},
   year = {2021}
}
@article{Mustafa2021,
   abstract = {Learning to memorize the Quran presents a challenge. This paper reports the development and alpha testing of a mobile application called “EzHifz” for Quran memorization based on the VARK learning style. The application received positive feedback for user acceptance testing and heuristic testing. The Fleiss kappa coefficient (κ) results for user acceptance testing show a very good level of agreement (κ = 0.850). Heuristic testing results show that κ = 0.731 for content, manual guide, memorization activities, performance information, and tasmik assessment attributes, while κ = 0.727 for presentation design, interactivity, multimedia elements, attraction, and motivation attributes. These results show a good level of agreement, which indicates that the EzHifz application meets the requirements of design and development based on the attributes evaluated. A combination of memorizing techniques in the application helps strengthen the use of preferred VARK learning styles. The techniques support the use of multiple senses that could facilitate the process of memorizing the Quran independently. This study contributes to the novel design and evaluation of the Quran memorization application based on the Quran memorization model. The application supports the teaching and learning of Quran memorization where it allows students to select their preferred VARK learning style with the technique of memorizing the Quran. This mobile application learning approach based on VARK’s learning style has the potential to be implemented in the process of memorizing the Quran as well as retaining memory through the use of memory senses in support of the learning materials developed.},
   author = {Nor Musliza Mustafa and Zulkifly Mohd Zaki and Khairul Anuar Mohamad and Mokmin Basri and Sedek Ariffin},
   doi = {https://doi.org/10.1155/2021/5567001},
   issue = {1},
   journal = {Advances in Human-Computer Interaction},
   pages = {5567001},
   title = {Development and Alpha Testing of EzHifz Application: Al-Quran Memorization Tool},
   volume = {2021},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/5567001},
   year = {2021}
}
@article{Chen2023,
   abstract = {The COVID-19 pandemic exacerbates challenges faced by human immunodeficiency virus patients, who are at heightened risk for infection due to compromised immune systems. This study aims to develop a reliable, home-based point-of-care testing (POCT) tool for early screening of acquired immunodeficiency syndrome (AIDS) coinfected with Talaromyces marneffei infection. Employing a “From weak to strong” deduction strategy for feature selection, data from 464 AIDS patients across four cohorts between February 5th, 2014, and January 8th, 2022, are analyzed. The top three features consistently observed are D-dimer, cluster of differentiation 4+, and aspartate transaminase. Based on these features, the simplest risk-scoring model is constructed, with the area under the receiver operating characteristic curve values of 0.91, 0.80, and 0.69 in the hold-out cohort, external cohort 1, and external cohort 2, respectively. This “From weak to strong” deduction strategy identifies advantageous clinical features, enabling the development of simplified clinical risk scores with multiple biomarkers. To facilitate practical implementation, enhanced POCT tools are introduced, specifically a strip with segmented testing capabilities that demonstrates sensitivity and strong correlation with clinical scoring models. Furthermore, an open-access website and a free Android mobile app are created to support community utilization. The findings underscore the effectiveness of the innovative deduction strategy and enhanced test strips, which enable bedside measurements without laboratory dependency.},
   author = {Haoman Chen and Hui Ye and Fanxuan Chen and Yi Shi and Jiahe Li and Yuhang Li and Chaoyi Wei and Chengxi Zhang and Jinhao Jin and Quanyue Zhang and Shuo Pan and Dong Chen and Shichao Quan and Xiaowei Xu and Zongxing Yang and Lijun Xu and Yungui Zhang and Yi Wang and Yunlong Zhou and Ou Liu and Xiaoming Jiang and Liyu Liu and Qingzu He and Xiaoqu Hu and Jingye Pan and Jianwei Shuai and Feifei Su},
   doi = {https://doi.org/10.1002/aisy.202300224},
   issue = {11},
   journal = {Advanced Intelligent Systems},
   keywords = {AIDS opportunistic infections,feature selection strategy,machine learning,point-of-care testing,risk scoring system,test strips},
   pages = {2300224},
   title = {Revolutionizing Infection Risk Scoring: An Ensemble “From Weak to Strong” Deduction Strategy and Enhanced Point-of-Care Testing Tools},
   volume = {5},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202300224},
   year = {2023}
}
@article{Kamal2022,
   abstract = {A long testing period is usually required for the life testing of high-reliability products or materials. It is possible to shorten the testing process by using ALTs (accelerated life tests). Due to the fact that ALTs test products in harsher settings than are typical use conditions, the life expectancy of the objects they evaluate is reduced. Censored data in which the specific failure timings of all units assigned to test are not known, or all units assigned to test have not failed, may arise in ALTs for a variety of reasons, including operational failure, device malfunction, expense, and time restrictions. In this paper, we have considered the step stress partially accelerated life test (SSPALT) under two different censoring schemes, namely the type-I progressive hybrid censoring scheme (type-I PHCS) and the type-II progressive censorship scheme (type-II PCS). The failure times of the items are assumed to follow NH distribution, while the tampered random variable (TRV) model is used to explain the effect of stress change. In order to obtain the estimates of the unknown parameters, the maximum likelihood estimation (MLE) approach is adopted. Furthermore, based on the asymptotic theory of MLEs, the approximate confidence intervals (ACIs) are also constructed. The point estimates under two censoring schemes are compared in terms of root mean squared errors (RMSEs) and relative absolute biases (RABs), while ACIs are compared in terms of their lengths and coverage probabilities (CPs). The performance of the estimators has been evaluated and compared under two censoring schemes with various sample sizes through a simulation study. Simulation results show that estimates with type-I PHCS outperform estimates with type-II PCS in terms of RMSEs, RABs, lengths, and CPs. Finally, a real-world numerical example of insulating fluid failure times is presented to show how the approaches will work in reality.},
   author = {Mustafa Kamal and Sabir Ali Siddiqui and Ahmadur Rahman and Hassan Alsuhabi and Ibrahim Alkhairy and Thierno Souleymane Barry},
   doi = {https://doi.org/10.1155/2022/3491732},
   issue = {1},
   journal = {Computational Intelligence and Neuroscience},
   pages = {3491732},
   title = {Parameter Estimation in Step Stress Partially Accelerated Life Testing under Different Types of Censored Data},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/3491732},
   year = {2022}
}
@article{Zheng2021,
   abstract = {In view of the current situation that the nursing teaching content is lacking pertinence and cannot effectively cultivate students’ critical thinking, this paper puts forward the application analysis of two-dimensional code online testing technology in nursing teaching in colleges and universities. Through the analysis of two-dimensional code recognition technology architecture and common application fields, an intelligent nursing teaching platform for two-dimensional code online testing technology was designed. Systematically, we summarize the advantages of intelligent nursing teaching platform, use flash and 3D technology to complete the virtualization of nursing teaching scene, and use XML technology to update and save teaching resources; we further build an intelligent nursing teaching platform by using the basic plate, nursing station plate, ward plate, dispensing room plate, and case and operation review plate. The experimental results show that under the two-dimensional code online test technology, the nursing teaching method in colleges and universities has strong teaching resource processing efficiency, improves the operation level of students’ nursing technology, and effectively cultivates the academic thinking of nursing students, which is of great significance to the progress of nursing teaching.},
   author = {Hai-yan Zheng and Xing-cheng Ran},
   doi = {https://doi.org/10.1155/2021/6380501},
   issue = {1},
   journal = {Scientific Programming},
   pages = {6380501},
   title = {Application of QR Code Online Testing Technology in Nursing Teaching in Colleges and Universities},
   volume = {2021},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/6380501},
   year = {2021}
}
@article{Ma2022,
   abstract = {Abstract Despite the growing importance of the microservice architecture (MSA), interactions among the various elements (e.g., services, endpoints, and versions) remain difficult to manage. This research devised a system, called version-based microservice analysis, monitoring, and visualization (VMAMV), including multiple proposed methods to facilitate the testing, monitoring, and visualization of microservice systems by considering service versions and risks. VMAMV can generate version-based service dependency graphs, provide graph search services, perform consumer-driven contract testing, and conduct a risk analysis. Besides, VMAMV can also notify users of existing anomalies and potential risks to facilitate the development and operation of microservice systems. Experiments were conducted to demonstrate the efficacy of the VMAMV system in the detection of service anomalies, old patch versions, low-usage service versions, the presentation of contract test results and service error chains, and the validity of risk analysis.},
   author = {Shang-Pin Ma and I-Hsiu Liu and Chun-Yu Chen and Yu-Te Wang},
   doi = {https://doi.org/10.1002/smr.2429},
   issue = {10},
   journal = {Journal of Software: Evolution and Process},
   keywords = {consumer-driven contract test,microservice,microservice architecture,microservice dependency,microservice monitoring,risk analysis,semantic versioning},
   pages = {e2429},
   title = {Version-based and risk-enabled testing, monitoring, and visualization of microservice systems},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2429},
   year = {2022}
}
@article{Nie2023,
   abstract = {Abstract Mobile apps with tested Graphical User Interface (GUI) tend to have higher downloads in the apps store. In recent years, few efforts were made to analyse the research community and research status of the literature for GUI testing on mobile apps, which brings an obstacle to characterise and understand this field. In this study, the authors propose a systematic mapping study to gain insights into the field. First, the authors conduct an extensive search of relevant literature over seven popular digital libraries. From 4427 candidate studies, 114 primary studies published between January 2011 and September 2022 were selected. Next, the authors analyse these primary studies from the perspectives of bibliometric and qualitative analysis. For the bibliometric analysis, first, the authors analyse the popular research topics and their relationships. Second, the authors study the authors' community. For the qualitative analysis, the authors analyse the objectives, approaches and evaluation metrics employed in these primary studies. Their investigation reports several major findings: (1) there are relatively more studies on two topics, that is, test case generation and the automated test; (2) the most productive authors tend to collaborate and often have relatively broad research interests; (3) the functionality is the main objective of GUI testing; the model-based approach is the most widely used.},
   author = {Liming Nie and Kabir Sulaiman Said and Lingfei Ma and Yaowen Zheng and Yangyang Zhao},
   doi = {https://doi.org/10.1049/sfw2.12123},
   issue = {3},
   journal = {IET Software},
   keywords = {GUI testing,mapping study,mobile application},
   pages = {249-267},
   title = {A systematic mapping study for graphical user interface testing on mobile apps},
   volume = {17},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12123},
   year = {2023}
}
@article{Bertolino2020,
   abstract = {While great emphasis is given in the current literature about the potential of leveraging the cloud for testing purposes, the authors have scarce factual evidence from real-world industrial contexts about the motivations, drawbacks and benefits related to the adoption of automated cloud testing technology. In this study, the authors present an empirical study undertaken within the ongoing European Project ElasTest, which has developed an open source platform for end-to-end testing of large distributed systems. This study aims at validating the ElasTest solution, and consists of the assessment of four demonstrators belonging to different application domains, namely e-commerce, 5G networking, WebRTC and Internet of Things. For each demonstrator, they collected differing requirements, and achieved varying results, both positive and negative, showing that cloud testing needs careful assessment before adoption.},
   author = {Antonia Bertolino and Antonello Calabrò and Eda Marchetti and Anton Cervantes Sala and Guiomar de Hita and Ilie Daniel Gheorghe Pop and Varun Gowtham},
   doi = {https://doi.org/10.1049/iet-sen.2019.0140},
   issue = {5},
   journal = {IET Software},
   keywords = {5G networking,ElasTest response,ElasTest solution,European Project ElasTest,Internet of Things,WebRTC,application domains,automated cloud testing technology,automatic testing,cloud computing,cloud testing automation,demonstrator,distributed systems,e-commerce,electronic commerce,end-to-end testing,home automation,open source platform,program testing,public domain software,real-world industrial contexts,scarce factual evidence,testing purposes},
   pages = {553-562},
   title = {Cloud testing automation: industrial needs and ElasTest response},
   volume = {14},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2019.0140},
   year = {2020}
}
@article{Hierons2021,
   author = {Robert M Hierons and Tao Xie},
   doi = {https://doi.org/10.1002/stvr.1775},
   issue = {5},
   journal = {Software Testing, Verification and Reliability},
   pages = {e1775},
   title = {Editorial: Testing, Debugging, and Defect Prediction},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1775},
   year = {2021}
}
@article{Serpa2020,
   abstract = {Abstract Research in the area of collision detection permeates most of the literature on simulations, interaction and agents planning, being commonly regarded as one of the main bottlenecks for large-scale systems. To this day, despite its importance, most subareas of collision detection lack a common ground to test and validate solutions, reference implementations and widely accepted benchmark suites. In this paper, we delve into the broad-phase of collision detection systems, providing both an open-source framework, named Broadmark, to test, compare and validate algorithms, and an in-deep analysis of the main techniques used so far to tackle the broad-phase problem. The technical challenges of building this framework from the software and hardware perspectives are also described. Within our framework, several original and state-of-the-art implementations of CPU and GPU algorithms are bundled, alongside three benchmark scenes to stress algorithms under several conditions. Furthermore, the system is designed to be easily extensible. We use our framework to bring out an extensive performance comparison among assembled solutions, detailing the current CPU and GPU state-of-the-art on a common ground. We believe that Broadmark encompasses the principal insights and tools to derive and evaluate novel algorithms, thus greatly facilitating discussion about successful broad-phase collision detection solutions.},
   author = {Ygor Rebouças Serpa and Maria Andréia Formico Rodrigues},
   doi = {https://doi.org/10.1111/cgf.13884},
   issue = {1},
   journal = {Computer Graphics Forum},
   keywords = {CPU and GPU algorithms,Categories and Subject Descriptors (according to ACM CCS): Computer Graphics Computing Methodologies: Animation–Collision detection,broad phase,collision detection,open-source framework,state-of-the-art implementations},
   pages = {436-449},
   title = {Broadmark: A Testing Framework for Broad-Phase Collision Detection Algorithms},
   volume = {39},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13884},
   year = {2020}
}
@article{Resing2019,
   abstract = {Abstract This study examined whether computerized dynamic testing by utilizing a robot would lead to different patterns in children's (aged 6–9 years) potential for learning and strategy use when solving series-completion tasks. The robot, in a “Wizard of Oz” setting, provided instructions and prompts during dynamic testing. It was found that a dynamic training resulted in greater accuracy and more correctly placed pieces at the post-test than repeated testing only. Moreover, children who were dynamically trained appeared to use more heuristic strategies at the post-test than their peers who were not trained. In general, observations showed that children were excited to work with the robot. All in all, the study revealed that computerized dynamic testing by means of a robot has much potential in tapping into children's potential for learning and strategy use. The implications of using a robot in educational assessment were stressed further in the discussion.},
   author = {Wilma C M Resing and Merel Bakker and Julian G Elliott and Bart Vogelaar},
   doi = {https://doi.org/10.1111/jcal.12358},
   issue = {4},
   journal = {Journal of Computer Assisted Learning},
   keywords = {computer,dynamic testing,educational assessment,inductive reasoning,robot,series completion},
   note = { JCAL-18-302.R1},
   pages = {540-554},
   title = {Dynamic testing: Can a robot as tutor be of help in assessing children's potential for learning?},
   volume = {35},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12358},
   year = {2019}
}
@article{Menegassi2020,
   abstract = {Cross-platform apps stand out by their ability to run in various operating systems (OSs), such as Android, iOS, and Windows. Such apps are developed using popular frameworks for cross-platform app development such as Apache Cordova, Xamarin, and React Native. However, the mechanisms to automate their tests are not cross-platform and do not support multiple configurations. Hence, different test scripts have to be coded for each platform, yet there is no guarantee they will work in different configurations varying, e.g. platform, OS version, and hardware available. This study proposes mechanisms to produce automated tests for cross-platform mobile apps. In order to set up the tests to execute in multiple configurations, the authors’ approach adopts two reference devices: one running Android and other iOS. As both platforms have their own user interface (UI) XML representation, they also investigated six individual expression types and two combined strategies to locate UI elements. They have developed a prototype tool called cross-platform app test script recorder (x-PATeSCO) to support the proposed approach, as well as the eight locating strategies considered. They evaluated the approach with nine cross-platform mobile apps, comparing the locating strategies in six real devices.},
   author = {Andre Augusto Menegassi and Andre Takeshi Endo},
   doi = {https://doi.org/10.1049/iet-sen.2018.5445},
   issue = {1},
   journal = {IET Software},
   keywords = {XML,automated tests,cross-platform app development,cross-platform app test script recorder,cross-platform apps,cross-platform mobile apps,different test scripts,mobile computing,multiple configurations,operating systems (computers),program testing,user interfaces},
   pages = {27-38},
   title = {Automated tests for cross-platform mobile apps in multiple configurations},
   volume = {14},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5445},
   year = {2020}
}
@inbook{Sanchez2024,
   abstract = {Summary Chatbots are vulnerable to vulnerabilities and attacks that are often utilized, such as cross-site scripting and SQL injections. As a result of these variables, it is quite probable that chatbots will also be the target of an attack. As a result of this, it is vital, while analyzing chatbots, to examine for problems about their level of security. The process of scanning and assessing vulnerabilities is a thorough analysis of computer networks and the components that make up such networks. By doing this study, we want to get a better understanding of the safety precautions that are now in place as well as the degree to which they are effective. The fundamental objective of penetration testing is to locate vulnerabilities in a protected setting so that they may be patched up before they are exploited by malicious actors (hackers, for example). In addition, doing penetration testing may be of use in determining how well the system can withstand genuine assaults. This article presents an investigation of vulnerability assessment and penetration testing for chatbots.},
   author = {Domenic T Sanchez and Rodel S Sartagoda},
   doi = {https://doi.org/10.1002/9781394200801.ch19},
   isbn = {9781394200801},
   booktitle = {Conversational Artificial Intelligence},
   keywords = {Chatbot security,attacks,penetration testing,privacy,threats,vulnerabilities,vulnerability assessment},
   pages = {303-318},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Security Threats and Security Testing for Chatbots},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781394200801.ch19},
   year = {2024}
}
@article{Hong2021,
   abstract = {Abstract This study tests how evaluations of various performances of artificial intelligence (AI) influence the willingness to use AI technologies. We examined the influences of (a) whether expectations about AI performances are met, (b) the valence of the violation, and (c) the types of AI performances, using a 2 (expectancy violation vs. confirmation) × 2 (positive vs. negative evaluation) × 2 (humanlike vs. machine-distinctive performance) design. The relationship between attitude toward AI and intention to use AI was also analyzed. Participants (n = 238) in an online survey read randomly assigned reading materials about AI performances. We found that people prefer to use AI with machine-distinctive performances instead of AI with humanlike performances. Also, the violation of expectations about AI performances was found to be a factor that affects the intention to use AI technologies. Finally, there was a two-way interaction effect between AI performance types and the expectancy violation. The implications for expectancy violations theory and perceiving the humanness of AI performances are discussed.},
   author = {Joo-Wha Hong},
   doi = {https://doi.org/10.1002/hbe2.292},
   issue = {5},
   journal = {Human Behavior and Emerging Technologies},
   keywords = {anthropomorphism,artificial intelligence,expectation violation theory,human–computer interaction,technology acceptance},
   pages = {1023-1032},
   title = {Artificial intelligence (AI), don't surprise me and stay in your lane: An experimental testing of perceiving humanlike performances of AI},
   volume = {3},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbe2.292},
   year = {2021}
}
@article{Omeh2022,
   abstract = {Abstract This study focused on the impact of teaching a computer programming course using innovative pedagogy namely a combination of context-based learning and problem-based interspersed with live online tools (Google Classroom and Google Meet) on university students in southeast, Nigeria. This study used a quasi-experimental research design and intact classes with a nonequivalent group. The population for the study comprised all 152 second-year computer education students in universities in Southeast Nigeria that offers computer programming. The sample consists of 60 males and 92 females in the three universities used. A computer programming achievement test was used to collect the data. The result showed that students worked collaboratively on the Google classroom and Google meet platforms, effectively enhancing collaboration learning among novice programmers as they chose when to learn, thus making the teaching and learning of programming less difficult. Findings further showed that student academic achievement significantly improved in programming skills, digital skills development, and self-efficacy in the treatment group more than in the control group. This improved performance was attributed to self-efficacy which was attributed to IP students were exposed to. It was suggested that computer lecturers should integrate innovative pedagogy using Google classroom and Google meet in their instructional delivery approaches.},
   author = {Christian B Omeh and Chijioke J Olelewe and Emmanuel C Nwangwu},
   doi = {https://doi.org/10.1002/cae.22527},
   issue = {5},
   journal = {Computer Applications in Engineering Education},
   keywords = {Google classroom,computer programming,context-based learning,digital skills development,innovative Pedagogy,problem-based learning,self-efficacy},
   pages = {1390-1405},
   title = {Impact of teaching computer programming using innovative pedagogy embedded with live online lectures and related tools: A randomized control trial},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.22527},
   year = {2022}
}
@article{Rakovi2021,
   abstract = {Abstract Developing knowledge-transforming skills in writing may help students increase learning by actively building knowledge, regardless of the domain. However, many undergraduate students struggle to transform knowledge when drafting essays based on multiple sources. Writing analytics can be used to scaffold knowledge transforming as writers bring evidence to bear in supporting claims. We investigated how to automatically identify sentences representing knowledge transformation in argumentative essays. A synthesis of cognitive theories of writing and Bloom's typology identified 22 linguistic features to model processes of knowledge transforming in a corpus of 38 undergraduates' essays. Findings indicate undergraduates mostly paraphrase or copy information from multiple sources rather than engage deeply with sources' content. Eight linguistic features were important for discriminating evidential sentences as telling versus transforming source knowledge. We trained a machine learning algorithm that accurately classified nearly three of four evidential sentences as knowledge-telling or knowledge-transforming, offering potential for use in future research.},
   author = {Mladen Raković and Philip H Winne and Zahia Marzouk and Daniel Chang},
   doi = {https://doi.org/10.1111/jcal.12531},
   issue = {4},
   journal = {Journal of Computer Assisted Learning},
   keywords = {argumentative writing,evidence,knowledge telling,knowledge transforming,machine learning},
   pages = {903-924},
   title = {Automatic identification of knowledge-transforming content in argument essays developed from multiple sources},
   volume = {37},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12531},
   year = {2021}
}
@article{Chen2022,
   abstract = {Touch is one of the most important human senses. With the popularization of touch-screen mobile phones, tablet computers, and other devices, touch-screen interactive technology has become a norm in people’s daily lives, and advertisements that were once dominated by vision and hearing have added an interactive experience in the dimension of touch. Traditional advertising media screens can only complete simple information dissemination functions and cannot interact with users in a two-way manner. They can only receive information one-way and passively and lack interactivity. Touch-screen interactive advertising forms a good interaction with the target audience, thereby disseminating advertising information to achieve the purpose of promotion or brand image building. This paper designs a set of advertising media screen interaction systems based on smart sensors, including a gesture interaction module, a remote interaction module, and a touch interaction module. The gesture interaction module can recognize 5 static gestures and send gesture commands to control the advertising media screen. The remote interaction module can remotely control the advertising media screen, and the touch interaction module can control the advertising media screen through the touch screen. According to the functional requirements, the overall design of software and hardware is given, and the technical background of each module of the software is introduced. Next, the depth image-based gesture recognition method is studied. The number of fingers and the center distance feature are fused as feature vectors, and the weighted template matching method is used to classify and recognize gestures. Finally, the design and implementation of the interactive system are introduced.},
   author = {Si Chen and Dandan Cheng and Quan Zhou},
   doi = {https://doi.org/10.1155/2022/4467739},
   issue = {1},
   journal = {Scientific Programming},
   pages = {4467739},
   title = {Design and Application of Interactive Algorithm for Advertising Media Screen Based on Smart Sensor},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/4467739},
   year = {2022}
}
@article{RoyChoudhury2021,
   abstract = {Abstract The Integrated Child Development Services of India provides Supplementary Nutrition Program (SNP) to preschoolers. Using this platform, the current study examined the impact of diversifying a cereal/pulse–based SNP-meal with guava on iron status and cognitive development among 24–48 months old children. A three-arm, nonblinded, cluster-randomized controlled trial (CTRI/2014/09/004983) included 399 beneficiaries from 28 preschools in 16 villages in Telangana state, India. The villages were randomly assigned to receive 25 g of guava (guava group (GG)), banana (banana group (BG)), or cucumber (cucumber group (CG)) along with a SNP meal for 140 days. Nutrient biomarkers (iron status, plasma vitamin C, vitamin B12, and folate), cognitive development, anthropometric indicators (WAZ, HAZ, and WHZ), and morbidity were assessed at baseline and endline. A linear mixed model and a generalized estimating equation were applied to compare changes in outcomes across the groups. All outcome variables were comparable across groups at baseline. The iron to vitamin C molar ratio improved in the GG from 1:1.4 to 1:12 but remained unaltered in control groups. Higher hemoglobin (P = 0.002), serum ferritin (SF; P < 0.001), vitamin C (P = 0.047), and lower soluble transferrin receptor (sTfR; P < 0.001) causing decreased prevalence of iron deficiency (ID) (P = 0.003) were observed in the GG compared with BG and CG. Prevalence of acute respiratory infection (ARI) was lower in the GG (P = 0.035) versus controls. No impact was observed on cognitive development or growth. Thus, diversifying a cereal/pulse–based meal with guava increased meal vitamin C content, thereby reducing ID and ARI-related morbidity. This approach represents a valid and scalable strategy to address ID among young children.},
   author = {Dripta Roy Choudhury and Madhavan Nair Krishnapillai and Balakrishna Nagalla and Radhakrishna Vijaya Kankipati and Sudip Ghosh and Jagdish Buwade and Sylvia Fernandez-Rao},
   doi = {https://doi.org/10.1111/nyas.14556},
   issue = {1},
   journal = {Annals of the New York Academy of Sciences},
   keywords = {growth and development,guava,iron status,morbidity,preschoolers,supplementary meal},
   pages = {82-95},
   title = {Guava with an institutional supplementary meal improves iron status of preschoolers: a cluster-randomized controlled trial},
   volume = {1492},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14556},
   year = {2021}
}
@article{Skulmowski2021,
   abstract = {Abstract Previous research in technology-enhanced learning suggests that realism in visualizations can overwhelm learners and lower their learning performance. However, some studies offer evidence that realism may be helpful in learning tests that require detailed visual knowledge. A potential mechanism may be that realistic details act as retrieval cues (i.e., visual cues that facilitate retrieval during testing). In order to assess the impact of realism in learning and testing, we conducted an experiment (N = 40) using realistic and schematic renderings generated for this study. Realism was varied as a between-subjects factor while participants completed both a schematic and realistic retention test. As hypothesized, a realistic test visualization primarily benefits those who had learned using a realistic rather than a schematic visualization. In addition, the realistic test led to higher retention scores regardless of the realism level of the learning materials. A second experiment (n = 50) was conducted to assess whether schematic visualizations can be made more learner-friendly by including verbal descriptions that enhance the less detailed visualizations with additional cues. The experiment revealed that learning with schematic visualizations can be fostered using verbal descriptions for a multiple-choice retention test, but at the cost of lower scores in an image-based retention test. Both studies suggest that matching the realism and concreteness in the learning and testing stages lead to the best performance. Our results have implications for the design of a range of technology-enhanced learning contexts such as virtual reality education.},
   author = {Alexander Skulmowski and Günter Daniel Rey},
   doi = {https://doi.org/10.1002/hbe2.209},
   issue = {2},
   journal = {Human Behavior and Emerging Technologies},
   keywords = {concreteness,e-learning,human cognition,human-computer interaction,instructional design,learning,realism,retrieval cues,retrieval specificity,visualizations},
   pages = {283-295},
   title = {Realism as a retrieval cue: Evidence for concreteness-specific effects of realistic, schematic, and verbal components of visualizations on learning and testing},
   volume = {3},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbe2.209},
   year = {2021}
}
@article{Raz2022,
   abstract = {Abstract Cognitive impairment has been associated with anemia and iron deficiency; however, brain electrophysiological studies correlating red blood cell (RBC) indices and iron status to cognition in adulthood are scarce. We aimed to assess neurocognitive function in young adults of the general population and its correlation with RBC indices and iron status. Neurocognitive function was investigated using scalp-recorded event-related potentials (ERPs) within the context of a task-switching paradigm. ERPs and test performance were also compared across groups of “high”/“low” RBC and iron indices. Working memory was examined using the digit span test, in which mean corpuscular hemoglobin (MCH), mean corpuscular volume (MCV), and ferritin were found to be significant predictors of test performance, with higher MCH/MCV/ferritin being associated with better test scores. In the switching task, MCH, MCV, and ferritin were found to be significant predictors of task performance, with higher MCH/MCV/ferritin levels associated with a lower percentage of errors. Electrophysiological results showed that MCH and MCV were significant predictors of ERPs amplitude, with lower MCH/MCV levels associated with greater amplitude, which may reflect compensatory processes. P1, N1, P2, and P3 were greater for the low MCH/MCV groups. This is the first evidence of association between levels of MCH/MCV and brain function while engaged in an executive function task; possibly reflecting brain iron availability.},
   author = {Sivan Raz and Ariel Koren and Carina Levin},
   doi = {https://doi.org/10.1111/nyas.14877},
   issue = {1},
   journal = {Annals of the New York Academy of Sciences},
   keywords = {cognitive function,event-related potentials,iron status,red blood cell indices},
   pages = {300-313},
   title = {Associations between red blood cell indices and iron status and neurocognitive function in young adults: Evidence from memory and executive function tests and event-related potentials},
   volume = {1517},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14877},
   year = {2022}
}

@article{Abuali2022,
   abstract = {This paper aims to find the relationship between the full diacritization of the Arabic text and the quality of the speech synthesized in screen readers and presents a new methodology to develop screen readers for the visually impaired, focusing on preprocessing and diacritization of the text before converting it to audio. First, the actual need for our proposal was measured by conducting a MOS (Mean Opinion Score) questionnaire to evaluate the quality of the speech synthesized before and after full diacritization in the NVDA (https://www.nvda-ar.org/) screen reader. Then, an e-reader was built by integrating two models: the first one is for automatic Arabic diacritization (depending on Shakkala), and the second is a TTS model (depending on Tacotron). The quality of our proposed system was measured in terms of (1) pronunciation and (2) intelligibility, in which our system outperformed the commercial screen readers, NVDA and IBSAR (https://www.sakhr.com), as it recorded 60.67\%, 17.67\%, and 21.67\% as correct, incorrect, and partially correct, respectively, for the isolated word test, and 84\% correct results for the homograph test, and 78.50\% and 93\% correct results, respectively, for the DRT and DMRT tests.},
   author = {Batool Abuali and Mohamad-Bassam Kurdy},
   doi = {https://doi.org/10.1155/2022/1186678},
   issue = {1},
   journal = {Advances in Human-Computer Interaction},
   pages = {1186678},
   title = {Full Diacritization of the Arabic Text to Improve Screen Readers for the Visually Impaired},
   volume = {2022},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/1186678},
   year = {2022}
}
@article{Vakkari2021,
   abstract = {Abstract Few studies have investigated how search behavior affects complex writing tasks. We analyze a dataset of 150 long essays whose authors searched the ClueWeb09 corpus for source material, while all querying, clicking, and writing activity was meticulously recorded. We model the effect of search and writing behavior on essay quality using path analysis. Since the boil-down and build-up writing strategies identified in previous research have been found to affect search behavior, we model each writing strategy separately. Our analysis shows that the search process contributes significantly to essay quality through both direct and mediated effects, while the author's writing strategy moderates this relationship. Our models explain 25–35\% of the variation in essay quality through rather simple search and writing process characteristics alone, a fact that has implications on how search engines could personalize result pages for writing tasks. Authors' writing strategies and associated searching patterns differ, producing differences in essay quality. In a nutshell: essay quality improves if search and writing strategies harmonize—build-up writers benefit from focused, in-depth querying, while boil-down writers fare better with a broader and shallower querying strategy.},
   author = {Pertti Vakkari and Michael Völske and Martin Potthast and Matthias Hagen and Benno Stein},
   doi = {https://doi.org/10.1002/asi.24451},
   issue = {7},
   journal = {Journal of the Association for Information Science and Technology},
   pages = {839-852},
   title = {Predicting essay quality from search and writing behavior},
   volume = {72},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24451},
   year = {2021}
}

@article{Balasubramanian2024,
   abstract = {Summary Nowadays, autism spectrum disorder (ASD) is one of the fastest-growing developmental disorders globally. Screening test consumes more time and expensive to detect the autism signs. Due to advances in artificial intelligence and machine learning (ML), Autism can be predicted at the initial stage. Numerous researches have been conducted using different methods, but none of these researches presented any anticipated results about the capability to predict autism traits under different age groups. Therefore, this paper proposes an effectual prediction method based upon ML strategy to develop a mobile application for predicting ASD of any age people. The autism prediction model is developed by five ML classifiers, such as Gaussian Naive Bayes, Decision Tree Classifier, K-Nearest Neighbors (KNN), Multinomial Logistic Regression (MLR), Support Vector Machine (SVM) and also a mobile application is developed using proposed prediction method. The proposed method is analyzed with IAPQ records gathered from certain areas in Erode district, Tamil Nadu, India. From the analysis, the SVM classifier achieves maximum sensitivity of 23.14\%, 6.04\%, 5.89\%, 11.03\% than other classifiers, like Gaussian Naive Bayes, Decision Tree Classifier, KNN, and MLR.},
   author = {Jaishankar Balasubramanian and Bharathi Gururaj and Nandam Gayatri},
   doi = {https://doi.org/10.1002/cpe.7898},
   issue = {2},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {Decision Tree Classifier,Gaussian Naive Bayes,Indian Autism Parental Questionnaire,machine learning},
   pages = {e7898},
   title = {An effective autism spectrum disorder screening method using machine learning classification techniques},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7898},
   year = {2024}
}
@article{McKay2024,
   abstract = {Abstract Information is powerful; it can influence peoples' views and, in turn, their behavior. Much recent research and discussion on the role information plays in view change has focused on filter bubbles, echo chambers and misinformation and how they might influence what people think and how they act. However, no prior work has focused specifically on understanding the human information behavior (HIB) that drives and facilitates view change. We report findings from interviews with 18 people who recently changed views on issues they considered important. We found a tight symbiotic relationship between HIB and view change; passive information encountering sparked change, often spurring follow-up active seeking and verification which progressed the change to a “point of no return,” supported making the change and reinforced the decision to change. When shared, information that contributed to the change sometimes sparked changes in others (as did expressing or debating the change), serving as an information encounter that perpetuated a cycle of HIB and view change. This understanding of the integral role of HIB in view change can inform policy and systems design to promote view change autonomy and a broader research agenda of understanding HIB to support democratic principles and values.},
   author = {Dana McKay and Stephann Makri and Marisela Gutierrez-Lopez and Colin Porlezza and Andrew Macfarlane and Glenda Cooper and Sondess Missaoui},
   doi = {https://doi.org/10.1002/asi.24885},
   issue = {7},
   journal = {Journal of the Association for Information Science and Technology},
   pages = {844-858},
   title = {I'm the same, I'm the same, I'm trying to change: Investigating the role of human information behavior in view change},
   volume = {75},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24885},
   year = {2024}
}
@article{Malik2021,
   abstract = {Abstract Manual testing of software requirements written in natural language for agile or any other methodology requires more time and human resources. This leaves the testing process error prone and time consuming. For satisfied end users with bug-free software delivered on time, there is a need to automate the test oracle process for natural language or informal requirements. The automation of the test oracle is relatively easier with formal requirements, but this task is difficult to achieve with natural language requirements. This study proposes an approach called Restricted Natural Language Agile Requirements Testing (ReNaLART) to automate the test oracle from restricted natural language agile requirements. For this purpose, it uses an existing user story template with some modifications for writing user stories. This helps in identifying test input and expected output for a user story. For comparison of expected and observed outputs it makes use of a regex pattern and string distance functions. It is capable of assigning different types of verdicts automatically depending upon the similarity/dissimilarity between observed and expected outputs of user stories. ReNaLART is validated using several case studies of different domains, namely, OLX Pakistan, Mental Health Tests, McDelivery Pakistan, BlueStacks, Power Searching with Google, TensorFlow Playground, w3Schools 2018 offline and Touch'D. It revealed several faults in five of the above listed eight applications. Plus, the proposed test oracle on an average took 0.02 s for test data generation, expected output generation and verdict assignment. Both these facts show the fault revealing effectiveness and efficiency of ReNaLART.},
   author = {Maryam Imtiaz Malik and Muddassar Azam Sindhu and Akmal Saeed Khattak and Rabeeh Ayaz Abbasi and Khalid Saleem},
   doi = {https://doi.org/10.1111/exsy.12608},
   issue = {1},
   journal = {Expert Systems},
   keywords = {agile requirements,automated test oracle,automated test-driven development,behaviour-driven development,restricted natural language},
   pages = {e12608},
   title = {Automating test oracles from restricted natural language agile requirements},
   volume = {38},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12608},
   year = {2021}
}
@article{Li2024-6,
   abstract = {Abstract Software process simulation models (SPSMs) that are based on descriptive process models offer the executability that can demonstrate dynamic changes of software processes over time. Verification and validation (V&V) is critical in SPSMs for guaranteeing the quality and reliability of models. V&V of dynamic software process models is more complex and challenging than for static software process models. This work systematically summarizes and maps V&V studies in SPSM to provide guidelines for future research and practice. Specifically, this study aims at identifying the focus of research on V&V, the methods used for V&V, and how to implement V&V of SPSMs in software engineering research. We conducted a systematic mapping study on studies of SPSMs that report on their V&V activities. Under the guidance of a V&V meta-model for SPSMs, we study four research questions about V&V process. We identified 107 primary studies from a pool of 313 papers on SPSMs until 2021. There are two main results of our study. The first one presents the relationship between quality aspects of SPSMs and the V&V methods to assure them. The second result reveals the relationships among the modeling process, three modeling steps, five quality aspects, and 10 V&V methods. Generally, researchers do not pay sufficient attention to V&V, as 65.8\% ( (313−107)/313) failed to mention or elaborate on their V&V process. We systematically summarize and map the state-of-the-art V&V research in software process modeling field to support modelers' practice and improve their V&V process.},
   author = {Yue Li and He Zhang and Bohan Liu and Liming Dong and Haojie Gong and Guoping Rong},
   doi = {https://doi.org/10.1002/smr.2612},
   issue = {6},
   journal = {Journal of Software: Evolution and Process},
   keywords = {model verification and validation,process simulation,software process modeling,systematic mapping study},
   pages = {e2612},
   title = {Verification and validation of software process simulation models: A systematic mapping study},
   volume = {36},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2612},
   year = {2024}
}
@article{Kumari2019,
   abstract = {With an exponential growth of raw data generated from different sensors, actuators, and mobile devices, data analytics is becoming a challenging issue keeping in view of the heterogeneity of the data generated. The traditional database systems are not able to handle this huge amount of data. Current research and development are mainly focused on the analytics of this big data generated from different devices and overlook the difficulty of its secure streaming. In this study, the authors analyse and provide an overview on how to handle secure streaming data generated from different devices. It includes the major threats and risks, introduced during big data processing in Internet of things environment. Moreover, they analyse the existing security approaches and highlight emerging challenges required to process the heterogeneous big data in a secure manner for IoT applications, such as denial of service, malware, and phishing. The architectural details and security approaches required in each phase of big data processing life-cycle are explored in detail. Finally, various research challenges along with a case study are discussed and analysed.},
   author = {Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar},
   doi = {https://doi.org/10.1049/iet-net.2018.5187},
   issue = {3},
   journal = {IET Networks},
   keywords = {Big Data,Internet of Things,Internet of Things environment,IoT applications,actuators,big data analytics,big data processing life-cycle,data analysis,database systems,denial of service,malware,mobile devices,phishing,secure streaming data,security approaches,security of data,sensors,validation techniques,verification techniques},
   pages = {155-163},
   title = {Verification and validation techniques for streaming big data analytics in internet of things environment},
   volume = {8},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-net.2018.5187},
   year = {2019}
}
@article{Ali2021,
   abstract = {Abstract Model validation methods (e.g., k-fold cross-validation) use historical data to predict how well an estimation technique (e.g., random forest) performs on the current (or future) data. Studies in the contexts of software development effort estimation (SDEE) and software fault prediction (SFP) have used and investigated different model validation methods. However, no conclusive indications to suggest which model validation method has a major impact on the prediction accuracy and stability of estimation techniques. Some studies have investigated model validation methods specific to data about either SDEE or SFP. To the best of our knowledge, there is no study in the literature, which has employed different validation methods both with SDEE and SFP data. The aim of this paper is to consider different methods (10) from the family of cross-validation (CV) and bootstrap validation methods to identify which one contributes to obtaining a better prediction accuracy for both types of data. We also evaluate which model validation methods allow the estimation techniques to provide stable performances (i.e., with lower variance). To this aim, we present an empirical study involving six datasets from the domain of SDEE and six datasets from the SFP domain. The results reveal that repeated 10-fold CV with SDEE and optimistic boot with SFP data are the model validation methods that provide a better prediction accuracy in a greater number of experiments than the other model validation methods. Furthermore, a model validation method can improve the prediction accuracy up to 60\% with SDEE data and up to 36\% when employing SFP data. The analysis also reveals that repeated fivefold CV produces more stable performances when the experiments are repeated on the same data.},
   author = {Asad Ali and Carmine Gravino},
   doi = {https://doi.org/10.1002/smr.2367},
   issue = {8},
   journal = {Journal of Software: Evolution and Process},
   keywords = {model validation methods,software development efforts estimation,software faults prediction},
   pages = {e2367},
   title = {An empirical comparison of validation methods for software prediction models},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2367},
   year = {2021}
}
@article{Gissel2021,
   abstract = {Abstract A large-scale, cluster-randomized controlled field trial (Nclassrooms = 47; Nstudents = 1,013) assessed the impact of a digital text-to-speech reading material that supported 8-year-olds’ decoding and reading comprehension. An active control group used the most prevalent Danish learning material with a research-based systematic, explicit phonics approach supporting primarily decoding. The digital tool allows children to read unfamiliar text for meaning. Students are supported in mapping between orthography and phonology by three levels of text-to-speech support and in identifying spelling patterns. The risk of students overusing text-to-speech was countered by postponing access to having words read aloud by directing students towards identifying and training relevant orthographic patterns before activating text-to-speech. Results showed no statistically significant difference in decoding, but treatment improved reading comprehension. The study demonstrates how digital tools can facilitate strengthening students' decoding skills as efficiently as a traditional phonics-based programme while students are reading text of relatively high orthographic complexity for meaning.},
   author = {Stig Toke Gissel and Simon Calmar Andersen},
   doi = {https://doi.org/10.1111/jcal.12488},
   issue = {2},
   journal = {Journal of Computer Assisted Learning},
   keywords = {connectionism,digital learning tool,randomized controlled trial,reading instruction,text-to-speech},
   pages = {287-304},
   title = {A cluster-randomized trial measuring the effects of a digital learning tool supporting decoding and reading for meaning in grade 2},
   volume = {37},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12488},
   year = {2021}
}
@inbook{BERNOT2023,
   abstract = {Summary The biological regulation networks determine the behavior of biological systems and mathematical modeling to a large extent, and is essential to understand how they work, clarify the causal links in order to modify biological functions, design pharmaceutical treatments, etc., for example. This chapter illustrates the major contribution of formal methods for modeling complex biological systems in choosing to focus on regulatory networks. It introduces the first way to select the desired configuration based completely on the logic of Hoare: from a trace observed experimentally and transcribed into a formal language, it is possible to construct the constraints that make this trace possible in the formal model. The chapter presents the TotemBioNet platform dedicated to identifying parameters for the discrete formalization of René Thomas by using the complementarity of the two approaches based on Hoare's logic and temporal logic.},
   author = {Gilles BERNOT and Hélène COLLAVIZZA and Jean-Paul COMET},
   doi = {https://doi.org/10.1002/9781394229086.ch8},
   isbn = {9781394229086},
   booktitle = {Symbolic Approaches to Modeling and Analysis of Biological Systems},
   keywords = {René Thomas,TotemBioNet platform,biological regulation networks,biological systems,mathematical modeling,pharmaceutical treatments,regulatory networks},
   pages = {255-312},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Formal Verification Methods for Modeling in Biology},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781394229086.ch8},
   year = {2023}
}
@article{Rajput2022,
   abstract = {Abstract Breast cancer is detected in one out of eight females worldwide. Principally biomedical image processing techniques work with images captured by a microscope and then analyzed with the help of different algorithms and methods. Instead of microscopic image diagnosis, machine learning algorithms are now incorporated to detect and diagnose therapeutic imagery. Computer-aided mechanisms are used for better efficiency and reliability compared with manual pathological detection systems. Machine learning algorithms detect tumors by extracting features through a convolutional neural network (CNN) and then classifying them using a fully connected network. As Machine learning does not require prior expertise, it is profoundly used in biomedical imaging. This article has customized a convolutional neural network by mathematical modeling of a proposed activation function. We have obtained an appreciable prediction accuracy of up to 99\%, along with a precision of 0.97.},
   author = {Gunjan Rajput and Shashank Agrawal and Kunika Biyani and Santosh Kumar Vishvakarma},
   doi = {https://doi.org/10.1002/ima.22701},
   issue = {4},
   journal = {International Journal of Imaging Systems and Technology},
   keywords = {Mias dataset,breast cancer classification,convolutional neural network,deep learning,detection},
   pages = {1101-1118},
   title = {Early breast cancer diagnosis using cogent activation function-based deep learning implementation on screened mammograms},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ima.22701},
   year = {2022}
}
@article{Shukla2019,
   abstract = {Summary Unlike traditional networking devices, control and management plane are decoupled from data plane in software-defined networks (SDN). The logically centralized control and management plane facilitate dynamic orchestration of network resources, services, and policies by writing software programs. This provides much needed flexibility and programmability where networking rules and policies can be modified dynamically depending upon the application context. As the operation of network services entirely depends on a program, a small fault may induce several issues which can adversely affect the expected behavior of the network. Formal modeling and verification help in catching inconsistencies and existence of errors prior to the deployment of the programs that control the behavior of a network. In this paper, we provide a comprehensive survey of tools and techniques available in the literature for formal modeling and verification of SDN. These tools and techniques are classified based on their types, the components of SDN where they can be applied, and the design and development phase when they are utilized. In particular, their respective benefits and limitations are discussed in terms of ease of use, interfaces, and the ability to capture and verify intended network properties.},
   author = {Nitin Shukla and Mayank Pandey and Shashank Srivastava},
   doi = {https://doi.org/10.1002/nem.2082},
   issue = {5},
   journal = {International Journal of Network Management},
   note = {e2082 nem.2082},
   pages = {e2082},
   title = {Formal modeling and verification of software-defined networks: A survey},
   volume = {29},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nem.2082},
   year = {2019}
}
@article{RazaKazmi2020,
   abstract = {Summary Web service composition is a key technology to resolve cross-organizational business process integration, and it faces many constraints and requirements as unexpected bugs can cause shutdown of the software. A high degree of reliability is expected from such systems, and formal modeling of web service composition is an active area of research. In this work, we propose a new approach for the formal modeling of diverse web service composition. Our approach verifies the terminate property on all possible runs of the system to ensure deadlock-free workings. For a verification of the properties of interest, we use the NuSMV model checker and specify the properties in CTL*. The approach can be used to verify the soundness property of diverse web service composition to ensure live and deadlock-free web services.},
   author = {Syed Asad Raza Kazmi and Awais Qasim and Adnan Khalid and Ruttaba Assad and Muhammad Shahbaz},
   doi = {https://doi.org/10.1002/cpe.5249},
   issue = {21},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {CTL*,NuSMV,Petri nets,model checking,web service composition (WSC)},
   note = {e5249 cpe.5249},
   pages = {e5249},
   title = {Formal modeling and verification of cloud-based web service composition},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5249},
   year = {2020}
}
@article{Choi2023,
   abstract = {Summary Embedded control software that controls safety-critical IoT devices requires systematic and comprehensive verification to ensure safe operation of the device. However, rigorous verification in this domain has not been feasible due to the high complexity of embedded control software, which is characterized by the frequent use of multi-tasking, interrupts, and periodic alarms. Realizing that two major factors, scalability and exactness, are extremely difficult to achieve at the same time but critical for effective and efficient verification in this domain, this work introduces a domain-specific compositional OS-in-the-Loop (OiL) verification approach and sets out to push the boundary in achieving both factors. The suggested approach (1) models the behavior of the underlying operating system to limit the search space using the notion of controlled concurrency, (2) performs heterogeneous composition of controllers with the formal OS model to reduce verification complexity, and (3) utilizes state-of-the-art verification techniques for the purpose of comprehensive verification up to a given search depth.},
   author = {Yunja Choi},
   doi = {https://doi.org/10.1002/stvr.1834},
   issue = {1},
   journal = {Software Testing, Verification and Reliability},
   keywords = {OS-in-the-Loop,heterogeneous composition,model checking,multi-tasking},
   pages = {e1834},
   title = {OS-in-the-Loop verification for multi-tasking control software},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1834},
   year = {2023}
}
@article{Jilani2022,
   abstract = {Abstract Model transformation testing has become crucial as model-driven engineering has raised the abstraction level for developing software systems. Transformation is written to transform models from one level of abstraction to another, for example, model to model or model to code. A major challenge in testing the transformation is the creation of test models, such that (i) they conform to the source meta-model (i.e., multiplicities and Object Constraint Language [OCL] constraints on meta-model) and (ii) they provide coverage of the complete transformation (solving branch conditions for traversing all paths). Manual creation of test models requires a lot of time and effort. Still, the validity of the developed test models cannot be ensured. This paper aims to solve the above challenges using an automated search-based strategy. The proposed approach is two-stepped. First, valid test models are generated by solving source meta-model constraints. Second, the generated models are evolved for achieving the structural coverage of the transformation by solving the branch conditions. A toolset model transformation testing environment (MOTTER) is developed to automate the search-based solution. The proposed work is empirically evaluated on two case studies using four search algorithms. The result reflects that it successfully generates valid test models for achieving desired structural coverage with high performance on both the case studies.},
   author = {Atif Aftab Jilani and Muhammad Uzair Khan and Muhammad Zohaib Iqbal and Muhammad Usman},
   doi = {https://doi.org/10.1002/smr.2461},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   keywords = {constraint solving,instance generation,model transformation,search based,structural testing,test model},
   pages = {e2461},
   title = {An automated search-based test model generation approach for structural testing of model transformations},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2461},
   year = {2022}
}
@article{Ackermans2024,
   abstract = {Abstract Background A practical test that measures the information and communication technology (ICT) skills students need for effectively using ICT in primary education has yet to be developed (Oh et al., 2021). This paper reports on the development, validation, and reliability of a test measuring primary school students' ICT skills required for effectively using ICT (the ECC-ICT test). Objectives Based on existing literature, three ICT use domains were identified for effectively using ICT: Effective, collaborative, and creative use of ICT. For these three domains, 24 corresponding teaching objectives were identified from a widely used digital literacy framework. Thirty-four test items cover these teaching objectives in an online test. Methods A mixed-method approach was used for the ECC-ICT test. Four pilot rounds (n=25) implemented qualitative interviews for cognitive validity and refining the test items, followed by a qualitative usability study(n=6). Confirmatory factor analysis and ANOVA provided quantitative insight into the large-scale test administration(n=575). Results and Conclusions Composite reliability of our conceptual 3-factor confirmatory model showed that the test reliably measured primary school effective use of ICT (ω = 0.82), collaborative use of ICT (ω = 0.80) and creative use of ICT (ω = 0.64). Convergent validity (ranging from 0.41 to 0.46) was acceptable. Internal consistency (ranging from 0.84 to 0.91) and discriminant validity (HTMT values below 0.90) are good. ANOVA results show that mean test scores are higher for students in higher grade levels (p < 0.001). The post hoc Bonferroni results show that most grade-by-grade comparisons are significant (p < 0.001).},
   author = {Kevin Ackermans and Marjoke Bakker and Pierre Gorissen and Anne-Marieke van Loon and Marijke Kral and Gino Camp},
   doi = {https://doi.org/10.1111/jcal.12924},
   issue = {3},
   journal = {Journal of Computer Assisted Learning},
   keywords = {ICT skills,assessment measurement,effectively using ICT,primary education},
   pages = {960-972},
   title = {Development and validation of a test for measuring primary school students' effective use of ICT: The ECC-ICT test},
   volume = {40},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12924},
   year = {2024}
}
@article{Lindau2022,
   abstract = {Abstract CommunityRx is a theory-driven, information technology-based intervention, developed with and in a predominantly African American/Black community, that provides patients with personalized information (a “HealtheRx”) about self-management and social care resources in their community. We described patient and clinician information sharing after exposure to the intervention during a clinical trial. Survey data from 145 patients (ages 45–74) and 121 clinicians were analyzed. Of patients who shared information at least once (49\%), 47\% reported sharing ≥3 times (range 1–14). Patient sharers were in poorer physical health (mean PCS 37.6 vs. 40.8, p = .05) than nonsharers and more likely to report going to a resource on their HealtheRx (79 vs. 41\%, p ≤ .05). Most patient sharers provided others a look at or copy of their HealtheRx, keeping the original. Patients used the HealtheRx to promote credibility of the information and communicate that resources were disease-specific and local. Half of clinicians shared HealtheRx resource information with peers; sharers were 3 times more likely than nonsharers to feel they were well-informed about resources to address social needs (55 vs. 18\%, p < .01). Information sharing by clinicians and patients is an understudied mechanism that could amplify the effects of a growing class of community resource referral information technologies.},
   author = {Stacy Tessler Lindau and Jennifer A Makelarski and Emily M Abramsohn and David G Beiser and Kelly Boyd and Elbert S Huang and Kelsey Paradise and Elizabeth L Tung},
   doi = {https://doi.org/10.1002/asi.24560},
   issue = {3},
   journal = {Journal of the Association for Information Science and Technology},
   pages = {438-448},
   title = {Sharing information about health-related resources: Observations from a community resource referral intervention trial in a predominantly African American/Black community},
   volume = {73},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24560},
   year = {2022}
}
@article{Godoy2022,
   abstract = {Abstract Exposure to adverse environments are risk factors for neurodevelopmental problems in childhood. Children exposed to such environments may benefit from interventions that target social communication abilities, since these are protective factors for healthy neurodevelopment. This randomized controlled trial will test the efficacy of Paediatric Autism Communication Therapy (PACT) in improving social communication development in young children at risk for neurodevelopmental difficulties living in poverty in Brazil. Participants will be 160 children aged 2–4 years with lower-than-average social communication abilities and their primary caregivers. Child–caregiver dyads will be recruited from public childhood education centers in impoverished urban regions of the city of São Paulo, Brazil. Lower-than-average social communication abilities will be defined by standard scores (≤84) on the socialization and/or communication domains of the Vineland Adaptive Behavior Scales. Child–caregiver dyads will be randomized to receive 12 sessions of the PACT intervention (n = 80) or 5 months of community support as usual plus psychoeducation (n = 80). The primary outcome (parent–child interaction) and secondary outcomes (parent-reported social communication abilities and neurophysiological activity during a live social interaction) will be measured pre- and postintervention. This study may lead to new interventions for vulnerable young children in Brazil and better understanding of the neural mechanisms of PACT.},
   author = {Priscilla B G Godoy and Elizabeth Shephard and Adriana Argeu and Leticia R Silveira and Erica Salomone and Catherine Aldred and Jonathan Green and Guilherme V Polanczyk and Alicia Matijasevich},
   doi = {https://doi.org/10.1111/nyas.14784},
   issue = {1},
   journal = {Annals of the New York Academy of Sciences},
   keywords = {early child development,preventative intervention,randomized controlled trial,social communication development,socioeconomic disadvantage},
   pages = {104-115},
   title = {Social communication therapy for children at risk for neurodevelopmental difficulties: Protocol for a clinical trial},
   volume = {1514},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14784},
   year = {2022}
}
@article{Beamonte2022,
   abstract = {Abstract As a key part of model-driven development, modeling allows users to represent the application workflow or to automatically generate source code. This is convenient for developers, particularly to create or improve real-time applications embedded in complex systems. Multicore systems are difficult to debug because the concurrently running processes can interfere with each other. In real-time systems, timing constraints add to the complexity, invalidating results when a deadline is missed. Tracing is usually the most accurate and reliable tool to study the runtime behaviour of those applications. However, the interpretation of voluminous detailed execution traces requires a deep understanding of the operating system and application behaviour, and time to dig through the millions of trace events.In this paper, we present the use of model-based constraints on top of user-space and kernel traces to provide weighted analysis results. Our algorithms have been applied to multiple traces showing common problems for multi-core real-time systems. The experimental results show that our algorithms can quickly identify many different types of problems with a low runtime, even for traces with millions of events, thus helping to save time when analyzing thousands of trace events for complex systems.},
   author = {Raphael Beamonte and Naser Ezzati-Jivan and Michel R Dagenais},
   doi = {https://doi.org/10.1002/cpe.6974},
   issue = {17},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {concurrent systems,execution tracing,model verification,multicore system,real-time systems},
   pages = {e6974},
   title = {Execution trace-based model verification to analyze multicore and real-time systems},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6974},
   year = {2022}
}
@article{Merniz2021,
   abstract = {Abstract The merit of higher order functions for hardware description and transformation is widely acknowledged by hardware designers. However, the use of higher order types makes their correctness proof very difficult. Herein, a new proof approach based on the principle of partial application is proposed which transforms higher order functions into partially applied first-order ones. Therefore, parameterised architectures modelled by higher order functions could be easily redefined only over first-order types. The proof could be performed by induction within the same specification framework that avoids translating the higher order properties between different semantics, which remains extremely difficult. Using the notion of parameterisation where verified components are used as parameters to build more complex ones, the approach fits elegantly in the incremental bottom-up design where both the design and its proof could be developed in a systematic way. The potential features of the proposed methodological proof approach are demonstrated over a detailed example of a circuit design and verification within a functional framework.},
   author = {Salah Merniz and Saad Harous},
   doi = {https://doi.org/10.1049/cdt2.12024},
   issue = {5},
   journal = {IET Computers and Digital Techniques},
   keywords = {electronic engineering computing,formal specification,functional programming,hardware description languages,hardware-software codesign,network synthesis},
   pages = {335-348},
   title = {Modelling and verification of parameterized architectures: A functional approach},
   volume = {15},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cdt2.12024},
   year = {2021}
}
@article{Pawar2023,
   abstract = {Abstract FieldimageR.Extra is an advanced R package that enhances user experience and computational efficiency in analyzing orthomosaic images from agricultural field trials. Through the integration of modern GIS libraries like terra, stars, and sf, it surpasses traditional packages like raster and sp. The FieldimageR.Extra offers interactive visualization, vector feature creation, and spatial data editing within the R environment through the integration of mapview and mapedit packages. Notably, FieldimageR.Extra introduces new functions like fieldShape_render and fieldShape_edit, enabling flexible plot grid generation and editing capabilities. Comparative evaluations demonstrate the superior performance of FieldimageR.Extra in handling spatial data, crop and extract operations, and plot grid generation. With its comprehensive features, FieldimageR.Extra stands as a valuable addition to the FieldimageR R package, offering researchers efficient tools for unmanned aerial vehicle-based orthomosaic image analysis in agricultural research. FIELDimageR.Extra is publicly available as a GitHub repository (https://github.com/filipematias23/FIELDimageR.Extra).},
   author = {Popat S Pawar and Filipe Inacio Matias},
   doi = {https://doi.org/10.1002/ppj2.20083},
   issue = {1},
   journal = {The Plant Phenome Journal},
   pages = {e20083},
   title = {FIELDimageR.Extra: Advancing user experience and computational efficiency for analysis of orthomosaic from agricultural field trials},
   volume = {6},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ppj2.20083},
   year = {2023}
}
@article{Hu2021,
   abstract = {In this paper, the authenticity of news information on the 5G Internet of Things (IoT) is studied, and a network false news information screening platform is designed and optimized by IoT combined with passive RFID. The electronic license chain based on data sovereignty is established, in which, combined with the identity identification and strong correlation ability based on the electronic license chain, a cross-industry, cross-business, and cross-field behavior record base database is formed; then, a digital library is constructed based on this base library; finally, through data sharing and management, a false news information feature extraction and screening platform is formed for the orderly management and reasonable dispatch of government resources and reducing various risks. The main functional modules implemented by the platform are the acquisition of news data and comment data, the retrieval and analysis of news data, the false detection of online news, and the visualization of false news data. However, there is still much public who are not aware or do not understand that news truth is this dynamic form. Therefore, this paper aims to inform the public that news truth in news context is a dynamic process by 5G Internet of Things combined with passive RFID. The public understands the circumstances where news truth may be dynamic truth to avoid being misled by false news.},
   author = {Dahai Hu and Qiong Xia},
   doi = {https://doi.org/10.1155/2021/9696472},
   issue = {1},
   journal = {Computational Intelligence and Neuroscience},
   pages = {9696472},
   title = {[Retracted] Internet False News Information Feature Extraction and Screening Based on 5G Internet of Things Combined with Passive RFID},
   volume = {2021},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/9696472},
   year = {2021}
}
@article{Yang2022,
   abstract = {Summary Deep learning has led to important breakthroughs in natural language processing and obtained the state-of-the-art results on machine reading comprehension. However, it is essential to consider the entity recognition and the detection of unanswerable questions for accuracy improvement. A novel question answering model is proposed with knowledge enhancement and answer verification to promote the performance of reading comprehension. With knowledge enhancement, the proposed model is able to recognize entities from the passage and detect word boundary precisely. To deal with unanswerable questions, the answerability of questions is evaluated based on the textual entailment. Empirical studies suggest that the proposed model has better ability of reading comprehension than others, with improvement on question answering tasks.},
   author = {Ziming Yang and Yuxia Sun and Qingxuan Kuang},
   doi = {https://doi.org/10.1002/cpe.5828},
   issue = {12},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {answer verification,automatic question answering,knowledge enhancement,machine Reading comprehension},
   pages = {e5828},
   title = {Question answering model based on machine reading comprehension with knowledge enhancement and answer verification},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5828},
   year = {2022}
}
@article{MahfoozUlHaque2021,
   abstract = {Abstract Parking spaces have been considered as vital resources in urban areas. Finding parking spaces in jam-packed areas is often challenging, stressful, and uncertain for the drivers that causes traffic congestion with a consequent of wastage of time, fuel, and increase of pollution. In recent years, context-aware computing paradigm has been considered to be the most effective approach to address these kinds of issues. Context-aware systems acquire and understand contextual information according to the current situation, perform reasoning, and then act intelligently on behalf of the user. These applications often run on tiny resource-bounded smart devices with the incorporation of embedded or attached sensors on these devices and they often exhibit complex and adaptive behaviour. In this paper, we propose a context-aware parking application framework to assist drivers in finding parking slots dynamically while moving and/or arriving at the destination. We optimize the context-aware parking framework with bounds on computational resources for the decision support dynamically in a highly decentralized environment. To illustrate the use of the proposed system, we model the context-aware parking system using Uppaal model checker for formal analysis and verify the correctness properties of the system.},
   author = {Hafiz Mahfooz Ul Haque and Haidar Zulfiqar and Abrar Ahmed and Yasir Ali},
   doi = {https://doi.org/10.1002/cpe.5401},
   issue = {2},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {Uppaal model checker,context-awareness,model checking,smart parking},
   note = {e5401 cpe.5401},
   pages = {e5401},
   title = {A context-aware framework for modelling and verification of smart parking systems in urban cities},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5401},
   year = {2021}
}
@article{Zong2022,
   abstract = {Abstract Current web accessibility guidelines ask visualization designers to support screen readers via basic non-visual alternatives like textual descriptions and access to raw data tables. But charts do more than summarize data or reproduce tables; they afford interactive data exploration at varying levels of granularity—from fine-grained datum-by-datum reading to skimming and surfacing high-level trends. In response to the lack of comparable non-visual affordances, we present a set of rich screen reader experiences for accessible data visualization and exploration. Through an iterative co-design process, we identify three key design dimensions for expressive screen reader accessibility: structure, or how chart entities should be organized for a screen reader to traverse; navigation, or the structural, spatial, and targeted operations a user might perform to step through the structure; and, description, or the semantic content, composition, and verbosity of the screen reader's narration. We operationalize these dimensions to prototype screen-reader-accessible visualizations that cover a diverse range of chart types and combinations of our design dimensions. We evaluate a subset of these prototypes in a mixed-methods study with 13 blind and visually impaired readers. Our findings demonstrate that these designs help users conceptualize data spatially, selectively attend to data of interest at different levels of granularity, and experience control and agency over their data analysis process. An accessible HTML version of this paper is available at: http://vis.csail.mit.edu/pubs/rich-screen-reader-vis-experiences.},
   author = {Jonathan Zong and Crystal Lee and Alan Lundgard and JiWoong Jang and Daniel Hajas and Arvind Satyanarayan},
   doi = {https://doi.org/10.1111/cgf.14519},
   issue = {3},
   journal = {Computer Graphics Forum},
   keywords = {Accessibility design and evaluation methods,CCS Concepts,• Human-centered computing → Visualization design and evaluation methods},
   pages = {15-27},
   title = {Rich Screen Reader Experiences for Accessible Data Visualization},
   volume = {41},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14519},
   year = {2022}
}
@article{Boukadida2021,
   abstract = {Abstract NeoVascularization (NV) occurs in the Proliferative Diabetic Retinopathy (PDR) stage, where the development progress of new vessels presents a high risk for severe vision loss and blindness. Therefore, early NV detection is primordial to preserve patient's vision. Several automated methods have been proposed to detect the NV on retinograph-captured fundus images. However, their employment is constrained by the reduced ophthalmologist-per-person ratio and the expensive equipment for image capturing. This paper presents a novel method for NV detection in smartphone-captured fundus images. The implementation of the method on a smartphone having an optical lens for fundus capturing leads to a Mobiles-Aided-Screening system of PDR (MAS-PDR). The challenge is to ensure accurate and robust detection even with moderate quality of fundus image, on reduced execution time. Within this objective, we identify the major criteria of neovascularized vessels which are tortuosity, width, bifurcation, and density. Our main contribution consists in proposing a sharp feature to reflect each criterion on reduced computational complexity processing. Therefore, the features are provided to a random forest classifier to deduce the PDR stage. A dataset raised from publicly databases is used on a 10-cross validation process where 98.69\% accuracy, 97.73\% sensitivity, and 99.12\% specificity are achieved. To evaluate the robustness, the same experimentation is repeated after applying motion blur filters to the fundus image dataset, where 98.91\% accuracy, 96.75\% sensitivity, and 100\% specificity are deduced. Moreover, NV screening is performed under 3 s when executed in smartphone devices demonstrating the appropriateness of our method to MAS-PDR.},
   author = {Rahma Boukadida and Yaroub Elloumi and Mohamed Akil and Mohamed Hedi Bedoui},
   doi = {https://doi.org/10.1002/ima.22547},
   issue = {3},
   journal = {International Journal of Imaging Systems and Technology},
   keywords = {fundus images,mobile health,mobile-aided-screening (MAS) system,neovascularization,random forest (RF),smartphone captured fundus image},
   pages = {1638-1654},
   title = {Mobile-aided screening system for proliferative diabetic retinopathy},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ima.22547},
   year = {2021}
}
@article{Arfi2023,
   abstract = {Abstract Formal methods are proven techniques that provide a rigorous mathematical basis to software development. In particular, they allow the quality of development to be effectively improved by making accurate and explicit modelling, so that anomalies like ambiguities and incompleteness are identified in the early phases of the software development process. Semi-formal UML models and formal Timed Automata models are used to design a telerehabilitation system through a practical approach based on abstraction and refinement. The formal verification of expected properties of the system is performed by the Uppaal tool. The motivation of this work is threefold: (i) showing the usefulness of formal methods to satisfy the validation needs of a medical telerehabilitation system; (ii) demonstrating our approach of system analysis through refinements to guide the development of a complex system; and (iii) highlighting, from a real-life experience, the usefulness of models to involve the stakeholders all along the design of a system, from requirements to detailed specifications.},
   author = {Farid Arfi and Anne-Lise Courbis and Thomas Lambolais and François Bughin and Maurice Hayot},
   doi = {https://doi.org/10.1049/sfw2.12128},
   issue = {4},
   journal = {IET Software},
   keywords = {formal specification,formal verification,health care,software engineering,software reliability,unified modelling language},
   pages = {582-599},
   title = {Formal verification of a telerehabilitation system through an abstraction and refinement approach using Uppaal},
   volume = {17},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12128},
   year = {2023}
}
@article{Belmoukadam2021,
   abstract = {Summary Screen resolution along with network conditions are main objective factors impacting the user experience, in particular for video streaming applications. User terminals on their side feature more and more advanced characteristics resulting in different network requirements for good visual experience. Previous studies tried to link mean opinion score (MOS) to video bitrate for different screen types (e.g., Common Intermediate Format [CIF], Quarter Common Intermediate Format [QCIF], and High Definition [HD]). We leverage such studies and formulate a Quality of Experience (QoE)-driven resource allocation problem to pinpoint the optimal bandwidth allocation that maximizes the QoE over all users of a network service provider located behind the same bottleneck link, while accounting for the characteristics of the screens they use for video playout. For our optimization problem, QoE functions are built using curve fitting on datasets capturing the relationship between MOS, screen characteristics, and bandwidth requirements. We propose a simple heuristic based on Lagrangian relaxation and Karush Kuhn Tucker (KKT) conditions to efficiently solve the optimization problem. Our numerical simulations show that the proposed heuristic is able to increase overall QoE up to 20\% compared to an allocation with a TCP look-alike strategy implementing max-min fairness.},
   author = {Othmane Belmoukadam and Muhammad Jawad Khokhar and Chadi Barakat},
   doi = {https://doi.org/10.1002/nem.2128},
   issue = {1},
   journal = {International Journal of Network Management},
   note = {e2128 nem.2128},
   pages = {e2128},
   title = {On accounting for screen resolution in adaptive video streaming: QoE-driven bandwidth sharing framework},
   volume = {31},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nem.2128},
   year = {2021}
}
@article{Stevenson2022,
   abstract = {Abstract Background Screen-sharing technologies enable applications such as screen mirroring, video streaming and instant messaging across multiple device screens. Despite their increasing use in many contemporary classrooms, there is a paucity of research directly examining pedagogical benefits and issues of these technologies. Objectives This study investigated the influence of screen-sharing technologies on teachers' practices, highlighting pedagogical benefits and issues encountered. Methods The paper drew on a sample of 321 K-12 teachers and utilised principal components analysis, descriptive statistics and inductive coding. Results and Conclusions Teachers reported greater mobility, increased ease of content sharing and deeper learner cognition as attributes of improved learning and teaching with the technologies. However, a minority highlighted technical issues such as network and hardware problems affecting their confidence. Despite increased mobility as an affordance, teachers' perceptions about mobility largely appeared to be predicated on assumptions that the technologies were intended for didactic instruction. Takeaways The study contributes to a more nuanced understanding of screen-sharing affordances while underscoring learner-led screensharing as a focus for future research and practice.},
   author = {Michael Stevenson and Jennifer W M Lai and Matt Bower},
   doi = {https://doi.org/10.1111/jcal.12647},
   issue = {3},
   journal = {Journal of Computer Assisted Learning},
   keywords = {pedagogy,screen-casting,screen-mirroring,screen-sharing},
   pages = {770-783},
   title = {Investigating the pedagogies of screen-sharing in contemporary learning environments—A mixed methods analysis},
   volume = {38},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12647},
   year = {2022}
}
@inbook{Stokes2023,
   abstract = {Summary Global commercial leaders (e.g. Google, Amazon, and Toyota) and governments around the world are heavily investing in intelligent, bidirectional interactions between humans and technologies that involve complex social interactions. The military sector, in particular, is investing in modernization strategies that target artificial intelligence/machine learning (AI/ML) techniques that lend themselves to human–machine teaming in order to prepare for a future of multidomain operations. As with the pioneering empirical approach to human assessment and selection by global military leaders post–World War I [16], the immediacy, complexities, size, diversity, and resource capability of military use cases can generate the foundational underpinnings for shared problems, such as human-machine systems (HMS). When executed with strategic partners (e.g. commercial sector, partner nations), these underpinnings can be extrapolated and validated in multiple application domains. This chapter outlines key social cognitive complexities best examined in situ with real users, and highlights collaboration opportunities with the U.S. military (e.g. Army Project Convergence) as one potential path for in situ test and validation.},
   author = {Charlene K Stokes and Monika Lohani and Arwen H DeCostanza and Elliot Loh},
   doi = {https://doi.org/10.1002/9781119863663.ch7},
   isbn = {9781119863663},
   booktitle = {Handbook of Human‐Machine Systems},
   keywords = {embodied cognition,experimentation methodology,human-machine collaboration/teaming,social cognition,socio-technical systems},
   pages = {71-82},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Human–Machine Social Systems: Test and Validation via Military Use Cases},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119863663.ch7},
   year = {2023}
}
@article{Boukelkoul2021,
   abstract = {Summary In the purpose of analyzing and checking time constraints in business processes, we propose to model and simulate them in the “discrete event system specification” (DEVS) formalism. The proposed model will contain atomic DEVS models representing the activities, supervised by, other DEVS control models, and based on one to one mapping. Hence, guaranteeing a close concordance with the temporal requirements. Thus, the simulation of the model results in outputs about the successful execution regarding to the temporal constraints soundness. Therefore, the modeler will be able to predict unsuitable temporal behavior. Thus, the model responds to the lack of modeling tools that do not offer enough formal specification of time constraints at the earlier phases of design as in, the standard “business process management notation.” The proposed model handles different type of time constraints in business process such as intra-activities, inter-activities, and inter-process time constraints. The proposed model is implemented into JAVADEVS using DEVS-Suite platform.},
   author = {Sofiane Boukelkoul and Ramdane Maamri and Mohamed Chihoub},
   doi = {https://doi.org/10.1002/cpe.5753},
   issue = {1},
   journal = {Concurrency and Computation: Practice and Experience},
   keywords = {Business process,DEVS,model checking,modeling and simulation,time constraints},
   pages = {e5753},
   title = {A discrete event model for analysis and verification of time-constrained business processes},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5753},
   year = {2021}
}
@article{Artho2020,
   abstract = {Summary In this paper, we extend work on model-based testing for Apache ZooKeeper, to handle watchers (triggers) and improve scalability. In a distributed asynchronous shared storage like ZooKeeper, watchers deliver notifications on state changes. They are difficult to test because watcher notifications involve an initial action that sets the watcher, followed by another action that changes the previously seen state. We show how to generate test cases for concurrent client sessions executing against ZooKeeper with the tool Modbat. The tests are verified against an oracle that takes into account all possible timings of network communication. The oracle has to verify that there exists a chain of events that triggers both the initial callback and the subsequent watcher notification. We show in detail how the oracle computes whether watch triggers are correct and how the model was adapted and improved to handle these features. Together with a new search improvement that increases both speed and accuracy, we are able to verify large test setups and confirm several defects with our model.},
   author = {Cyrille Artho and Kazuaki Banzai and Quentin Gros and Guillaume Rousset and Lei Ma and Takashi Kitamura and Masami Hagiya and Yoshinori Tanabe and Mitsuharu Yamamoto},
   doi = {https://doi.org/10.1002/stvr.1720},
   issue = {7-8},
   journal = {Software Testing, Verification and Reliability},
   keywords = {Apache ZooKeeper,asynchronous systems,concurrency,model-based testing,networked systems,trigger},
   note = {e1720 stvr.1720},
   pages = {e1720},
   title = {Model-based testing of Apache ZooKeeper: Fundamental API usage and watchers},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1720},
   year = {2020}
}
@article{Ortega-Cabezas2020,
   abstract = {Abstract Innovative techniques to validate software are needed to reduce cost and increase software quality. This research aims to check if two rule-based expert systems (EXs) combined with dynamic-link libraries (dlls) perform better than other techniques widely employed in the automotive sector when validating the engine electronic control unit (ECU) software by using a hardware-in-the-loop (HIL) simulation. To perform this research, 15 software modules (SMs) of different complexities were chosen to be validated in an HIL simulation by using different techniques such as the manual execution, the tester-in-the-loop, the model-based testing, a rule-based EX, and the combination of two EXs to establish the code and functional coverage, the productivity gain, the number of bugs found, potential limitations of each technique, and the success rate of the HIL simulation. The test cases used are described in-depth in Section 2. The enhancement, which dlls and EXs offer, depends on the number of states in the functional model used in the EXs and the number of subintervals in which the SM inputs can be divided. A range between 6 and 16 more bugs can be detected when using two EXs. The HIL enhancement can reach 6\%, 16.8\%, and 18\% depending on the SM complexity.},
   author = {Pedro Miguel Ortega-Cabezas and Antonio Colmenar-Santos and David Borge-Diez and Jorge Juan Blanes-Peiró},
   doi = {https://doi.org/10.1002/smr.2223},
   issue = {1},
   journal = {Journal of Software: Evolution and Process},
   keywords = {dynamic-link library,embedded software,expert system,model-based testing,software validation},
   note = {e2223 JSME-18-0154.R2},
   pages = {e2223},
   title = {Application of rule-based expert systems in hardware-in-the-loop simulation case study: Software and performance validation of an engine electronic control unit},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2223},
   year = {2020}
}
@article{Bucaioni2022,
   abstract = {Abstract Software product line engineering emerged as an effective approach for the development of families of software-intensive systems in several industries. Although its use has been widely discussed and researched, there are still several open challenges for its industrial adoption and application. One of these is how to efficiently develop and reuse shared software artifacts, which have dependencies on the underlying electrical and hardware systems of products in a family. In this work, we report on our experience in tackling such a challenge in the railway industry and present a model-based approach for the automatic generation of test scripts for product variants in software product lines. The proposed approach is the result of an effort leveraging the experiences and results from the technology transfer activities with our industrial partner Alstom SA in Sweden. We applied and evaluated the proposed approach on the Aventra software product line from Alstom SA. The evaluation showed that the proposed approach mitigates the development effort, development time, and consistency drawbacks associated with the traditional, manual creation of test scripts. We performed an online survey involving 37 engineers from Alstom SA for collecting feedback on the approach. The result of the survey further confirms the aforementioned benefits.},
   author = {Alessio Bucaioni and Fabio Di Silvestro and Inderjeet Singh and Mehrdad Saadatmand and Henry Muccini},
   doi = {https://doi.org/10.1002/smr.2498},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   keywords = {automation,model-based software engineering,product line engineering,testing},
   pages = {e2498},
   title = {Model-based generation of test scripts across product variants: An experience report from the railway industry},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2498},
   year = {2022}
}
@inproceedings{TerziAnastasiaandBibi2024,
   abstract = {ChatGPT can advise developers and provide code on how to fix bugs, add new features, refactor, reuse, and secure their code but currently, there is little knowledge about whether the developers trust ChatGPT's responses and actually use the provided code. In this context, this study aims to identify patterns that describe the interaction of developers with ChatGPT with respect to the characteristics of the prompts and the actual use of the provided code by the developer. We performed a case study on 267,098 lines of code provided by ChatGPT related to commits, pull requests, files of code, and discussions between ChatGPT and developers. Our findings show that developers are more likely to integrate the given code snapshot in their code base when they have provided information to ChatGPT through several rounds of brief prompts that include problem-related specific words instead of using large textual or code prompts. Results also highlight the ability of ChatGPT to handle efficiently different types of problems across different programming languages.},
   author = {Stamatia
and Tsitsimiklis Nikolaos
and Angelidis Pantelis Terzi Anastasia
and Bibi},
   city = {Cham},
   editor = {Lidia
and Papadopoulos George Angelos Achilleos Achilleas
and Fuentes},
   isbn = {978-3-031-66459-5},
   booktitle = {Reuse and Software Quality},
   pages = {137-152},
   publisher = {Springer Nature Switzerland},
   title = {Using Code from ChatGPT: Finding Patterns in the Developers' Interaction with ChatGPT},
   year = {2024}
}

@book{kitchenham2007guidelines,
    title = {Guidelines for performing systematic literature reviews in software engineering},
    nationality = {},
    author = { and Kitchenham, {BA} and Charters, S},
    publisher = {{EBSE} Technical Report},
    month = jan,
    year = {2007}
}

@article{prisma2020,
    author = {Page, Matthew J and McKenzie, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M and Hr{\'o}bjartsson, Asbj{\o}rn and Lalu, Manoj M and Li, Tianjing and Loder, Elizabeth W and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A and Stewart, Lesley A and Thomas, James and Tricco, Andrea C and Welch, Vivian A and Whiting, Penny and Moher, David},
    title = {The PRISMA 2020 statement: an updated guideline for reporting systematic reviews},
    volume = {372},
    elocation-id = {n71},
    year = {2021},
    doi = {10.1136/bmj.n71},
    publisher = {BMJ Publishing Group Ltd},
    URL = {https://www.bmj.com/content/372/bmj.n71},
    eprint = {https://www.bmj.com/content/372/bmj.n71.full.pdf},
    journal = {BMJ}
}

@inproceedings{Alshahwan2024-2,
   author = {Alshahwan, Nadia and Chheda, Jubin and Finogenova, Anastasia and Gokkaya, Beliz and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy},
title = {Automated Unit Test Improvement using Large Language Models at Meta},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663839},
doi = {10.1145/3663529.3663839},
abstract = {This paper describes Meta’s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75\% of TestGen-LLM’s test cases built correctly, 57\% passed reliably, and 25\% increased coverage. During Meta’s Instagram and Facebook test-a-thons, it improved 11.5\% of all classes to which it was applied, with 73\% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {185–196},
numpages = {12},
keywords = {Automated Test Generation, Genetic Improvement, LLMs, Large Language Models, Unit Testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{Xiao2024-1,
   author = {Xiao, Danni and Guo, Yimeng and Li, Yanhui and Chen, Lin},
title = {Optimizing Search-Based Unit Test Generation with Large Language Models: An Empirical Study},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3674813},
doi = {10.1145/3671016.3674813},
abstract = {Search-based unit test generation methods have been considered effective and widely applied, and Large Language Models (LLMs) have also demonstrated their powerful generation ability. Therefore, some scholars have proposed using LLMs to enhance search-based unit test generation methods and have preliminarily confirmed that LLMs can help alleviate the problem of test coverage plateaus. However, it is still unclear when and how LLMs should intervene in the time-consuming test generation process. This paper explores the application of LLMs at various stages of search-based test generation (SBTG) (including the initial stage, the test generation period, and the test coverage plateaus), as well as strategies for controlling the frequency of LLM intervention. A comprehensive empirical study was conducted on 486 Python benchmark modules from 27 projects. The experimental results show that 1) LLM intervention has a positive effect at any stage, whether to improve coverage over a fixed period or to reduce the time to reach a specific coverage; 2) a reasonable intervention frequency is crucial for LLMs to have a positive effect on SBTG. This work can better help understand when and how LLMs should be applied in SBTG and provide valuable suggestions for developers in practice.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {71–80},
numpages = {10},
keywords = {Large Language Model, Search-based Testing, Unit Test},
location = {Macau, China},
series = {Internetware '24}
}


@inproceedings{He2023-2,
   abstract = {Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1\% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3\% (or degraded to 36.8\%). Importantly, SVEN closely matches the original LMs in functional correctness.},
   title = {Large Language Models for Code: Security Hardening and Adversarial Testing},
   url={http://dx.doi.org/10.1145/3576915.3623175},
   DOI={10.1145/3576915.3623175},
   booktitle={Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
   publisher={ACM},
   author={He, Jingxuan and Vechev, Martin},
   year={2023},
   month=nov, pages={1865–1879},
   collection={CCS ’23} }
}

@inproceedings{Alshahwan2024-1,
   abstract = {TestGen automatically generates unit tests, carved from serialized observations of complex objects, observed during app execution.  We describe the development and deployment of TestGen at Meta.   In particular, we focus on the scalability challenges overcome during development in order to deploy observation-based test carving at scale in industry.  So far, TestGen has landed 518 tests into production, which have been executed 9,617,349 times in continuous integration, finding 5,702 faults.   Meta is currently in the process of more widespread deployment.  Our evaluation reveals that, when carving its observations from 4,361 reliable end-to-end tests, TestGen was able to generate tests for at least 86\% of the classes covered by end-to-end tests.   Testing on 16 Kotlin Instagram app-launch-blocking tasks demonstrated that the TestGen tests would have trapped 13 of these before they became launch blocking.},
   title = {Observation-Based Unit Test Generation at Meta},
}

@inproceedings{Liu2022-1,
   abstract = {End-to-end test cases that exercise the application under test via its user interface (UI) are known to be hard for developers to read and understand; consequently, diagnosing failures in these tests and maintaining them can be tedious. Techniques for computing natural-language descriptions of test cases can help increase test readability. However, so far, such techniques have been developed for unit test cases; they are not applicable to end-to-end test cases.In this paper, we focus on the problem of computing natural-language labels for the steps of end-to-end UI test cases for web applications. We present two techniques that apply natural-language processing to information available in the browser document object model (DOM). The first technique is an instance of a supervised approach in which labeling-relevant DOM attributes are ranked via manual analysis and fed into label computation. However, supervised approach requires a training dataset. So we propose the second technique, which is unsupervised: it leverages probabilistic context-free grammar learning to compute dominant DOM attributes automatically. We implemented these techniques, along with two simpler baseline techniques, in a tool called CrawLabel (available as a plugin to Crawljax, a state-of-the-art UI test-generation tool for web applications) and evaluated their effectiveness on open-source web applications. Our results indicate that the supervised approach can achieve precision, recall, and Fl-score of 83.38, 60.64, and 66.40, respectively. The unsupervised approach, although less effective, is competitive, achieving scores of 72.37, 58.12, and 59.77. We highlight key results and discuss the implications of our findings.},
   title = {CrawLabel: computing natural-language labels for UI test cases},
}

@article{Munley2024,
   title = {LLM4VV: Developing LLM-driven testsuite for compiler validation},
    journal = {Future Generation Computer Systems},
    volume = {160},
    pages = {1-13},
    year = {2024},
    issn = {0167-739X},
    doi = {https://doi.org/10.1016/j.future.2024.05.034},
    url = {https://www.sciencedirect.com/science/article/pii/S0167739X24002449},
    author = {Christian Munley and Aaron Jarmusch and Sunita Chandrasekaran},
    keywords = {Large language models, Code generation, Validation and verification, OpenACC},
    abstract = {Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. The goal of this work is to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based parallel programming paradigm, OpenACC. To do so, in this paper, we explore the capabilities of state-of-the-art LLMs, including open-source LLMs - Meta’s Codellama, Phind’s fine-tuned version of Codellama, Deepseek’s Deepseek Coder and closed-source LLMs - OpenAI’s GPT-3.5-Turbo and GPT-4-Turbo. We further fine-tune the open-source LLMs and GPT-3.5-Turbo using our own testsuite dataset along with using the OpenACC specification. We also explored these LLMs using various prompt engineering techniques that include code template, template with retrieval-augmented generation (RAG), one-shot example, one-shot with RAG, expressive prompt with code template and RAG. This paper highlights our findings from over 5000 tests generated via all the above mentioned methods. Our contributions include: (a) exploring the capabilities of the latest and relevant LLMs for code generation, (b) investigating fine-tuning and prompt methods, and (c) analyzing the outcome of LLMs generated tests including manually analysis of representative set of tests. We found the LLM Deepseek-Coder-33b-Instruct produced the most passing tests followed by GPT-4-Turbo.}
}

@inproceedings{Nguyen2023,
   author = {Nguyen, Cuong and Bui, Huy and Nguyen, Vu and Nguyen, Tien},
title = {An Approach to Generating API Test Scripts Using GPT},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628947},
doi = {10.1145/3628797.3628947},
abstract = {As more software systems publish and use web services or APIs today, automated API testing is an important activity to help effectively ensure the quality of software services before they are released for their usage. Generating test scripts and data is a crucial step to perform API test automation successfully. In this paper, we propose an approach leveraging GPT, a large language model, and API’s Swagger specification to automatically generate test scripts and test data for API testing. Our approach also applies GPT’s self-refining with the feedback by executing tests on Katalon. We evaluate our proposed approach using a data set of seven APIs consisting of 157 endpoints and 179 operations. The result shows that while our approach generates fewer test scripts and data inputs, it can cover more successful status codes of 2xx than a state-of-the-art tool. This result suggests that leveraging the ability of GPT as a large language model to interpret API’s Swagger specification has the potential for improving the efficacy of generating test scripts and data for API testing.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {501–509},
numpages = {9},
keywords = {API Test Data, API Test Script, Automated API Testing, GPT, Katalon, Large Language Models (LLMs), Prompt Engineering, Restful APIs},
location = {Ho Chi Minh, Vietnam},
series = {SOICT '23}
}

@inproceedings{Lu2024,
   abstract = {eBPF is being used to implement increasingly critical pieces of system logic. eBPF's verifier raises the cost of adoption of the technology, as making programs pass the verifier can be very effortful. We observe that the guarantees provided by the verifier have only been used for the narrow objective of verifying these programs' safety, despite them also enabling the automatic verification of program functional correctness. We envision a framework allowing developers to easily specify and automatically verify their eBPF programs with very little extra cost compared to simply passing the verifier.We showcase our implementation of DRACO, built on top of KLEE. DRACO allows developers to fully or partially specify eBPF programs, add verification-time assert statements, and reason about multiple eBPF programs interacting with each other and userspace, all at minimal additional cost to the developers. We use DRACO to either fully or partially verify the correctness of several real-world or experimental XDP programs.},
   title = {Towards Functional Verification of eBPF Programs},
}

@inproceedings{Cankar2023,
   abstract = {Security represents one of the crucial concerns when it comes to DevOps methodology-empowered software development and service delivery process. Considering the adoption of Infrastructure as Code (IaC), even minor flaws could potentially cause fatal consequences, especially in sensitive domains such as healthcare and maritime applications. However, most of the existing solutions tackle either Static Application Security Testing (SAST) or run-time behavior analysis distinctly. In this paper, we propose a) IaC Scan Runner, an open-source solution developed in Python for inspecting a variety of state-of-the-art IaC languages in application design time and b) the run time anomaly detection tool called LOMOS. Both tools work in synergy and provide a valuable contribution to a DevSecOps tool set. The proposed approach is demonstrated and their results will be demonstrated on various case studies showcasing the capabilities of static analysis tool IaC Scan Runner combined with LOMOS - log analysis artificial intelligence-enabled framework.},
   title = {Security in DevSecOps: Applying Tools and Machine Learning to Verification and Monitoring Steps},
}

@inproceedings{Yang2024-3,
   abstract = {Rust is a relatively new programming language known for its memory safety and numerous advanced features. It has been widely used in system software in recent years. Thus, ensuring the reliability and robustness of the only implementation of the Rust compiler, rustc, is critical. However, compiler testing, as one of the most effective techniques to detect bugs, faces difficulties in generating valid Rust programs with sufficient diversity due to its stringent memory safety mechanisms. Furthermore, existing research primarily focuses on testing rustc to trigger crash errors, neglecting incorrect compilation results - miscompilation. Detecting miscompilation remains a challenge in the absence of multiple implementations of the Rust compiler to serve as a test oracle.This paper tackles these challenges by introducing rust-twins, a novel and effective approach to performing automated differential testing for rustc to detect both crashes and miscompilations. We devise four Rust-specific mutators and adapt fourteen general mutators for Rust, each intends to produce a syntax and semantic valid Rust program to trigger rustc crashes. Additionally, we develop a macroize approach to rewrite a regular Rust program into dual macros with equivalent behaviors but in different implementations. Furthermore, we design an assessment component to check the equivalence by comparing the expansion results with a simple macro input. Finally, rust-twins attempts to expand the two macros with numerous complex inputs to detect differences. Due to the macro expansion mechanism, the root causes of differences can arise not only from the macro expansion part but also from any other mis-implemented compiler code.We have evaluated rust-twins on the latest version of rustc. Our experimental results indicate that rust-twins achieves twice the total line coverage and identifies more crashes and differences than the best baseline technique, rustsmith, after 24 hours of testing. In total, rust-twins triggered 10 rustc crashes, and 229 of the generated macros exposed rustc differences. We analyzed and reported 12 previously unknown bugs, of which 8 have already been confirmed and fixed.},
   title = {Rust-twins: Automatic Rust Compiler Testing through Program Mutation and Dual Macros Generation},
}

@inproceedings{Shi2024-1,
   abstract = {Implementing automation testing is difficult and as a consequence there is a growing desire for semi-automated software testing systems with humans in the loop. Leveraging the growth of LLMs, recent research has demonstrated LLMs’ potential to improve performance on test generation, reporting, and bug triaging. However, relatively little work has explored the interactivity issues that emerge in semi-automated LLM-assisted software test case development. To fill this gap, we present two user studies (N1 = 16, N2 = 24) that investigate productivity, creativity, and user attention in three semi-automated LLM-assisted interaction strategies: (1) pre-emptive prompting; (2) buffered response; and (3) guided input. We find that pre-emptively prompting the user significantly enhances branch coverage and task creativity by more than 30\% while reducing user’s off-task idle time by up to 48.7\%. We conclude by suggesting concrete research directions applying mixed-initiative principles for LLM-based interactive systems for semi-automated software testing.},
   author = {Shi, Billy and Kristensson, Per Ola},
title = {Pay Attention! Human-Centric Improvements of LLM-based Interfaces for Assisting Software Test Case Development},
year = {2024},
isbn = {9798400707186},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3672539.3686341},
doi = {10.1145/3672539.3686341},
booktitle = {Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {83},
numpages = {3},
location = {Pittsburgh, PA, USA},
series = {UIST Adjunct '24}
}

@inproceedings{Liu2023,
   abstract = {Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87\%, which is 93\% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42\% more app activities, 52\% more pages, and subsequently help reveal 122\% more bugs compared with the raw tool.},
   title = {Fill in the Blank: Context-Aware Automated Text Input Generation for Mobile GUI Testing},
}

@article{PradoLima2020,
   abstract = {Context: Continuous Integration (CI) environments allow frequent integration of software changes, making software evolution more rapid and cost-effective. In such environments, the regression test plays an important role, as well as the use of Test Case Prioritization (TCP) techniques. Such techniques attempt to identify the test case order that maximizes certain goals, such as early fault detection. This research subject has been raising interest because some new challenges are faced in the CI context, as TCP techniques need to consider time constraints of the CI environments. Objective: This work presents the results of a systematic mapping study on Test Case Prioritization in Continuous Integration environments (TCPCI) that reports the main characteristics of TCPCI approaches and their evaluation aspects. Method: The mapping was conducted following a plan that includes the definition of research questions, selection criteria and search string, and the selection of search engines. The search returned 35 primary studies classified based on the goal and kind of used TCP technique, addressed CI particularities and testing problems, and adopted evaluation measures. Results: The results show a growing interest in this research subject. Most studies have been published in the last four years. 80\% of the approaches are history-based, that is, are based on the failure and test execution history. The great majority of studies report evaluation results by comparing prioritization techniques. The preferred measures are Time and number/percentage of Faults Detected. Few studies address CI testing problems and characteristics, such as parallel execution and test case volatility. Conclusions: We observed a growing number of studies in the field. Future work should explore other information sources such as models and requirements, as well as CI particularities and testing problems, such as test case volatility, time constraint, and flaky tests, to solve existing challenges and offer cost-effective approaches to the software industry.},
   title = {Test Case Prioritization in Continuous Integration environments: A systematic mapping study},
}

