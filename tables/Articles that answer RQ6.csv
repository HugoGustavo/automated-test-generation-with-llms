Article,Year,Main Contribution
Degiovanni2022,2022,Introduces a mutation testing tool using CodeBERT to generate mutants.
Richter2022,2022,Proposes a learning-based mutation operator using masked language models to generate realistic bugs for benchmarks.
Akli2023,2023,Introduces the first few-shot learning approach using CodeBERT and Siamese networks to classify flaky tests.
Cardoso2023,2023,Evaluating DRL-MOBTEST's performance in generating test cases for Android apps.
Happe2023,2023,"Use of LLMs as AI sparring partners for penetration testing, including high-level task planning and low-level vulnerability hunting."
He2023-2,2023,Proposes a learning-based approach that uses property-specific vectors to control code generation security.
Kang2023,2023,"Introducing LIBRO, an LLM-based framework that automatically generates bug-reproducing tests from general bug reports."
LeTraon2023,2023,Discusses challenges in testing ML-enabled software and proposes adversarial input generation for robustness.
Alshahwan2024-2,2024,"Presenting TestGen-LLM, Meta's automated test improvement tool that uses LLMs to enhance human-written tests."
Chen2024-1,2024,Introduces ChatUniTest an LLM-based unit test generation framework with adaptive context and validation-repair mechanisms.
Feng2024,2024,Proposing a cost-effective UI automation testing approach combining LLMs with retrieval-augmented generation and machine learning.
Garg2024,2024,Studies CodeBERT-generated mutants' coupling with software vulnerabilities.
He2024-3,2024,"Introducing UniTSyn, a large-scale multilingual dataset of 2.7 million focal-test pairs that enhances LLMs' unit test generation."
Hoffmann2024-1,2024,Demonstrating the effectiveness of fine-tuned smaller LLMs for mobile app test generation.
Jiri2024,2024,"Explores LLMs' ability to generate comprehensive unit tests, focusing on Python and analyzing missing test cases."
Liu2024-2,2024,Proposes an LLM-based mobile GUI testing framework that formulates testing as a Q\&A task.
Liu2024-5,2024,Developing a domain-adapted LLM for VLSI design that outperforms general models on chip design tasks.
Paduraru2024,2024,Fine-tuning Code Llama for automated unit test creation in game development.
Pornphol2024,2024,Verification of relational completeness in SQL codes generated by ChatGPT.
Qin2024,2024,Case study on using GenAI for FOV analysis in VR exploration testing.
Rasool2024,2024,"Proposes VaryGen, an LLM-based approach for generating similar questions from unstructured text to test semantic caches."
Salcedo2024,2024,Demonstrates GPT-4's potential to accelerate vector processor SoC design and verification.
Schwachhofer2024,2024,Uses LLMs and reinforcement learning to generate test programs optimizing non-functional properties in SLT.
Shin2024,2024,"Proposes a Transformer-based approach for unit test generation, showing improvements compared to tools like EvoSuite."
Vito2024,2024,"Exploring LLM applications for automating software engineering tasks, specifically issue report classification and test scenario generation."
Wang2024-1,2024,Proposes a method that improves LLM-based unit test generation for complex Java methods by decomposing them into slices.
Wang2024-2,2024,Introduces a hybrid testing approach that combines static analysis with LLM-generated test instructions to replicate Android behaviors.
Xue2024-1,2024,"Proposes LLM4Fin, a fully automated approach combining LLMs and algorithmic methods to generate high-coverage FinTech test cases."
Xue2024-2,2024,Demonstrates that domain-specific knowledge integration significantly enhances LLM-generated test cases.
Yang2024-1,2024,Introduces the first test-free fault localization approach using bidirectional adapter layers on LLMs.
Yang2024-4,2024,Conducting the first comprehensive empirical evaluation of open-source LLMs for unit test generation.
Yuan2024,2024,"Proposes a ChatTester, a ChatGPT-based approach with iterative refinement that improves unit test generation."
Zhu2024,2024,Enhances test code completion by exploiting project similarities improving token and line-level completion accuracy.